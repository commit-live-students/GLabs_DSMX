{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GebRVKZggBMy"
   },
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g-0keyw5_FIj"
   },
   "outputs": [],
   "source": [
    "# NLP Pre-processing techniques or Methods:\n",
    "# Import The Text : reading the textual docs, pdf, word, text, csv, image files, HTML parsing\n",
    "# Non-ASCII characters, encoding= ASCII\n",
    "# UTF-8 characters\n",
    "\n",
    "# Understanding the terminologies in NLP\n",
    "# 1. Words\n",
    "# 2. sentence\n",
    "# 3. Paragraph\n",
    "# 4. Pages\n",
    "# 5. Document\n",
    "# 6. Corpus\n",
    "# 7. Directory\n",
    "\n",
    "# textual data cleaning steps\n",
    "# Sentencization\n",
    "# Toeknization\n",
    "# Stemming\n",
    "# Lemmatization\n",
    "# Stop Word removal\n",
    "# number removal\n",
    "# special symbols removal\n",
    "# vectorization of words\n",
    "# summarized data preparation\n",
    "\n",
    "# Tools for NLP\n",
    "# NLTK\n",
    "# spaCy\n",
    "# Gensim\n",
    "# Google NLP\n",
    "# Stanford NLP\n",
    "# Spark NLP\n",
    "# IBM NLP\n",
    "# Azure NLP\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NSOnFBY3B7CN"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 959,
     "status": "ok",
     "timestamp": 1565239016246,
     "user": {
      "displayName": "Bhushan Shewale",
      "photoUrl": "https://lh6.googleusercontent.com/-5a9j-ffSdSw/AAAAAAAAAAI/AAAAAAAAACw/ukiO53_VGNg/s64/photo.jpg",
      "userId": "17273113492750660774"
     },
     "user_tz": -330
    },
    "id": "zihVFJ7FTfwY",
    "outputId": "2a331535-1baa-4940-9213-37a22295e237"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When my loan was switched over to Navient i was never told that i had a deliquint balance because with XXXX i did not. When going to purchase a vehicle i discovered my credit score had been dropped from the XXXX into the XXXX. I have been faithful at paying my student loan. I was told that Navient was the company i had delinquency with. I contacted Navient to resolve this issue you and kept being told to just contact the credit bureaus and expalin the situation and maybe they could help me. I was so angry that i just hurried and paid the balance off and then after tried to dispute the delinquency with the credit bureaus. I have had so much trouble bringing my credit score back up.\n",
      "\n",
      "\n",
      "['Mortgage', 'Student loan', 'Credit card or prepaid card', 'Credit card', 'Debt collection', 'Credit reporting', 'Credit reporting, credit repair services, or other personal consumer reports', 'Bank account or service', 'Consumer Loan', 'Money transfers', 'Vehicle loan or lease', 'Money transfer, virtual currency, or money service', 'Checking or savings account', 'Payday loan', 'Payday loan, title loan, or personal loan', 'Other financial service', 'Prepaid card']\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "\n",
    "# Loading of dataset\n",
    "full_data = pd.read_csv('https://raw.githubusercontent.com/bhushan45/customer_complaints/master/customer_complaints.csv')\n",
    "\n",
    "# keeping the relevant columns\n",
    "data = full_data[[\"Consumer complaint narrative\", \"Product\"]]\n",
    "data.columns = [\"X\", \"y\"]\n",
    "data.head()\n",
    "\n",
    "# Printing out the first non-empty value of the X column. Hence the second value, index is 1\n",
    "print(data[\"X\"][1])\n",
    "print(\"\\n\")\n",
    "print(list(data[\"y\"].unique()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K-lNW_11g0Ii"
   },
   "source": [
    "## Tokenize first complaint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1496,
     "status": "ok",
     "timestamp": 1565238165176,
     "user": {
      "displayName": "Bhushan Shewale",
      "photoUrl": "https://lh6.googleusercontent.com/-5a9j-ffSdSw/AAAAAAAAAAI/AAAAAAAAACw/ukiO53_VGNg/s64/photo.jpg",
      "userId": "17273113492750660774"
     },
     "user_tz": -330
    },
    "id": "TnJM7Svwdt28",
    "outputId": "2b24c77e-2b0f-4bb2-83eb-8b72ab387cb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1018,
     "status": "ok",
     "timestamp": 1565239027568,
     "user": {
      "displayName": "Bhushan Shewale",
      "photoUrl": "https://lh6.googleusercontent.com/-5a9j-ffSdSw/AAAAAAAAAAI/AAAAAAAAACw/ukiO53_VGNg/s64/photo.jpg",
      "userId": "17273113492750660774"
     },
     "user_tz": -330
    },
    "id": "ar5ThoAaTuGX",
    "outputId": "8b05e5bf-87b9-41c6-9a3d-d067af042b82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First Complaint\n",
      "\n",
      "When my loan was switched over to Navient i was never told that i had a deliquint balance because with XXXX i did not. When going to purchase a vehicle i discovered my credit score had been dropped from the XXXX into the XXXX. I have been faithful at paying my student loan. I was told that Navient was the company i had delinquency with. I contacted Navient to resolve this issue you and kept being told to just contact the credit bureaus and expalin the situation and maybe they could help me. I was so angry that i just hurried and paid the balance off and then after tried to dispute the delinquency with the credit bureaus. I have had so much trouble bringing my credit score back up.\n",
      "\n",
      "Using the Split Command\n",
      "\n",
      "['When', 'my', 'loan', 'was', 'switched', 'over', 'to', 'Navient', 'i', 'was', 'never', 'told', 'that', 'i', 'had', 'a', 'deliquint', 'balance', 'because', 'with', 'XXXX', 'i', 'did', 'not.', 'When', 'going', 'to', 'purchase', 'a', 'vehicle', 'i', 'discovered', 'my', 'credit', 'score', 'had', 'been', 'dropped', 'from', 'the', 'XXXX', 'into', 'the', 'XXXX.', 'I', 'have', 'been', 'faithful', 'at', 'paying', 'my', 'student', 'loan.', 'I', 'was', 'told', 'that', 'Navient', 'was', 'the', 'company', 'i', 'had', 'delinquency', 'with.', 'I', 'contacted', 'Navient', 'to', 'resolve', 'this', 'issue', 'you', 'and', 'kept', 'being', 'told', 'to', 'just', 'contact', 'the', 'credit', 'bureaus', 'and', 'expalin', 'the', 'situation', 'and', 'maybe', 'they', 'could', 'help', 'me.', 'I', 'was', 'so', 'angry', 'that', 'i', 'just', 'hurried', 'and', 'paid', 'the', 'balance', 'off', 'and', 'then', 'after', 'tried', 'to', 'dispute', 'the', 'delinquency', 'with', 'the', 'credit', 'bureaus.', 'I', 'have', 'had', 'so', 'much', 'trouble', 'bringing', 'my', 'credit', 'score', 'back', 'up.']\n",
      "\n",
      "Using tokenize\n",
      "\n",
      "['When', 'my', 'loan', 'was', 'switched', 'over', 'to', 'Navient', 'i', 'was', 'never', 'told', 'that', 'i', 'had', 'a', 'deliquint', 'balance', 'because', 'with', 'XXXX', 'i', 'did', 'not', '.', 'When', 'going', 'to', 'purchase', 'a', 'vehicle', 'i', 'discovered', 'my', 'credit', 'score', 'had', 'been', 'dropped', 'from', 'the', 'XXXX', 'into', 'the', 'XXXX', '.', 'I', 'have', 'been', 'faithful', 'at', 'paying', 'my', 'student', 'loan', '.', 'I', 'was', 'told', 'that', 'Navient', 'was', 'the', 'company', 'i', 'had', 'delinquency', 'with', '.', 'I', 'contacted', 'Navient', 'to', 'resolve', 'this', 'issue', 'you', 'and', 'kept', 'being', 'told', 'to', 'just', 'contact', 'the', 'credit', 'bureaus', 'and', 'expalin', 'the', 'situation', 'and', 'maybe', 'they', 'could', 'help', 'me', '.', 'I', 'was', 'so', 'angry', 'that', 'i', 'just', 'hurried', 'and', 'paid', 'the', 'balance', 'off', 'and', 'then', 'after', 'tried', 'to', 'dispute', 'the', 'delinquency', 'with', 'the', 'credit', 'bureaus', '.', 'I', 'have', 'had', 'so', 'much', 'trouble', 'bringing', 'my', 'credit', 'score', 'back', 'up', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "#nltk.download('punkt')\n",
    "\n",
    "# Dropping nan values from dataframe\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Storing the first complaint\n",
    "first_complaint = data.iloc[0][0]\n",
    "\n",
    "\n",
    "# Printing the first complaint\n",
    "print(\"\\nFirst Complaint\\n\")\n",
    "print(first_complaint)\n",
    "\n",
    "# Using the split command\n",
    "print(\"\\nUsing the Split Command\\n\")\n",
    "bag_of_words_1 = first_complaint.split(\" \")\n",
    "print(bag_of_words_1)\n",
    "\n",
    "# Using the tokenize command\n",
    "print(\"\\nUsing tokenize\\n\")\n",
    "bag_of_words_2 = word_tokenize(first_complaint)\n",
    "print(bag_of_words_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EhXC6jQShPej"
   },
   "source": [
    "## Tokenize into sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1036,
     "status": "ok",
     "timestamp": 1565238224733,
     "user": {
      "displayName": "Bhushan Shewale",
      "photoUrl": "https://lh6.googleusercontent.com/-5a9j-ffSdSw/AAAAAAAAAAI/AAAAAAAAACw/ukiO53_VGNg/s64/photo.jpg",
      "userId": "17273113492750660774"
     },
     "user_tz": -330
    },
    "id": "Og-efGyZUPgc",
    "outputId": "03a3ff3a-ccc2-4652-873e-110db06b60f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of sentences\n",
      " ['When my loan was switched over to Navient i was never told that i had a deliquint balance because with XXXX i did not.', 'When going to purchase a vehicle i discovered my credit score had been dropped from the XXXX into the XXXX.', 'I have been faithful at paying my student loan.', 'I was told that Navient was the company i had delinquency with.', 'I contacted Navient to resolve this issue you and kept being told to just contact the credit bureaus and expalin the situation and maybe they could help me.', 'I was so angry that i just hurried and paid the balance off and then after tried to dispute the delinquency with the credit bureaus.', 'I have had so much trouble bringing my credit score back up.']\n",
      "\n",
      " ['when', 'my', 'loan', 'was', 'switched', 'over', 'to', 'navient', 'i', 'was', 'never', 'told', 'that', 'i', 'had', 'a', 'deliquint', 'balance', 'because', 'with', 'xxxx', 'i', 'did', 'not', '.', 'when', 'going', 'to', 'purchase', 'a', 'vehicle', 'i', 'discovered', 'my', 'credit', 'score', 'had', 'been', 'dropped', 'from', 'the', 'xxxx', 'into', 'the', 'xxxx', '.', 'i', 'have', 'been', 'faithful', 'at', 'paying', 'my', 'student', 'loan', '.', 'i', 'was', 'told', 'that', 'navient', 'was', 'the', 'company', 'i', 'had', 'delinquency', 'with', '.', 'i', 'contacted', 'navient', 'to', 'resolve', 'this', 'issue', 'you', 'and', 'kept', 'being', 'told', 'to', 'just', 'contact', 'the', 'credit', 'bureaus', 'and', 'expalin', 'the', 'situation', 'and', 'maybe', 'they', 'could', 'help', 'me', '.', 'i', 'was', 'so', 'angry', 'that', 'i', 'just', 'hurried', 'and', 'paid', 'the', 'balance', 'off', 'and', 'then', 'after', 'tried', 'to', 'dispute', 'the', 'delinquency', 'with', 'the', 'credit', 'bureaus', '.', 'i', 'have', 'had', 'so', 'much', 'trouble', 'bringing', 'my', 'credit', 'score', 'back', 'up', '.']\n"
     ]
    }
   ],
   "source": [
    "# first_complaint is already loaded onto the workspace\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "# Tokenizing sentences\n",
    "list_of_sentences = sent_tokenize(first_complaint)\n",
    "\n",
    "print(\"List of sentences\\n\", list_of_sentences)\n",
    "\n",
    "# Lowering first complaint\n",
    "first_complaint_lower = first_complaint.lower()\n",
    "\n",
    "# Tokenizing first complaint lower\n",
    "bag_of_words_lower = word_tokenize(first_complaint_lower)\n",
    "\n",
    "print(\"\\n\",bag_of_words_lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wC3eYAB6hGRr"
   },
   "source": [
    "## Vectorizing first complaint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 965,
     "status": "ok",
     "timestamp": 1565238242572,
     "user": {
      "displayName": "Bhushan Shewale",
      "photoUrl": "https://lh6.googleusercontent.com/-5a9j-ffSdSw/AAAAAAAAAAI/AAAAAAAAACw/ukiO53_VGNg/s64/photo.jpg",
      "userId": "17273113492750660774"
     },
     "user_tz": -330
    },
    "id": "6LOwfEzCd-Lu",
    "outputId": "c26a3122-2986-49b1-e3ed-7f1005d97da6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'i': 11, 'the': 8, '.': 7, 'was': 5, 'to': 5, 'and': 5, 'my': 4, 'had': 4, 'credit': 4, 'navient': 3, 'told': 3, 'that': 3, 'with': 3, 'xxxx': 3, 'when': 2, 'loan': 2, 'a': 2, 'balance': 2, 'score': 2, 'been': 2, 'have': 2, 'delinquency': 2, 'just': 2, 'bureaus': 2, 'so': 2, 'switched': 1, 'over': 1, 'never': 1, 'deliquint': 1, 'because': 1, 'did': 1, 'not': 1, 'going': 1, 'purchase': 1, 'vehicle': 1, 'discovered': 1, 'dropped': 1, 'from': 1, 'into': 1, 'faithful': 1, 'at': 1, 'paying': 1, 'student': 1, 'company': 1, 'contacted': 1, 'resolve': 1, 'this': 1, 'issue': 1, 'you': 1, 'kept': 1, 'being': 1, 'contact': 1, 'expalin': 1, 'situation': 1, 'maybe': 1, 'they': 1, 'could': 1, 'help': 1, 'me': 1, 'angry': 1, 'hurried': 1, 'paid': 1, 'off': 1, 'then': 1, 'after': 1, 'tried': 1, 'dispute': 1, 'much': 1, 'trouble': 1, 'bringing': 1, 'back': 1, 'up': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Creating an object of count vectorizer\n",
    "count_vectorizer = Counter(bag_of_words_lower)\n",
    "\n",
    "print(count_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WRBry4WLhjBj"
   },
   "source": [
    "## Data Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1519,
     "status": "ok",
     "timestamp": 1565239219616,
     "user": {
      "displayName": "Bhushan Shewale",
      "photoUrl": "https://lh6.googleusercontent.com/-5a9j-ffSdSw/AAAAAAAAAAI/AAAAAAAAACw/ukiO53_VGNg/s64/photo.jpg",
      "userId": "17273113492750660774"
     },
     "user_tz": -330
    },
    "id": "GGlrfEW7eCjm",
    "outputId": "ef4b8464-4ce8-4b67-bc9c-342e1c35378c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4925373134328358\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "#Subsetting 'X'\n",
    "all_text = data[[\"X\"]]\n",
    "\n",
    "#Converting 'X' to lower case\n",
    "all_text[\"X\"] = all_text['X'].str.lower()\n",
    "\n",
    "#Initialising a count vectorizer object\n",
    "cv = CountVectorizer()\n",
    "\n",
    "#Creating the count vectorizer of our 'X' column\n",
    "vector =cv.fit_transform(all_text[\"X\"])\n",
    "\n",
    "#Converting the count vectoriser to array\n",
    "X = vector.toarray()\n",
    "\n",
    "#Subsetting y\n",
    "labels = data[[\"y\"]]\n",
    "\n",
    "#Initialising a label encoder object\n",
    "le = LabelEncoder()\n",
    "\n",
    "#Label encoding 'y' column\n",
    "labels[\"y\"] = le.fit_transform(labels[\"y\"])\n",
    "\n",
    "#Splitting the dataset into train and test\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,labels[\"y\"],test_size=0.4,random_state=42)\n",
    "\n",
    "#Initialising Logistic Regression model\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "\n",
    "#Fitting the model on train data\n",
    "log_reg.fit(X_train,y_train)\n",
    "\n",
    "#Finding the accuracy score on test data\n",
    "acc = log_reg.score(X_test,y_test)\n",
    "print (acc)\n",
    "\n",
    "#Code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_U6-Wgb-h1Ib"
   },
   "source": [
    "## Removal of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1032,
     "status": "ok",
     "timestamp": 1565239275530,
     "user": {
      "displayName": "Bhushan Shewale",
      "photoUrl": "https://lh6.googleusercontent.com/-5a9j-ffSdSw/AAAAAAAAAAI/AAAAAAAAACw/ukiO53_VGNg/s64/photo.jpg",
      "userId": "17273113492750660774"
     },
     "user_tz": -330
    },
    "id": "FfuyBvvzeRIr",
    "outputId": "af21acda-39fe-4d05-9890-dc7a883c4dd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43283582089552236\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Initialising the tf-idf model\n",
    "tfidf = TfidfVectorizer(stop_words=\"english\")\n",
    "\n",
    "#Vectorizing the 'X' column\n",
    "vector =tfidf.fit_transform(all_text[\"X\"])\n",
    "\n",
    "#Converting the vector to array\n",
    "X_tfidf = vector.toarray()\n",
    "\n",
    "#Splitting the dataset into train and test\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_tfidf,labels[\"y\"],test_size=0.4,random_state=42)\n",
    "\n",
    "#Initialising the logistic regression model\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "\n",
    "#Fitting the model with train data\n",
    "log_reg.fit(X_train,y_train)\n",
    "\n",
    "#Finding the accuracy score of model on test data\n",
    "tfidf_acc = log_reg.score(X_test,y_test)\n",
    "print (tfidf_acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kqOyzjOciFux"
   },
   "source": [
    "## TF-IDF Vectorizer using naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2066,
     "status": "ok",
     "timestamp": 1565239448459,
     "user": {
      "displayName": "Bhushan Shewale",
      "photoUrl": "https://lh6.googleusercontent.com/-5a9j-ffSdSw/AAAAAAAAAAI/AAAAAAAAACw/ukiO53_VGNg/s64/photo.jpg",
      "userId": "17273113492750660774"
     },
     "user_tz": -330
    },
    "id": "-m_1wK_2epZw",
    "outputId": "bcd6b5e9-e796-4035-945e-a05c76cde2c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42461964038727523\n"
     ]
    }
   ],
   "source": [
    "# import packages \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# reading the data\n",
    "data = pd.read_csv(\"https://raw.githubusercontent.com/bhushan45/customer_complaints/master/customer_complaints_full.csv\")\n",
    "\n",
    "# keeping the relevant columns\n",
    "data = data[[\"Consumer complaint narrative\", \"Product\"]]\n",
    "\n",
    "# renaming the columns\n",
    "data.columns = [\"X\", \"y\"]\n",
    "\n",
    "# dropping the nan values\n",
    "data = data.dropna()\n",
    "\n",
    "\n",
    "# Subsetting 'X' column\n",
    "all_text = data[[\"X\"]]\n",
    "\n",
    "# Converting the 'X' column to lower case\n",
    "all_text[\"X\"] = all_text['X'].str.lower()\n",
    "\n",
    "# Initialising a tfidf vectorizer object with stopwords\n",
    "tfidf = TfidfVectorizer(stop_words=\"english\")\n",
    "\n",
    "# Vectorizing the 'X' column\n",
    "vector = tfidf.fit_transform(all_text[\"X\"])\n",
    "\n",
    "# Converting vector to array\n",
    "X_tfidf = vector.toarray()\n",
    "\n",
    "\n",
    "# Subsetting 'y' column\n",
    "labels = data[[\"y\"]]\n",
    "\n",
    "# Initialising label encoder object\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Label encoding 'y' column\n",
    "labels[\"y\"] = le.fit_transform(labels[\"y\"])\n",
    "\n",
    "# Splitting the data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_tfidf, labels[\"y\"], test_size=0.4, random_state=42)\n",
    "\n",
    "# Initialsing a naive bayes classifier\n",
    "nb = MultinomialNB()\n",
    "\n",
    "# Fitting the model on train data\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Finding the accuracy score of model on test data\n",
    "nb_acc = nb.score(X_test, y_test)\n",
    "print(nb_acc)\n",
    "\n",
    "#Code ends here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PD35Jx4pirW8"
   },
   "source": [
    "## Dealing with imbalaced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1388,
     "status": "ok",
     "timestamp": 1565239559111,
     "user": {
      "displayName": "Bhushan Shewale",
      "photoUrl": "https://lh6.googleusercontent.com/-5a9j-ffSdSw/AAAAAAAAAAI/AAAAAAAAACw/ukiO53_VGNg/s64/photo.jpg",
      "userId": "17273113492750660774"
     },
     "user_tz": -330
    },
    "id": "YQqDyvnzfD7R",
    "outputId": "12b5ce1d-003b-4073-d3cd-71a30f0cd2f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6016597510373444\n"
     ]
    }
   ],
   "source": [
    "# import package\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#Initialising a random over sampler object\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "\n",
    "#Sampling the train data\n",
    "X_ros, y_ros = ros.fit_sample(X_train, y_train)\n",
    "\n",
    "#Initialsing multinomial naive bayes model\n",
    "nb = MultinomialNB()\n",
    "\n",
    "#Fitting the sampled train data\n",
    "nb.fit(X_ros,y_ros)\n",
    "\n",
    "#Finding the accuracy score of model on test data\n",
    "ros_score=nb.score(X_test,y_test)\n",
    "print(ros_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "se-s_soGjFSD"
   },
   "source": [
    "## Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 144068,
     "status": "ok",
     "timestamp": 1565239722627,
     "user": {
      "displayName": "Bhushan Shewale",
      "photoUrl": "https://lh6.googleusercontent.com/-5a9j-ffSdSw/AAAAAAAAAAI/AAAAAAAAACw/ukiO53_VGNg/s64/photo.jpg",
      "userId": "17273113492750660774"
     },
     "user_tz": -330
    },
    "id": "f95QeyW3fJzA",
    "outputId": "e4053c8b-53a0-4600-80d6-b4bbe1cc63d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.648686030428769\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "#Code starts here\n",
    "\n",
    "#Initialising a support vector model with linear kernel\n",
    "svc = SVC(kernel=\"linear\", random_state=0)\n",
    "\n",
    "#Fitting the model on train data\n",
    "svc.fit(X_ros,y_ros)\n",
    "\n",
    "#Finding the accuracy score of the model on test data\n",
    "svc_score=svc.score(X_test,y_test)\n",
    "\n",
    "print(svc_score)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Introduction_to_nlp_code_along.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
