{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 align=\"center\">Feature Selection</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![feature_selection](../images/feature_selection.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Program so far\n",
    "\n",
    "***\n",
    "\n",
    "- Basics of Python\n",
    "- Descriptive and Inferential Statistics\n",
    "- Linear Regression\n",
    "- L1/L2 Regularization\n",
    "- Basic data cleaning and Preprocessing\n",
    "- Feature extraction and Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Table of contents\n",
    "***\n",
    "\n",
    "- Importance of Feature Selection\n",
    "    - Variance Threshold\n",
    "    - Pearson\n",
    "    - Select K Best\n",
    "    - f_regression\n",
    "    - RFE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**What is to be covered later?**\n",
    "\n",
    "\n",
    "- Filter Methods\n",
    "- Wrapper Methods\n",
    "- Embedded Methods\n",
    "- Difference between Filter and Wrapper methods\n",
    "- Walk-through example\n",
    "\n",
    "*****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## So what is Feature Selection?\n",
    "***\n",
    "* In machine learning and statistics, feature selection, also known as variable selection, attribute selection or variable subset selection, is the process of selecting a subset of relevant features (variables, predictors) for use in model construction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Importance of Feature Selection\n",
    "***\n",
    "* This becomes even more important when the number of features are very large.\n",
    "* You need not use every feature at your disposal for creating an algorithm. \n",
    "* You can assist your algorithm by feeding in only those features that are really important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Where to use feature selection?\n",
    "***\n",
    "- It enables the machine learning algorithm to train faster.\n",
    "- It reduces the complexity of a model and makes it easier to interpret.\n",
    "- It improves the accuracy of a model if the right subset is chosen.\n",
    "- It reduces overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Type of Feature Selection\n",
    "***\n",
    "Jay helped John identify different types of feature selection:-\n",
    "\n",
    "* Univariate Feature Selection\n",
    "* Multivariate Feature Selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/Technical-Stuff.png\" alt=\"Technical-Stuff\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br >\n",
    "\n",
    "## Univariate Feature Selection (1/2)\n",
    "***\n",
    "* Univariate feature selection methods examine -\n",
    "    - the predictive power of individual features\n",
    "    - the strength of the relationship of the feature with the response variable\n",
    "* These methods are simple to run and understand\n",
    "* They also prove to be good for gaining a better understanding of data, and can often be the starting point for multivariate feature selection methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Technical-Stuff.png\" alt=\"Technical-Stuff\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br >\n",
    "\n",
    "## Univariate Feature Selection (2/2)\n",
    "***\n",
    "* We can look at the interaction between the top variables instead of all possible combinations\n",
    "* These methods are not necessarily good for optimizing the feature set for better generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Method 1 : Removing features with low variance (1/2)\n",
    "***\n",
    "* One of the most basic, yet a very powerful feature selection technique\n",
    "* We want to remove all features whose variance doesn't meet some threshold\n",
    "* For example, a feature with 0 variance means it has the same value for every sample. This means that such a feature would not bring any predictive power to the model. \n",
    "* Hence, we should remove all such zero-variance features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "### Removing features with low variance (2/2)\n",
    "***\n",
    "* John thought, if he had a dataset with boolean features, and he wanted to remove all features that were either one or zero (on or off) in more than 80% of the samples. How would he do that?\n",
    "\n",
    "* Answer : Boolean features are Bernoulli random variables, and the variance of such variables is given by  Var(x)=p(1−p)\n",
    "\n",
    "* *from sklearn.feature_selection import VarianceThreshold*   is a handy method to remove such features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "X = [[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1]]\n",
    "sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "sel.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Method 2 : Pearson Correlation\n",
    "***\n",
    "* Another method that Jay suggested was Pearson Correlation\n",
    "* The population correlation coefficient  $ρ_{(X,Y)}$  between two random variables  X and  Y with expected values  $μ_X$  and  $μ_Y$  and standard deviations  $σ_X$ and  $σ_Y$.\n",
    "* It is used as a measure for quantifying linear dependence between two continuous variables X and Y. Its value varies from -1 to +1. Pearson’s correlation is given as:\n",
    "![FS3.JPG](../images/FS3.JPG)\n",
    "* where E is the expected value operator, cov means covariance, and corr is a widely used alternative notation for the correlation coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Pearson Correlation\n",
    "***\n",
    "* One of the simplest method for understanding a feature's relation to the response variable is Pearson correlation coefficient, which measures linear correlation between two variables\n",
    "* Scipy's Pearson's method computes both the correlation and p-value for the correlation, roughly showing the probability of an uncorrelated system creating a correlation value of this magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "iowa = pd.read_csv('../data/house_prices_multivariate.csv')\n",
    "\n",
    "X = iowa.iloc[:,:-1]\n",
    "y = iowa.iloc[:,-1]\n",
    "\n",
    "for i in X.columns:\n",
    "    print(pearsonr(X[i],y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Pearson Correlation\n",
    "***\n",
    "* With smaller amount of noise, the correlation is relatively strong, with a very low p-value\n",
    "* However, for the noisy comparison, the correlation is very small and p-value is very high\n",
    "* The p-value roughly indicates the probability of an uncorrelated system producing datasets that have a Pearson correlation at least as extreme as the one computed from these datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Pearson Correlation: \n",
    "***\n",
    "* Pearson correlation is the ratio of co-variance of two variables to a product of variance (of the variables)\n",
    "* The correlation coefficient has values between -1 to 1\n",
    "    - A value closer to 0 implies weaker correlation (exact 0 implying no correlation)\n",
    "    - A value closer to 1 implies stronger positive correlation\n",
    "    - A value closer to -1 implies stronger negative correlation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Pearson Correlation\n",
    "***\n",
    "**Correlation is not Transitive:**\n",
    "\n",
    "* Let  X,  Y and  Z be random variables.\n",
    "    - ρX,Y > 0.8\n",
    "    - ρY,Z > 0.7\n",
    "\n",
    "* Can we say  ρX,Z  will be strongly positive?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Pearson Correlation\n",
    "***\n",
    "**Correlation is not Transitive:**\n",
    "\n",
    "* Not really! \n",
    "* ρX,Z will be positive if ρX,Y and ρY,Z are very close to 1\n",
    "* Mathematically speaking:\n",
    "![FS4.JPG](../images/FS4.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Pearson Correlation\n",
    "***\n",
    "**Sensitivity to Outliers:**\n",
    "Once again:\n",
    "![FS5.JPG](../images/FS5.JPG)\n",
    "* Means themselves are sensitive to outliers\n",
    "* The correlation itself will be sensitive to outliers as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Pearson Correlation\n",
    "***\n",
    "**Correlation doesn't capture nonlinear relationships**\n",
    " \n",
    "* One obvious drawback of Pearson correlation as a feature ranking mechanism is that it is only sensitive to a linear relationship.\n",
    "* If the relation is non-linear, Pearson correlation can be close to zero even if there is a 1-1 correspondence between the two variables.\n",
    "* For example, correlation between x and $x_2$ is zero, when x is centered on 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Pearson Correlation\n",
    "***\n",
    "**Correlation doesn't capture nonlinear relationships**\n",
    " \n",
    "* In the image in the next slide, there are several sets of (x, y) points, with the Pearson correlation coefficient of x and y for each set.\n",
    "* Note that the correlation reflects the noisiness and direction of a linear relationship (top row), but not the slope of that relationship (middle), nor many aspects of nonlinear relationships (bottom).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Pearson Correlation\n",
    "***\n",
    "**Correlation doesn't capture nonlinear relationships**\n",
    "\n",
    "![FS6.JPG](../images/FS6.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Pearson Correlation\n",
    "*** \n",
    "![FS7.JPG](../images/FS7.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Now that John understood Pearson's Correlation, he wanted to get deeper into Feature Extraction in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/Technical-Stuff.png\" alt=\"Technical-Stuff\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br>\n",
    "\n",
    "## Sklearn for Feature Selection\n",
    "***\n",
    "* Scikit-learn has a wide variety of functions for eliminating features based information-criteria. \n",
    "* For performing univariate feature selection, we need to specify two parameters\n",
    "    - Selection criteria\n",
    "    - Metric to be used for selection\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Sklearn for Feature Selection**\n",
    "##### Selection Criteria\n",
    "* SelectKBest: Removes all but the k highest scoring features\n",
    "* SelectPercentile: Removes all but a user-specified highest scoring percentage of features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Sklearn for Feature Selection**\n",
    "##### Regression\n",
    "* **f_regression**: The methods based on F-test estimate the degree of linear dependency between two random variables.\n",
    "* **Mutual_info_regression**:Mutual information methods can capture any kind of statistical dependency, but being nonparametric, they require more samples for accurate estimation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "** Sklearn for Feature Selection**\n",
    "##### Selection Metric\n",
    "##### Classification\n",
    "\n",
    "* **chi2**: Based on Chi Squared\n",
    "* **F_classif**: Based on f-test\n",
    "* **Mutual_info_classif**: Based on Mutual Information Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's understand how f-test and mutual information behave differently. Let's start by creating a toy dataset, in which y depends on three components: a linear, a non-linear and a random.\n",
    "\n",
    "$y = x_1 + sin(6 * pi * x_2) + 0.1 * N(0, 1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(67)\n",
    "X = np.random.rand(1000, 3)\n",
    "y = X[:, 0] + np.sin(5* np.pi * X[:, 1]) + 0.1 * np.random.randn(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's calculate the importance scores of the features and normalize them so that they can be compared with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_regression, mutual_info_regression\n",
    "\n",
    "f_test, _ = f_regression(X, y)\n",
    "f_test /= np.max(f_test)\n",
    "\n",
    "mi = mutual_info_regression(X, y)\n",
    "mi /= np.max(mi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "f_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "mi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now, let's plot these values and compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "for i in range(3):\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "    plt.scatter(X[:, i], y)\n",
    "    plt.xlabel(\"$x_{}$\".format(i + 1), fontsize=14)\n",
    "    if i == 0:\n",
    "        plt.ylabel(\"$y$\", fontsize=14)\n",
    "    plt.title(\"F-test={:.2f}, MI={:.2f}\".format(f_test[i], mi[i]),\n",
    "              fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can clearly see how f-test can only capture the linear relationship, whereas MI can capture no-linear relationships as well. You can vary the coefficient of the `sin` component to examine how the MI behaves when the coefficient is made sufficiently low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Various methodologies and techniques  to subset  feature space and make models perform better and efficiently.\n",
    "***\n",
    "John got a good measure of feature selection and was more confident than ever on it. He started reading more on techniques to make his models more efficient. This is what he found out-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/Technical-Stuff.png\" alt=\"Technical-Stuff\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br>\n",
    "\n",
    "##  Filter Methods\n",
    "***\n",
    "![Filter_1.png](../images/Filter_1.png)\n",
    "[Source: Analytics Vidya Blog](https://www.analyticsvidhya.com/blog/2016/12/introduction-to-feature-selection-methods-with-an-example-or-how-to-select-the-right-variables/) \n",
    "\n",
    "* Filter methods are generally used as a preprocessing step. The selection of features is independent of any machine learning algorithms. \n",
    "* Instead, features are selected on the basis of their scores in various statistical tests for their correlation with the outcome variable. The correlation is a subjective term here.\n",
    "* Examples that we saw just now, are some of the filter mathod techniques.\n",
    "* For basic guidance, you can refer to the following table for defining correlation co-efficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Filter Methods\n",
    "\n",
    "![FS1.png](../images/FS1.png)\n",
    "[Source: An Analysis of Feature Selection Techniques](http://syllabus.cs.manchester.ac.uk/pgt/2017/COMP61011/goodProjects/Shardlow.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##  Filter Methods \n",
    "***\n",
    "**Pearson’s Correlation**: It is used as a measure for quantifying linear dependence between two continuous variables X and Y. Its value varies from -1 to +1. Pearson's correlation is given as:\n",
    "![FS2.png](../images/FS2.png)\n",
    "[Source: An Analysis of Feature Selection Techniques](http://syllabus.cs.manchester.ac.uk/pgt/2017/COMP61011/goodProjects/Shardlow.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(iowa.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##  Filter Methods\n",
    "***\n",
    "**LDA**: Linear discriminant analysis is used to find a linear combination of features that characterizes or separates two or more classes (or levels) of a categorical variable.\n",
    "\n",
    "[Source: An Analysis of Feature Selection Techniques](http://syllabus.cs.manchester.ac.uk/pgt/2017/COMP61011/goodProjects/Shardlow.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##  Filter Methods \n",
    "***\n",
    "**ANOVA**: ANOVA stands for Analysis of variance. It is similar to LDA except for the fact that it is operated using one or more categorical independent features and one continuous dependent feature. It provides a statistical test of whether the means of several groups are equal or not.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##  Filter Methods \n",
    "***\n",
    "**Chi-Square**: It is a is a statistical test applied to the groups of categorical features to evaluate the likelihood of correlation or association between them using their frequency distribution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Filter Methods vulnerability to Multicollinearity\n",
    "***\n",
    "One thing that should be kept in mind is that filter methods do not remove multicollinearity. So, one must deal with multicollinearity of features as well before training models for his/her data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-ALert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br >\n",
    "\n",
    "## Wrapper Methods\n",
    "***\n",
    "![Wrapper_1.png](../images/Wrapper_1.png)\n",
    "\n",
    "[Source: Analytics Vidya Blog](https://www.analyticsvidhya.com/blog/2016/12/introduction-to-feature-selection-methods-with-an-example-or-how-to-select-the-right-variables/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Wrapper Methods\n",
    "***\n",
    "* In wrapper methods, we try to use a subset of features and train a model using them.\n",
    "* Based on the inferences that we draw from the previous model, we decide to add or remove features from your subset.\n",
    "* The problem is essentially reduced to a search problem. These methods are usually computationally very expensive.\n",
    "* Some common examples of wrapper methods are forward feature selection, backward feature elimination, recursive feature elimination, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Wrapper Methods\n",
    "***\n",
    "**Forward Selection**: Forward selection is an iterative method in which we start with having no feature in the model. In each iteration, we keep adding the feature which best improves our model till an addition of a new variable does not improve the performance of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Wrapper Methods\n",
    "***\n",
    "**Backward Elimination**: In backward elimination, we start with all the features and removes the least significant feature at each iteration which improves the performance of the model. We repeat this until no improvement is observed on removal of features. This technique is also known as **Recursive Feature Elimination.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Recursive Feature Elimination (RFE)**\n",
    "\n",
    "* The Recursive Feature Elimination (RFE) method works by recursively removing attributes and building a model on those attributes that remain.\n",
    "* It uses an external estimator that assigns weights to features (for example, the coefficients of a linear model) to identify which attributes (and combination of attributes) contribute the most to predicting the target attribute.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "** Recursive Feature Elimination (RFE)**\n",
    "\n",
    "* It is a greedy optimization algorithm which aims to find the best performing feature subset.\n",
    "\n",
    "* It repeatedly creates models and keeps aside the best or the worst performing feature at each iteration. \n",
    "\n",
    "* It constructs the next model with the left features until all the features are exhausted. It then ranks the features based on the order of their elimination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Recursive Feature Elimination\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# create a base classifier used to evaluate a subset of attributes\n",
    "model = LinearRegression()\n",
    "\n",
    "X, y = iowa.iloc[:,:-1], iowa.iloc[:,-1]\n",
    "# create the RFE model and select 3 attributes\n",
    "rfe = RFE(model, 10)\n",
    "rfe = rfe.fit(X, y)\n",
    "\n",
    "# summarize the selection of the attributes\n",
    "print(rfe.support_)\n",
    "print(rfe.ranking_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-ALert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br>\n",
    "\n",
    "## Embedded Methods\n",
    "***\n",
    "![Embedded_1.png](../images/Embedded_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Embedded methods combine the qualities’ of filter and wrapper methods. It’s implemented by algorithms that have their own built-in feature selection methods.\n",
    "* Some of the most popular examples of these methods are LASSO and RIDGE regression which have inbuilt penalization functions to reduce overfitting.\n",
    "    - Lasso regression performs L1 regularization which adds penalty equivalent to absolute value of the magnitude of coefficients.\n",
    "    - Ridge regression performs L2 regularization which adds penalty equivalent to square of the magnitude of coefficients.\n",
    "* We already have learned how these algorithm works.Other examples of embedded methods are Regularized trees, Memetic algorithm, Random multinomial logit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Model Based Feature Selection**\n",
    "\n",
    "* Certain models can be used as feature selection mechanisms because their inner workings involves ordering or ranking of features.\n",
    "* We have already gone through such algorithms:\n",
    "* Brain Teaser:\n",
    "     - What are such algorithms?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Model Based Feature Selection**\n",
    "* Here are a few algorithms which help us select feature selection:\n",
    "    - L1 regularized linear regression\n",
    "    - Decision Trees\n",
    "    - Random Forests\n",
    "    - Gradient Boosting Machines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Difference between Filter and Wrapper methods \n",
    "***\n",
    "The main differences between the filter and wrapper methods for feature selection are:\n",
    "* Filter methods measure the relevance of features by their correlation with dependent variable while wrapper methods measure the usefulness of a subset of feature by actually training a model on it.\n",
    "* Filter methods are much faster compared to wrapper methods as they do not involve training the models. On the other hand, wrapper methods are computationally very expensive as well.\n",
    "* Filter methods use statistical methods for evaluation of a subset of features while wrapper methods use cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Difference between Filter and Wrapper methods \n",
    "***\n",
    "* Filter methods might fail to find the best subset of features in many occasions but wrapper methods can always provide the best subset of features.\n",
    "* Using the subset of features from the wrapper methods make the model more prone to overfitting as compared to using subset of features from the filter methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Good, Moving forward John asked one example with Iowa dataset ...\n",
    "\n",
    "So, Jay give an example with RandomForest ( about the model we will be learning later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Before');\n",
    "print(X.shape)\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X, y)\n",
    "\n",
    "print('\\nFeature Importance');\n",
    "print(clf.feature_importances_)\n",
    "\n",
    "model = SelectFromModel(clf, prefit=True)\n",
    "X_new = model.transform(X)\n",
    "\n",
    "print('\\nAfter'); \n",
    "print(X_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Build a forest and compute the feature importances\n",
    "forest = ExtraTreesClassifier(n_estimators=250,\n",
    "                              random_state=0)\n",
    "\n",
    "forest.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "importances = forest.feature_importances_\n",
    "importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\n",
    "std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "indices = np.argsort(importances)[::-1]\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X.shape[1]), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), indices)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Principal Component Analysis**, or **PCA**, is a dimensionality-reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. It transforms the original variables into a new set of variables such that they are orthogonal (and hence linearly independent) and then ranked according to the variance of data among them. These newly extracted variables are called Principal Components.\n",
    "\n",
    "* Principal components are extracted in such a way that the first principal component explains maximum variance in the dataset.\n",
    "* The second principal component(uncorrelated to the first) tries to explain the remaining variance(not explained by the first).\n",
    "* The third principal component explains the variance not explained by first and second and so on.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEPS INVOLVED IN PCA\n",
    "\n",
    "* Standardize the data and compute the Covariance Matrix\n",
    "* Compute the Eigenvectors and Eigenvalues of the Covariance Matrix \n",
    "* Sort the eigenvectors according to their eigenvalue (in decreasing order).\n",
    "* Use Eigenvectors corresponding to the (k)largest eigenvalues to reconstruct a large fraction of variance of the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In class Activity\n",
    "\n",
    "### About the Dataset\n",
    "\n",
    "The \"spam\" concept is diverse: advertisements for products/web sites, make money fast schemes, chain letters, pornography... \n",
    "\n",
    "Our collection of spam e-mails came from our postmaster and individuals who had filed spam. Our collection of non-spam e-mails came from filed work and personal e-mails, and hence the word 'george' and the area code '650' are indicators of non-spam. These are useful when constructing a personalized spam filter. One would either have to blind such non-spam indicators or get a very wide collection of non-spam to generate a general purpose spam filter. \n",
    "\n",
    "-  Number of Instances: 4601 (1813 Spam = 39.4%)\n",
    "-  Number of Attributes: 58 (57 continuous, 1 nominal class label)\n",
    "\n",
    " -  Attribute Information:\n",
    "\n",
    "    -  The last column of 'spambase.data' denotes whether the e-mail was \n",
    "       considered spam (1) or not (0)\n",
    "    \n",
    "    - 48 attributes are continuous real [0,100] numbers of type `word freq WORD` i.e. percentage of words in the e-mail that         match WORD\n",
    "\n",
    "    - 6 attributes are continuous real [0,100] numbers of type `char freq CHAR` i.e. percentage of characters in the e-mail           that match CHAR\n",
    "    \n",
    "    - 1 attribute is continuous real [1,...] numbers of type `capital run length average` i.e. average length of uninterrupted       sequences of capital letters\n",
    "\n",
    "    - 1 attribute is continuous integer [1,...] numbers of type `capital run length longest` i.e. length of longest                   uninterrupted sequence of capital letters\n",
    "\n",
    "    - 1 attribute is continuous integer [1,...] numbers of type `capital run length total` i.e. sum of length of uninterrupted       sequences of capital letters in the email\n",
    "\n",
    "    - 1 attribute is nominal {0,1} class  of type spam i.e  denotes whether the e-mail was considered spam (1) or not (0),  \n",
    "\n",
    "- Missing Attribute Values: None\n",
    "\n",
    "- Class Distribution:\n",
    "\tSpam\t  1813  (39.4%)\n",
    "\tNon-Spam  2788  (60.6%)\n",
    "\n",
    "\n",
    "\n",
    "You can read more about dataset [here](https://archive.ics.uci.edu/ml/datasets/spambase)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the Spam data for the mini challenge\n",
    "#Target variable is the 57 column i.e spam, non-spam classes \n",
    "df = pd.read_csv('../data/spambase.data.csv',header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get an overview of your data by using info() and describe() functions of pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overview of the data\n",
    "\n",
    "# Code starts here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Split the data into train and test set and fit the base logistic regression model on train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividing the dataset set in train and test set and apply base logistic model\n",
    "\n",
    "# Code starts here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Find out the accuracy , print out the Classification report and Confusion Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy , print out the Classification report and Confusion Matrix.\n",
    "\n",
    "# Code starts here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Copy dataset df into df1 variable and apply correlation on df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy df in new variable df1\n",
    "df1 = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. As we have learned  one of the assumptions of Logistic Regression model is that the independent features should not be correlated to each other(i.e Multicollinearity), So we have to find the features that have a correlation higher that 0.75 and remove the same so that the assumption for logistic regression model is satisfied. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Correlated features above 0.75 and then apply logistic model\n",
    "\n",
    "# Code starts here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Split the  new subset of the  data acquired by feature selection into train and test set and fit the logistic regression model on train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the new subset of data and fit the logistic model on training data\n",
    "\n",
    "# Code starts here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Find out the accuracy , print out the Classification report and Confusion Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy , print out the Classification report and Confusion Matrix for new data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. After keeping highly correlated features, there is not much change in the score. Lets apply another feature selection technique(Chi Squared test) to see whether we can increase our score. Find the optimum number of features using Chi Square and fit the logistic model on train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Chi Square and fit the logistic model on train data use df dataset\n",
    "nof_list=[20,25,30,35,40,50,55]\n",
    "high_score=0\n",
    "nof=0\n",
    "\n",
    "# Code starts here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Find out the accuracy , print out the Confusion Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy , print out the Confusion Matrix \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Using chi squared test there is no change in the score and the optimum features that we got is 55. Now lets see if we can increase our score using another feature selection technique called Anova.Find the optimum number of features using Anova and fit the logistic model on train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Anova and fit the logistic model on train data use df dataset\n",
    "nof_list=[20,25,30,35,40,50,55]\n",
    "high_score=0\n",
    "nof=0\n",
    "\n",
    "# Code starts here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Find out the accuracy , print out the Confusion Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy , print out the Confusion Matrix \n",
    "\n",
    "\n",
    "# Code starts here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Unfortunately Anova also couldn't give us a better score . Let's finally attempt PCA on train data and find if it helps in  giving a better model by reducing the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA and fit the logistic model on train data use df dataset\n",
    "nof_list=[20,25,30,35,40,50,55]\n",
    "high_score=0\n",
    "nof=0\n",
    "\n",
    "# Code starts here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Find out the accuracy , print out the Confusion Matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy , print out the Confusion Matrix \n",
    "\n",
    "# Code starts here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. You can also compare your predicted values and observed values by printing out values of logistic.predict(X_test[]) and  y_test[].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare observed value and Predicted value\n",
    "\n",
    "# Code starts here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/Recap.png\" alt=\"Recap\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br>\n",
    "\n",
    "# In-session Recap Time\n",
    "***\n",
    "* Feature Selection\n",
    "* Univariate Feature Selection\n",
    "* Multivariate Feature Selection\n",
    "* Filter Methods\n",
    "* Wrapper Methods\n",
    "* Embedded Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Thank You\n",
    "***\n",
    "### Next Session: Decision Trees"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
