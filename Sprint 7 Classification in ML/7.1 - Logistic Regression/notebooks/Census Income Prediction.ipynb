{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 align=\"center\"> Logistic Regression</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Logistic Regression is traced back to about 2000 years! \n",
    "***\n",
    "<center><img src=../images/meme_jesus.jpg alt=\"Drawing\" style=\"width: 300px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Program so far\n",
    "***\n",
    " - We learnt about different types of Machine Learning Problems\n",
    "\n",
    " - We learnt how to fit a linear model to our data to predict House Prices in New York\n",
    " \n",
    " - We learnt how to use \"Advanced Linear Regression\" techniques where we prevented overfitting by using\n",
    " \n",
    "      - **Lasso Regression: which performs the L1 Regularization technique**\n",
    "      \n",
    "      - **Ridge Regression: which performs the L2 Regularization technique**\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Agenda for the Day\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## John's Dilemma\n",
    "***\n",
    "Finally, John had decided on a house to buy. This was the perfect house for him in Brooklyn\n",
    "\n",
    "But he was running lower than expected on his savings and thus, didn't have enough money to buy a new one\n",
    "\n",
    "**So, he thought of taking a loan.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## John's Dilemma \n",
    "***\n",
    " - But taking a loan in New York City, at high interest rates, would be too expensive for him in the long run! \n",
    "\n",
    "- Besides, he wasn't sure whether he would get the required loan.\n",
    "\n",
    "- So, he started looking for current trends of the loan approval.\n",
    "\n",
    " - Luckily he stumbled upon a dataset which had details about the loan approval.\n",
    "Let's explore it, along with him."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br />\n",
    "\n",
    "## What type of Problem is this? \n",
    "***\n",
    " - Let's try and gain some intuition\n",
    " - The graph on the next slide depicts if a patient's Tumor is Malignant (Cancerous) or not **{1: Yes; 0: No}** \n",
    " based on the Tumor Size present in his/her body\n",
    " \n",
    " - Tumor Size is taken on the *x-axis* whereas the outcome, i.e., whether the patient's Tumor is Malginant or not is taken on the *Y-axis*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_samples=10, n_features=1, n_informative=1, n_redundant=0 , n_clusters_per_class=1, flip_y=0, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(X, y, c='r', marker='x')\n",
    "plt.ylabel(\"Malignant Tumor {1: Yes  0: No}\")\n",
    "plt.xlabel(\"Tumor Size\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br />\n",
    "\n",
    "## What type of Problem is this? \n",
    "***\n",
    " - The output, \"y\" has two **categories** i.e. 1 (Yes) or 0 (No) \n",
    " \n",
    " - Thus, as we have learnt before, this is a **Classification Problem** where we are using our dependent variables, in this case Tumor Size, and are getting a **binary** output, 1 (Yes) or 0 (No)! \n",
    " \n",
    " - Let's approach this problem with what we've learnt so far"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## John's Problem Explained\n",
    "***\n",
    " - Through the data set John collected, he basically wanted to find out that using different characteristics, do loans get approved {1: Yes} or not {0: No}\n",
    " \n",
    " - This is, in essence, nothing but a classification problem. John collected a data set that described what type of loan requests got approved and rejected\n",
    " \n",
    " - He wanted to check, before applying, whether his request would be approved after he fit the **right model** on the data! \n",
    " \n",
    " - Let's build our intution on the '*model building*' part using the Tumor data! You will see how comprehensive this approach is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Using Linear Regression to solve a classification problem\n",
    "***\n",
    "\n",
    "* Lets try and apply the same Linear Regression model we learned to solve the problem of classification on a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "z = np.linspace(-2, 2, 1000)\n",
    "z_predict = lm.intercept_ + (lm.coef_ * z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## So what now? Intuition behind the Decision Boundary\n",
    "***\n",
    "* We have fitted a linear regression model which is represented by the blue line\n",
    "* How do we convert a continuous output into a discrete one?\n",
    "* One way we can do is using a threshold value for the linear regression output\n",
    "\n",
    "\n",
    "* Our output is either 1 or 0, and since we can even get predicted values between 0 and 1 like 0.2, 0.6, etc. (The regression line), we need to come up with a method where our output (0.2, 0.6, etc) is **transformed** to either **0** or **1**\n",
    "\n",
    "\n",
    "* A reasonable threshold to keep for a 0-1 problem is 0.5. So, if our predicted value (y) is **greater than 0.5** then we assign a **\"1\"** to it \n",
    "* If y is **less than 0.5** then we assign a **\"0\"** to it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Boundary\n",
    "***\n",
    "\n",
    "* Tracing the line of y = 0.5 all the way to it's corresponding x - value (See graph below) we can get to know this \"Threshold\" value\n",
    "\n",
    "* This \"Threshold Value\" is x = __\n",
    "\n",
    "* This means that if x > _ then y > 0.5 (i.e. y = 1) \n",
    "\n",
    "* If x < _ then y < 0.5 (i.e. y = 0)\n",
    "\n",
    "* Therefore, this vertical line (x = _ ) which separates the 2 classes (y = 1 & 0) ---> **Decision Boundary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the threshold, x_critical\n",
    "\n",
    "# Your code here\n",
    "\n",
    "\n",
    "# Code ends here\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(X, y, c='r', marker='x')\n",
    "plt.plot(z, z_predict)\n",
    "plt.axvline(x=x_critical, color='r', linestyle='--')\n",
    "plt.axhline(y=0.5, color='g', linestyle='--')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## But wait! Sean Bean has something to say. . .\n",
    "***\n",
    "\n",
    "<center><img src=../images/meme_lotr.png alt=\"LOTR\" style=\"width: 400px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Linear Regression - Not a Good Idea\n",
    "***\n",
    "* It turns out, that Sean Ben is right! \n",
    "\n",
    "\n",
    "* The problem with this approach is that Linear Regression isn't robust to outliers and it changes the fit drastically in the presence of an outlier\n",
    "\n",
    "\n",
    "* We'll add an outlier and keep the same threshold as before to see the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "new_obs = 20\n",
    "X = np.vstack([X, new_obs])\n",
    "y = np.append(y, 1)\n",
    "lm = LinearRegression()\n",
    "lm.fit(X, y)\n",
    "z = np.linspace(-2, new_obs, 1000)\n",
    "z_predict = lm.intercept_ + (lm.coef_ * z)\n",
    "x_critical2 = (0.5 - lm.intercept_)/ lm.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(X, y, c='r', marker='x')\n",
    "plt.plot(z, z_predict)\n",
    "plt.axvline(x=x_critical, color='r', linestyle='--')\n",
    "plt.axvline(x=x_critical2, color='y', linestyle='--')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Regression - Not a Good Idea \n",
    "***\n",
    "* We see that our Decision Boundary has shifted to the right. As a result we get some incorrect predicted \"y-values\" (the one's left of the boundary) \n",
    "\n",
    "* Since the data is labeled we know that they're originally = \"1\". But being on the left of the decision boundary indicates that they should be assigned a \"0\"\n",
    "\n",
    "* Thus Linear Regression is not suitable for classification tasks as it is highly sensitive to outliers\n",
    "\n",
    "* We clearly need a better model to solve the problem of classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Enter Logistic Regression\n",
    "***\n",
    "* To overcome the limitations of Linear Regression we'll use another model suitable for classification i.e., Logistic Regression.\n",
    "\n",
    "\n",
    "* What do we need? \n",
    "  - We need 0 <= y <= 1\n",
    "  \n",
    "* For this purpose we need to use a different function (and not the Linear Regression \"y = mx + b\")\n",
    "\n",
    "* This function is the **Sigmoid Function**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Enter Logistic Regression \n",
    "***\n",
    "<center><img src=../images/meme_pratt.jpg alt=\"Oh Chris\" style=\"width: 400px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is the Sigmoid function?\n",
    "***\n",
    "* We need to get the values in the range of (0,1) \n",
    "\n",
    "* Let's have a look at the graph of the Sigmoid Function and then get into building the intuition\n",
    "***\n",
    "<center><img src=../images/sigmoid.png alt=\"Oh Chris\" style=\"width: 275px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/Maths-Insight.png\" alt=\"Maths-Insight\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br />\n",
    "\n",
    "## Intuition\n",
    "***\n",
    " - The function above is $g(z) = \\frac {1} {(1 + e^(-z))}$   for values of t ranging from - infinity , + infinity\n",
    " \n",
    " - $g(z)$ is also called as \"Sigmoid of 'z' or just sig(z)\"\n",
    " \n",
    " - Again, let's start by plugging in the extremes of it's range\n",
    " \n",
    " $g(-infinity) = \\frac {1} {(1 + infinity)} = ~0$\n",
    " \n",
    " \n",
    "   $g( infinity) = \\frac {1} {(1 + ~0)} = ~1$\n",
    " \n",
    " - No matter what value \"z\" takes, we will always get an answer in the range of **[0,1]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Technical-Stuff.png\" alt=\"Technical-Stuff\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br />\n",
    "\n",
    "## Simple Modifications\n",
    "***\n",
    "\n",
    " - Now, instead of sig(z) let's consider sig(y) **where y ---> our response**\n",
    " \n",
    " - But as discussed before; $y = \\theta_0 + \\theta_1(x1) + \\theta_2(x_2) + .. + \\theta_n(x_n)$\n",
    " \n",
    " - Thus, $g(y) = g(h_{\\theta}(x)) = 1/(1 + e^{-(\\theta* X)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Technical-Stuff.png\" alt=\"Technical-Stuff\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br />\n",
    "\n",
    "## Interpretation - Let's put it simply\n",
    "***\n",
    "In essence: \n",
    "\n",
    " - $h_θ(x)$ is the ** Estimated Probability that y = 1 on input \"X\"** \n",
    " \n",
    " - Example: $h_θ(x)$ = 0.7 in our Tumor problem **MEANS** there is a 70% chance of Tumor being **Malignant**\n",
    " \n",
    " - Thus, mathematically, $h_θ(x)$ = P(y=1 | x; θ) means that **Probability that that y = 1 given X is parameterized by θ**\n",
    " \n",
    " - Thus **P(y=1 | x;θ) + P(y=0 | x;θ) = 1**\n",
    "\n"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Technical-Stuff.png\" alt=\"Technical-Stuff\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br />\n",
    "\n",
    "## What is the odds ratio?\n",
    "***\n",
    "We will answer this question with the help of an example. Let the probability of success of an event is $p (0 <= p <= 10<=p<=1)$. So, the probability of event failure is 1 - p. The ratio of the probability of success to the probability of failure is called the odds ratio. Mathematically, it is equivalent to $\\frac{p}{1-p} $.\n",
    "If some event has odds of 4, then it means that the chances of success are 4 times more likely than those of failure.\n",
    "\n",
    "An interesting property of odds is that it is a monotonically increasing function. Odds increase as the probability increases or vice versa.\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Technical-Stuff.png\" alt=\"Technical-Stuff\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br />\n",
    "\n",
    "## Log odds\n",
    "***\n",
    "The transformation from odds to log of odds is the log transformation. This is also a monotonic transformation i.e. greater the odds, greater the log of odds and vice versa.\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Technical-Stuff.png\" alt=\"Technical-Stuff\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br />\n",
    "\n",
    "## Why take log odds?\n",
    "***\n",
    "It's difficult to model a variable with probability since it has a restricted range between 0 and 1. The log of the odds also called logit transformation is an attempt to get around the restricted range problem. It maps probability ranging between 0 and 1 to log-odds ranging from negative infinity to positive infinity.\n",
    "\n",
    "A logistic regression model allows us to establish a relationship between a binary outcome variable and a group of predictor variables. It models the logit-transformed probability as a linear relationship with the predictor variables. Mathematically,\n",
    "\n",
    "$$\\text{logit(p)} = log(\\frac{p}{1-p}) = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + …. + \\theta_nx_n$$\n",
    "\n",
    "\n",
    "Observe how the right-hand side of the equation looks similar to the linear regression counterpart.\n",
    "\n",
    "$$p = \\frac{e^{\\theta_0 + \\theta_1x_1 + \\theta_2x_2 + …. + \\theta_nx_n}}{1 + e^{\\theta_0 + \\theta_1x_1 + \\theta_2x_2 + …. + \\theta_nx_n}}$$\n",
    "\n",
    "$$p = \\frac{1}{1 + e^-({\\theta_0 + \\theta_1x_1 + \\theta_2x_2 + …. + \\theta_nx_n})}$$\n",
    " \n",
    "which is the sigmoid function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Technical-Stuff.png\" alt=\"Technical-Stuff\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br />\n",
    "\n",
    "## Decision Boundary Intuition with Examples\n",
    "***\n",
    "\n",
    "We can see that g(z) >= 0.5 when z >= 0\n",
    "\n",
    "Thus $h_θ(x)$ = g(θ*X)) >= 0.5 whenever θ x $X$ >= 0\n",
    "\n",
    "<center><img src=../images/sigmoid.png alt=\"Oh Chris\" style=\"width: 275px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " - Consider: hθ(x) = g(θ0 + θ1x1 + θ2x2)\n",
    "\n",
    " - The graph below shows y - values (0 when \"O\" & 1 when \"X\") given values of x1 & x2\n",
    "***\n",
    "<center><img src=../images/Image[4].png alt=\"Math\" style=\"width: 275px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So, for example  $θ_0 = -3, θ_1 = 1, θ_2 = 1$\n",
    "     \n",
    "So our parameter vector is a column vector with the above values\n",
    "   - So, θT  = [-3,1,1]\n",
    "   \n",
    "Thus the z here becomes θ x $X$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Technical-Stuff.png\" alt=\"Technical-Stuff\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br />\n",
    "\n",
    "## Diving Deeper\n",
    "***\n",
    "We predict \"y = 1\" if\n",
    "  - -3x0 + 1x1 + 1x2 >= 0\n",
    "  - -3 + x1 + x2 >= 0\n",
    "  \n",
    "We can also re-write this as\n",
    " - If (x1 + x2 >= 3) then we predict y = 1\n",
    " - If we plot x1 + x2 = 3 we graphically plot our decision boundary (See next slide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Technical-Stuff.png\" alt=\"Technical-Stuff\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br />\n",
    "\n",
    "## Diving Deeper\n",
    "***\n",
    "<center><img src=../images/Image[5].png alt=\"Math\" style=\"width: 275px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Technical-Stuff.png\" alt=\"Technical-Stuff\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br />\n",
    "\n",
    "## Diving Deeper\n",
    "***\n",
    "We have two regions on the graph\n",
    " - Blue = false\n",
    " - Magenta = true\n",
    "\n",
    "**Line = Decision Boundary**\n",
    "\n",
    " - The straight line is the set of points where hθ(x) = 0.5 exactly\n",
    " - The decision boundary is a property of the hypothesis\n",
    " - Means we can create the boundary with the hypothesis and parameters without any data\n",
    "\n",
    "Later, we use the data to determine the parameter values\n",
    "\n",
    "i.e. y = 1 if\n",
    "\n",
    "5 - x1 > 0\n",
    "\n",
    "5 > x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Technical-Stuff.png\" alt=\"Technical-Stuff\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br />\n",
    "\n",
    "## Non - Linear Decision Boundaries\n",
    "***\n",
    "We can also use Logistic Regression to fit a complex non-linear data set\n",
    "\n",
    " - Like polynomial regression: add higher order terms\n",
    "\n",
    "So say we have\n",
    " - $hθ(x) = g(θ_0 + θ_1x_1+ θ_2x_2 + θ_3x_1^2 + θ_4x_2^2)$\n",
    " \n",
    " - We take the transpose of the θ vector times the input vector \n",
    "\n",
    "Say θ was [-1,0,0,1,1] then we say;\n",
    " - Predict that \"y = 1\" if\n",
    " - $-1 + x_1^2 + x_2^2 >= 0$ or\n",
    " - $x_1^2 + x_2^2 >= 1$\n",
    " \n",
    "If we plot  \n",
    "$$x_1^2 + x_2^2 = 1$$\n",
    " \n",
    " - This gives us a circle with a radius of 1 around 0 (see next slide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Technical-Stuff.png\" alt=\"Technical-Stuff\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br />\n",
    "\n",
    "## Non - Linear Decision Boundaries\n",
    "***\n",
    "<center><img src=../images/Image[6].png alt=\"Math\" style=\"width: 275px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Non - Linear Decision Boundaries\n",
    "***\n",
    " -  We can build more complex decision boundaries by fitting complex parameters to this (relatively) simple hypothesis\n",
    " - More complex decision boundaries?\n",
    "      - By using higher order polynomial terms, we can get even more complex decision boundaries\n",
    "***      \n",
    " <center><img src=../images/Image[7].png alt=\"Math\" style=\"width: 200px;\"/></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "\n",
    "X, y = make_classification(n_samples=10, n_features=1, n_informative=1, n_redundant=0 , n_clusters_per_class=1, flip_y=0, random_state=7)\n",
    "\n",
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(X, y, c='r', marker='x')\n",
    "xt = np.linspace(-3, 3, 1000).reshape(1000,1)\n",
    "yt = clf.predict(xt)\n",
    "plt.plot(xt, yt)\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* In the case with no outliers Logistic Regression does a good job in seperating the classes\n",
    "* We'll add an outlier and check if it still manages to do the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "new_obs = 20\n",
    "X = np.vstack([X, new_obs])\n",
    "y = np.append(y, 1)\n",
    "\n",
    "# fit the Logistics Regression model\n",
    "\n",
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the values\n",
    "\n",
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Even after adding outliers LR seperates the classes well.\n",
    "* Thus its much more suitable for classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## John's Approach\n",
    "***\n",
    "- After learning this, John wanted to check how his data set could use the same techniques\n",
    "- He wanted further insight to what his data set looks like and how he would go about implementing this \n",
    "- He thought it would be smart if he split his data set into a *Training Set* and *Test Set* \n",
    "\n",
    "    - I'll leave it to you to figure out why he thought this would be appropriate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv('../data/loan_prediction.csv')\n",
    "X = dataframe.iloc[:,:-1]\n",
    "y = dataframe.iloc[:,-1]\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.3)\n",
    "logistic_regressor = LogisticRegression()\n",
    "pipeline = Pipeline(steps=[('add_poly_features', PolynomialFeatures()),\n",
    "                           ('logistic_regression', logistic_regressor)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Fit & Predict the model\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print (accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hyperparameter Tuning\n",
    "***\n",
    "* We saw while discussing regularization that by changing the value of 𝛂, we can control the bias-variance trade-off.<br>\n",
    "\n",
    "* Similarly there are many such parameters, by controlling and changing whom we can fine tune performance of a model.\n",
    "\n",
    "* Such parameters are called hyperparameters and the act of controlling and changing them to fine tune the performance of a model is called hyperparameter tuning.\n",
    "\n",
    "*  *Hyperparameters are parameters whose values are set prior to the commencement of the learning process.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Hyperparameter Tuning\n",
    "***\n",
    "* By contrast, the value of other parameters is derived via training.\n",
    "\n",
    "\n",
    "* Hyperparameter optimization or model selection is the problem of choosing a set of optimal hyperparameters for a learning algorithm, usually with the goal of optimizing a measure of the algorithm's performance on an independent data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Hyperparameter Tuning\n",
    "***\n",
    "So, to summarize Hyperparameters: \n",
    "\n",
    "* Define higher level concepts about the model such as complexity, or capacity to learn.\n",
    "* Cannot be learned directly from the data in the standard model training process and need to be predefined.\n",
    "* Can be decided by setting different values, training different models, and choosing the values that test better.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Hyperparameter Tuning\n",
    "***\n",
    "Some examples of hyperparameters: \n",
    "\n",
    "\n",
    "* Number of leaves or depth of a tree.\n",
    "* Number of latent factors in a matrix factorization.\n",
    "* Learning rate (in many models).\n",
    "* Number of hidden layers in a deep neural network.\n",
    "* Number of clusters in a k-means clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Overview of Methods of Hyperparamter Tuning\n",
    "***\n",
    "* Grid Search\n",
    "* Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Grid Search\n",
    "***\n",
    "* Grid search is an approach to parameter tuning that will methodically build and evaluate a model for each combination of algorithm parameters specified in a grid.\n",
    "\n",
    "The grid is generally provided as a parameter.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Grid Search\n",
    "***\n",
    "* Let's see how we can perform the gridsearch using sklearn.\n",
    "* Here, first we decide which parameters we check using grid search, and also decide the values at which we want to carry out the search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Defining the parameters in `sklearn`\n",
    "***\n",
    "* We can specify the parameters for a particular element in the pipeline by concatenating the parameter name with element name using a double underscore (\"__\")\n",
    "* For a single element, just specify parameter and values\n",
    "* See an example below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV,cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "## For a single object\n",
    "ridge = Ridge()\n",
    "params = {\"alpha\": [1, 2, 5, 10],\n",
    "          \"fit_intercept\": [True, False]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## For a pipeline object\n",
    "pipeline = Pipeline(steps=[('add_poly_features', PolynomialFeatures()),\n",
    "                           ('logistic_regression', logistic_regressor)])\n",
    "\n",
    "## Defining the parametrs grid\n",
    "params = {'add_poly_features__degree':[2,3,4,5],\n",
    "          'logistic_regression__penalty':['l1','l2']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Using `gridsearch` in `sklearn`\n",
    "***\n",
    "* After defining the object and the parameters grid, pass them in the  `GridSearchCV` and let sklearn take over!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(estimator=pipeline, param_grid=params)\n",
    "grid_search.fit(X_train,y_train)\n",
    "y_prediction = grid_search.predict(X_test)\n",
    "\n",
    "cross_val_score(grid_search.best_estimator_,X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Random Search\n",
    "***\n",
    "* Random search is an approach to parameter tuning that will sample algorithm parameters from a random distribution (i.e. uniform) for a fixed number of iterations.\n",
    "* A model is constructed and evaluated for each combination of parameters chosen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Using Random Search in `sklearn` \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "params = {'add_poly_features__degree': range(0,4),\n",
    "          'logistic_regression__penalty':['l1','l2']}\n",
    "\n",
    "rand_search = RandomizedSearchCV(estimator = pipeline,param_distributions = params, n_iter=5)\n",
    "rand_search.fit(X_train,y_train)\n",
    "y_prediction = rand_search.predict(X_test)\n",
    "\n",
    "cross_val_score(rand_search.best_estimator_,X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Changing Tracks for a bit\n",
    "***\n",
    "- Now that we're getting a better idea of Logistic Regression, we need to understand what is happening behind the scenes! \n",
    "\n",
    "- How are we getting correct predictions with the help of the sigmoid function? \n",
    "\n",
    "- As you might have guessed, like Linear Regression, this has a lot to do with our Paramters (thetas) AND how we get those optimal parameters (computationally)\n",
    "\n",
    "- Getting those optimal parameters is directly related to the Cost Function!! Let's get straight to it! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression - Cost Function and Gradient Descent\n",
    "***\n",
    "- Till now we studied the intuition behind the Sigmoid Function\n",
    "\n",
    "- We also studied how Logistic Regression works to get outputs in the range of [0,1]\n",
    "\n",
    "- We discussed the interpretation of the output too! \n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> Now, like Linear Regression let's delve deeper into the intuition behind the Cost Function and how we apply Gradient Descent to make things work in Logistic Regression. It's actually quite clever!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cost Function \n",
    "***\n",
    " - Fit θ parameters\n",
    " - Define the optimization object for the cost function we use the fit the parameters\n",
    "     - Training set consists of **\"m\"** training examples\n",
    "         - Each example has is **n+1** length column vector\n",
    "***      \n",
    " <center><img src=../images/Image[8].png alt=\"Math\" style=\"width: 350px;\"/></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Cost Function\n",
    "***\n",
    "* This is the situation: \n",
    "  - Set of m training examples\n",
    "  - Each example is a feature vector which is n+1 dimensional\n",
    "  - x0 = 1\n",
    "  - y ∈ {0,1}\n",
    "  - Hypothesis is based on parameters (θ)\n",
    "      - **Given the training set how to we chose/fit θ?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Cost Function\n",
    "***\n",
    " - For Linear Regression we use the following Cost Function to determine optimal θ's\n",
    " ***      \n",
    " <center><img src=../images/Image[9].png alt=\"Math\" style=\"width: 250px;\"/></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Cost Function\n",
    "***\n",
    "- Instead of writing the squared error term, we can write; if we define \"cost()\", \n",
    "Which evaluates to the cost for an individual example using the same measure as used in linear regression\n",
    "\n",
    " - We can redefine J(θ) as\n",
    "\n",
    "***      \n",
    " <center><img src=../images/Image[10].png alt=\"Math\" style=\"width: 250px;\"/></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Maths-Insight.png\" alt=\"Maths-Insight\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br />\n",
    "\n",
    "## Cost Function\n",
    "***\n",
    " - What does this actually mean?\n",
    "   - This is the cost you want the learning algorithm to pay if the **outcome is hθ(x)** and the *actual outcome is y*\n",
    "   \n",
    " - What else do we need to consider?\n",
    "   - We would like a **convex function** so if you run gradient descent you converge to a **global minimum**\n",
    "   \n",
    " - Why? \n",
    "   - If we just use the Sigmoid function as is in the cost function on the previous slide, it's a **NON-CONVEX** function \n",
    " \n",
    " - What do you mean by Non-Convex? \n",
    "   - Our hypothesis function has a non-linearity (sigmoid function of hθ(x) )\n",
    "   - This is a non-linear function\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Maths-Insight.png\" alt=\"Maths-Insight\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br />\n",
    "\n",
    "## Cost Function - Local Optima vs Global Optima\n",
    "***\n",
    "Since it's non-convex:\n",
    "\n",
    " - If you take hθ(x) and plug it into the Cost() function, and them plug the Cost() function into J(θ) and plot J(θ) we find many **local optimum** -> non convex function\n",
    " \n",
    " - Why is this a problem?\n",
    "     - Lots of **local minima** mean gradient descent may not find the **global optimum** - may get stuck in a global minimum\n",
    "     - We would like a convex function so if you run gradient descent you converge to a global minimum\n",
    "***      \n",
    " <center><img src=../images/local-optima.png alt=\"Math\" style=\"width: 500px;\"/></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Maths-Insight.png\" alt=\"Maths-Insight\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br />\n",
    "\n",
    "## A Convex Cost Function \n",
    "***\n",
    " - To get around this we need a different, convex Cost() function which means we can apply gradient descent\n",
    " - This is our Logistic Regression Cost Function: \n",
    " ***\n",
    " <center><img src=\"../images/Images[12].jpg\" alt=\"Math\" style=\"width: 500px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " - Let's consider y = 1 and plot the fucntion\n",
    "  ***  \n",
    " <center><img src=../images/Image[13].png alt=\"Math\" style=\"width: 250px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Intuition - Convex Cost Function\n",
    "***\n",
    " - X axis is what we predict\n",
    " \n",
    " - **Y axis is the cost associated with that prediction**\n",
    " \n",
    "So when we're right, i.e when the predicted and actual y = 1 \n",
    " - cost function is 0\n",
    " \n",
    "Else it slowly increases cost function as we become \"more\" wrong\n",
    "\n",
    "This cost functions has some interesting properties:\n",
    "\n",
    " - If y = 1 and hθ(x) = 1\n",
    "If hypothesis predicts exactly 1 and thats exactly correct then that corresponds to 0 (exactly, not nearly 0)\n",
    "\n",
    "As hθ(x) goes to 0\n",
    " - Cost goes to infinity\n",
    "\n",
    "This captures the intuition that if hθ(x) = 0 (predict P (y=1|x; θ) = 0) but y = 1 this will penalize the learning algorithm with a **massive cost**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Intuition - Convex Cost Function\n",
    "***\n",
    "Let's plot the cost function when y = 0\n",
    "***\n",
    " <center><img src=../images/Image[14].png alt=\"Math\" style=\"width: 250px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Maths-Insight.png\" alt=\"Maths-Insight\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br />\n",
    "\n",
    "## Intuition - Convex Cost Function\n",
    "***\n",
    " - When y = 0, we know that the cost is  -log(1- hθ( x ))\n",
    " \n",
    " \n",
    " - In essence, it's the inverese of the cost when y = 1. We can see that graphically [previous slide]\n",
    " \n",
    " \n",
    " - Now it goes to **plus infinity as hθ(x) goes to 1**\n",
    " \n",
    " \n",
    " - With our particular cost functions J(θ) is going to be **convex** and avoid local minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## This is true..\n",
    "\n",
    "***\n",
    " <center><img src=../images/meme_orlando.png alt=\"OB\" style=\"width: 400px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/Maths-Insight.png\" alt=\"Maths-Insight\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br />\n",
    "\n",
    "## Enter : Simplified Cost Function \n",
    "***\n",
    "An easier way to write the cost function is when we combine the 2: \n",
    "\n",
    " - cost(hθ, (x),y) = -ylog( hθ(x) ) - (1-y)log( 1- hθ(x) ) \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simplified Cost Function \n",
    "***\n",
    "When y = 1\n",
    " - Then our equation simplifies to:\n",
    "    - -log(hθ(x)) - (0)log(1 - hθ(x))\n",
    "    - = -log(hθ(x))\n",
    "\n",
    " - **Which is what we had before when y = 1**\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Simplified Cost Function \n",
    "***\n",
    "When y = 0\n",
    "\n",
    " - Then our equation simplifies to:\n",
    "     - -(0)log(hθ(x)) - (1)log(1 - hθ(x))\n",
    "     - = -log(1- hθ(x))\n",
    " - **Which is what we had before when y = 0**\n",
    " \n",
    "Clever!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Maths-Insight.png\" alt=\"Maths-Insight\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br />\n",
    "\n",
    "## Simplified Cost Function\n",
    "***\n",
    "To sum it up, our cost function for \"m\" training examples is: \n",
    "***\n",
    " <img src=../images/Image[16].png alt=\"Math\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Utilizing Gradient Descent\n",
    "***\n",
    " - Now to get minimum θ : Repeat: \n",
    "***\n",
    " <center><img src=../images/image31.png alt=\"OB\" style=\"width: 250px;\"/></center>\n",
    "***\n",
    " - We need to Minimize J(θ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Maths-Insight.png\" alt=\"Maths-Insight\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br />\n",
    "\n",
    "## Utilizing Gradient Descent\n",
    "***\n",
    " - After doing some mathematical derivations and re-arranging, we get an Algorithm that's similar to the one we encountered in Linear Regression:\n",
    "***\n",
    " <center><img src=../images/image34.png alt=\"OB\" style=\"width: 200px;\"/></center>\n",
    "***\n",
    "and this is how we simultaneously update \"θ\" for all \"j\".  Also:\n",
    "***\n",
    " <center><img src=../images/Image[17].png alt=\"OB\" style=\"width: 200px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Maths-Insight.png\" alt=\"Maths-Insight\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br />\n",
    "\n",
    "## Simply Put\n",
    "***\n",
    " <center><img src=../images/Image[18].png alt=\"OB\" style=\"width: 350px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Back to John's Problem\n",
    "***\n",
    "John wants apply this model to his loan prediction dataset, but he will have to wait till you solve your project!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ways to Evaluate \n",
    "***\n",
    " - We can tell that if we sit to cross-check each and every prediction, it can take days to find out how well our model did\n",
    " \n",
    " - This calls for a better method to evaluate our model. Methods such as Confusion Matrix, accuracy, F1-score, etc are all very handy mechanisms and are very easy to build due to the advanced libraries available to us! \n",
    " \n",
    " - Let's check these out! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evalution Metrics for Logistic Regression\n",
    "***\n",
    "* As we already know, we use different metrics for regression and classification\n",
    "* We know that we can use `MSE` for regression problems and `Accuracy` for classification problems\n",
    "* However, these might not be the best metrics in every situation<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Evalution Metrics for Logistic Regression\n",
    "***\n",
    "**Example**<br><br>\n",
    "We are trying to detect credit card fraud.\n",
    "Occurrence rate of fraud is 3 in 1000.\n",
    "Let's say our model predicted as in the table : \n",
    "\n",
    "\n",
    "| Value | Fraud  | Not Fraud |\n",
    "|---|---|---|\n",
    "| Predicted Fraud | 1 | 1 |\n",
    "| Predicted Not Fraud | 2 | 996 |\n",
    "\n",
    "\n",
    "* Following are the types of Classification Metrics :\n",
    "    * Confusion Matrix\n",
    "    * Classification Matrix\n",
    "    * F1 Score\n",
    "    * Area under ROC curve\n",
    "    * Classification Report\n",
    "    * Logarithmic Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Confusion Matrix\n",
    "***\n",
    "- The confusion matrix is a handy presentation of the accuracy of a model with two or more classes. Below is an example of a Confusion Matrix \n",
    "<br><br>\n",
    "\n",
    "\n",
    "| Value | Fraud  | Not Fraud |\n",
    "|---|---|---|\n",
    "| Predicted Fraud | 1 | 1 |\n",
    "| Predicted Not Fraud | 2 | 996 |\n",
    "\n",
    "\n",
    "    True Positives (TP): These are predicted yes and actually yes (Top Left)\n",
    "    True Negatives (TN): We predicted no, and actually no (Top Right) \n",
    "    False Positives (FP): We predicted yes, but actually no. (AKA \"Type I error.\") (Top Right) \n",
    "    False Negatives (FN): We predicted no, but yes. (AKA\"Type II error.\") (Bottom Left)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Confusion Matrix\n",
    "***\n",
    "* Classification accuracy is the number of correct predictions **(TN + TP)** made as a ratio of all predictions made. **(TN + TP +FN + FP)**<br><br>\n",
    "It is suitable when :\n",
    "* There are an equal number of observations in each class\n",
    "* That all predictions and prediction errors are equally important,which is often not the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## John's Confusion Matrix\n",
    "***\n",
    " - John now considers viewing the confusion matrix to see how well his model performed\n",
    " \n",
    " - Let's see how he did this in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Applying confusion matrix on above data\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Precision and Recall \n",
    "***\n",
    "So we have \n",
    "\n",
    "\n",
    "| Value | Actual Class 1  | Actual Class 0 |\n",
    "|---|---|---|\n",
    "| Predicted Class 1| True +ve  | False +ve |\n",
    "| Predicted Class 0 | False -ve | True -ve  |\n",
    "\n",
    "\n",
    "Consider this: Let's say *y=1* in presence of Cancer in a patient\n",
    "\n",
    " - Now, of all patients where we predicted y = 1, what fraction actually has cancer? Think about this given the table above! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Precision and Recall \n",
    "***\n",
    "It turns out that what we're trying to calculate is nothing but the **Precision** of the model\n",
    "\n",
    "Here, \n",
    "\n",
    "\n",
    "$$Precision = \\frac {(True +ves)} {(True +ves  +  False +ves)}$$\n",
    "\n",
    "Doesn't this make sense? This is the fraction that **ACTUALLY** has Cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Precision and Recall \n",
    "***\n",
    " - Now, of **all patients that actually have cancer**, what fraction did we **CORRECTLY** detect as having cancer? \n",
    " \n",
    " - This is nothing but the recall! \n",
    " \n",
    " - Here \n",
    "\n",
    "$$Recall = \\frac {(True +ves)} {(True +ves  +  False -ves)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=../images/image99.png alt=\"precision and recall\" style=\"width: 400px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Trade - Off: Precision Vs. Recall \n",
    "***\n",
    "- This is more of a in-class activity! \n",
    "- Think about this: What happens if we get an increased value of Precision? Do you think that would lower Recall? And vice-versa? \n",
    "\n",
    "- Think of an example! And use easy numerical calculations too. You can just use a pencil and paper, no need for code! \n",
    "\n",
    "- [**Hint**: There is a trade-off!] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### F1 Score\n",
    "***\n",
    " - To deal with this Trade-off we calculate something known as the F-1 Score: F1 score is a good approach to minimize a bias towards either the Precision or the Recall\n",
    "\n",
    " $$F1 Score = \\frac {2PR} {P + R} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## F-1 Score \n",
    "***\n",
    "F1 Score is defined as \n",
    "\n",
    "![](../images/image41.png)<br><br>\n",
    "\n",
    "\n",
    "* tp = true positive\n",
    "* tn = true negative\n",
    "* fp = false positive\n",
    "* fn = false negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## John's F1 - score \n",
    "***\n",
    "- Using this intuition, John wanted to calculate the F-1 Score to better understand the evaluation of his model\n",
    "\n",
    "- Let's see how he implemented this in Python! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## code for John's f-1 score\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Area under ROC Curve\n",
    "***\n",
    "Area under ROC Curve (or AUC for short) is a performance metric for binary classification problems.\n",
    "\n",
    "- The AUC represents a model’s ability to discriminate between positive and negative classes.\n",
    " - An area of 1.0 represents a model that made all predictions perfectly. An area of 0.5 represents a model as good as random.<br>\n",
    "**Brain Teaser**: What does area < 0.5 signify?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## John's ROC Curve\n",
    "***\n",
    "- After learning this, John wanted to plot his very own ROC curve! \n",
    "- Let's see how he went about implementing this in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn import metrics\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred, pos_label=1)\n",
    "metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Logarithmic Loss\n",
    "***\n",
    "Logarithmic loss (or logloss) is a performance metric for evaluating the predictions of probabilities of membership to a given class\n",
    "\n",
    "\n",
    "Where,\n",
    "* N is the number of samples or instances,\n",
    "* M is the number of possible labels,\n",
    "* y<sub>ij</sub> is a binary indicator of whether or not label j is the correct classification for instance i,\n",
    "* p<sub>ij</sub> is the model probability of assigning label j to instance i.\n",
    "\n",
    "![](../images/image49.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Logarithmic Loss\n",
    "***\n",
    "* The scalar probability between 0 and 1 can be seen as a measure of confidence for a prediction by an algorithm.<br>\n",
    "* Predictions that are correct or incorrect are rewarded or punished proportionally to the confidence of the prediction.<br>\n",
    "* logloss nearer to 0 is better, with 0 representing a perfect logloss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## John's Log Loss\n",
    "***\n",
    " - John wanted to check this too! \n",
    " \n",
    " - Let's check how he implemented this in Python, it's fairly easy thanks to sci-kit learn! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "log_loss(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Help John find out his status on loan approval\n",
    "***\n",
    "Now that you are equipped with the new weapon to handle classification problems and ways to evaluate the performances, as the next step, help John find out if he would get a loan or not!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In Class Activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Census Income Dataset\n",
    "\n",
    "## Problem Statement :\n",
    "The dataset given below contains the census information about the attributes of individuals as the features and their income as the target. The task is to predict whether a person makes over $50K a year or not.\n",
    "\n",
    "## About the dataset:\n",
    "This data was extracted from the 1994 Census bureau database by Ronny Kohavi and Barry Becker (Data Mining and Visualization, Silicon Graphics).  \n",
    "\n",
    "The data has 1032 instances and 15 features. For your information the target variable `Income` has already been label encoded.\n",
    "1 indicates the income is greater than or equal to 50K. 0 indicates the income is less than 50K. Below is a brief about the features in the dataset.\n",
    "\n",
    "|Features|Description|\n",
    "|-----|-----|\n",
    "|age|Age of the person|\n",
    "|Workclass|Categorical variable having 8 categories each denoting the type of employment|\n",
    "|fnlwgt|final weight. |\n",
    "|education|Categorical variable with 16 categories each denoting the qualification of the person.|\n",
    "|education-num|number of years of education|\n",
    "|marital-status|Categorical variable with 7 categories|\n",
    "|occupation|Categorical variable with 14 categories each denoting the type of occupation of the individual|\n",
    "|relationship|Categorical variable with 6 categories.|\n",
    "|race|Categorical variable with 5 categories each denoting the ethnicity of the individual|\n",
    "|sex|Gender of the person|\n",
    "|capital-gain|Continuous| \n",
    "|capital-loss|Continuous|\n",
    "|hours-per-week|Working hours per week|\n",
    "|native-country|Categorical|\n",
    "|Income|Target variable. 1 indicates >=50k. 0 indicates <50K|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the data using pandas module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import skew\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split,KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score,classification_report,f1_score,confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv('../data/adult_preprocessed.csv',index_col=0)\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for null values and get a summary of all the columns in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values\n",
    "dataframe.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a summary of the columns in the dataset\n",
    "dataframe.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that there are no null values in any of the columns. So it is a pretty clean dataset.<br/>\n",
    "The minimum, 25th percentile, median, 75th percentile values for the columns `capital-gain` and `capital-loss` are 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For further analysis split to bring all the continuous variables together and categorical variables together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate continuous variables and categorical variables\n",
    "\n",
    "# Your code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Plotting a stacked bar chart, have an estimate of the number of people with income above and below 50K with respect to each of the categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What can you infer from the above visualisations ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot a histogram for all the continuous variables to check for skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Majority of values in capital-gain and capital-loss are zeros. So remove the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for skewness in the continuous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The column `fnlwgt` is a highly right skewed column. Remove the skewness using a log transform and then check if the skewness has been removed by plotting its histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for the pearson correlation between the continuous variables by plotting a heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Check for correlation between continuous variables\n",
    "\n",
    "# Your code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encode all the categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encode Categorical variables\n",
    "\n",
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate both the continuous and categorical dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = new_dataframe.iloc[:,:-1]\n",
    "y = new_dataframe.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a StandardScaler on the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting a Standard Scaler \n",
    "\n",
    "# Your code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert array to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into train and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_valid,y_train,y_valid = train_test_split(X,y,random_state=42,test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the accuracy, f1_score, confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform hyperparameter tuning using GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "params = {'penalty':['l1','l2'],'C':[0.001,0.01,0.1,1,10,100]}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the best parameters and the metrics after grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Thank You\n",
    "***\n",
    "### Next Session: Improving your model with Feature Selection\n",
    "\n",
    "- Feature Selection Importance\n",
    "- Different types of Feature Selection Methods\n",
    "- PCA\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
