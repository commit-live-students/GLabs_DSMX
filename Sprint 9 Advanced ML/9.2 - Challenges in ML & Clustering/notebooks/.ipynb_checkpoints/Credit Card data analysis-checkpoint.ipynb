{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Challenges in Machine Learning & Clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Agenda:\n",
    "***\n",
    "* What is Imbalanced Data\n",
    "* Dealing with imbalanced data\n",
    "    * Evaluation Metrics\n",
    "    * Resampling Techniques\n",
    "    * Algorithmic Techniques\n",
    "* Dealing with small datasets\n",
    "* Values of K in K-Fold validation\n",
    "* Do we need hundreds of classifiers?\n",
    "***\n",
    "* Introduction to Clustering\n",
    "* k-means clustering\n",
    "* Hierarchical clustering\n",
    "* Clustering in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenges in Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## John and Lucius again\n",
    "***\n",
    "John and Lucius were finally out, in a nice looking pub, having drinks that were promised but never delivered to them. Having studied about all the cool algorithms, John was now pondering upon his journey. No amount of machine learning would have been able to predict his journey from house hunting to learning about ML algorithms. At least that's what he believed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## John and Lucius again\n",
    "***\n",
    "John, now seriously considering a career in ML and data science started thinking about all the practical challenges ML professionals would come across. One of the first ones that came to his mind was: \"What if the classes in a classification problem were really skewed towards one of the classes?\" He told about this to Lucius. Lucius wanted to enjoy his drinks for once, but looking at John's enthusiasm, he realized that he had no choice. \"Imbalanced Datasets\", he muttered reluctantly and started explaining:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "## Imbalanced Data\n",
    "***\n",
    "\n",
    "* Imbalanced Data is a slightly inaccurate term we use to describe datasets where the distribution of the target variable is imbalanced.\n",
    "* This is most visible and easily detected for binary classification tasks, where most of the instances (data points) belong to one of the classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "## Imbalanced Data\n",
    "***\n",
    "* Class-imbalance regularly occurs in datasets pertaining to multi-class classification tasks as well.\n",
    "* Detecting it in Regression tasks can take a little more effort, and plotting a histogram of the target variable is a good starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Examples\n",
    "***\n",
    "Imbalanced datasets frequently occur in\n",
    "\n",
    "* Anomaly detection\n",
    "    * Electricity pilferage\n",
    "    * Fraudulent transactions in banks\n",
    "* Predicting Rare events\n",
    "    * Ad click-through-rate (CTR) prediction (~1%)\n",
    "    * Identification of rare diseases\n",
    "    * User Churn (for example, usera churn is ~2% in telecom industry)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "## Understanding Imbalanced Data\n",
    "***\n",
    "Let's look at detection of credit card fraud to understand this in more detail, said Lucius recalling his encounter with such a problem.\n",
    "\n",
    "* Credit card fraud is a widespread problem and accounts for millions of dollars in loss.\n",
    "* But, given the extremely high number of credit card transactions everyday, fraudulent transactions represent only a small fraction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Even then, fraudulent transactions have an outsized impact on the revenue, because\n",
    "* Almost the entire value of a fraudulent transaction counts towards loss (less insurance), whereas\n",
    "* The profit from a genuine transaction is a fraction of the total transaction value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Thus it's more important to recall fraudulent transactions, even if that means we'll end up labeling some genuine transactions as fraud \n",
    "* This is more of a business call, actually, depending on a wide range of factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Even though the typical datasets recording credit card transaction are very large (millions of row), the number of fraudulent records tends to be a very small fraction of the datasets - around 1% to 2%.\n",
    "* Thus, if a given dataset of credit card transactions has a million row, only about 10,000 of them will be from fraudulent transactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So what? blurted John, I can still train a model and try and predict if a transaction in fraudulent or not. Yes, you can, but that is likely to give you not-so-good results, said Lucius. Let me explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "## Consequences of imbalanced classes:\n",
    "***\n",
    "**Bias in the model towards dominant class**\n",
    "\n",
    "* Most machine learning models will end up predicting most of the transactions as genuine as they end up learning from mostly positive instances.\n",
    "* Accounting for this in the way the model sees the data (resampling strategies) or the way it learns (error metric, algorithmic tweaks) can help us build models that are better at predicting the minor class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "## Consequences of imbalanced classes:\n",
    "***\n",
    "**Difficulty in assessing model performance**\n",
    "\n",
    "* For the credit card dataset discussed before, if we classify all transactions as genuine, we might still have an accuracy of 99%! (Bias towards dominant class)\n",
    "* Let's look at a problem we are more likely to encounter in practice. \n",
    "* Suppose we have built two models, A and B, to predict fraudulent transactions, and we want to select the one with better performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "## Consequences of imbalanced classes:\n",
    "***\n",
    "**Model A**\n",
    "\n",
    "* Of the 99% genuine transactions, this model predicts 98.5% correctly\n",
    "* Of the 1% fraudulent transactions, this model predicts 0.25% correctly\n",
    "\n",
    "**Model accuracy: 98.75%**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "## Consequences of imbalanced classes:\n",
    "***\n",
    "**Model B**\n",
    "* Of the 99% genuine transactions, this model predicts 98.25% correctly\n",
    "* Of the 1% fraudulent transactions, this model predicts 0.5% correctly\n",
    "\n",
    "**Model accuracy: 98.75%**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "## Consequences of imbalanced classes:\n",
    "***\n",
    "* **Which of these two models performs better?**\n",
    "* **Which one of these two should we use?**\n",
    "\n",
    "\n",
    "* Clearly Model B is more valuable to us, but Accuracy, one of the most common metrics used in classification, fails to reflect that.\n",
    "* We need to use a metric that not only captures the class imbalance better, but one that also lets us make meaningful trade-offs between precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\"Hmmm... Interesting...\" said John, scratching head, \"So how do we deal with such a dataset?\"\n",
    "\n",
    "Lucius tried to recall a few techniques he studied in college. After some time he started:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "## Dealing with Imbalanced Data\n",
    "***\n",
    "Here are the ways to handle the imbalanced data\n",
    "\n",
    "* More suited error metrics (comparatively immune to class imbalance)\n",
    "* Resampling strategies\n",
    "* Algorithmic techniques\n",
    "* Buy/Collect more data\n",
    "\n",
    "Let's look at them one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## More Appropriate Error Metrics\n",
    "***\n",
    "The idea is to choose an error metric that is immune to class imbalance. As we saw earlier, accuracy is something that is not very robust against class imbalance. Following are a few examples of such error metrics\n",
    "\n",
    "1. Confusion Matrix\n",
    "2. Precision / Recall / Sensitivity / Specificity\n",
    "3. AUC ROC\n",
    "4. f1 Score\n",
    "5. Cohen's Kappa\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We have already gone through all the metrics, except Cohen's kappa. So let's understand Cohen's kappa better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "## Cohen’s Kappa:\n",
    "***\n",
    "* The Kappa statistic (or value) is a metric that compares an Observed Accuracy with an Expected Accuracy (random chance). \n",
    "* Let's understand how Cohen's kappa is defined using a confusion matrix\n",
    "![](../images/image20.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Here, let's say rows (A) are the predicted values and columns (B) are the actual values.\n",
    "* Now let's understand observed accuracy and expected accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Observed Accuracy is simply the number of instances that were classified correctly throughout the entire confusion matrix.\n",
    "\n",
    "![](../images/image20.png)\n",
    "![](../images/image19.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Expected Accuracy is defined as the accuracy that any classifier would be expected to achieve by random chance.\n",
    "\n",
    "Sounds confusing? Let's break this down.\n",
    "\n",
    "Our classifier classifies \n",
    "* (a + b) observations as yes\n",
    "* (c + d) observations as no\n",
    "\n",
    "\n",
    "* Hence, the probability of a randomly chosen observation being classified as yes is (P1): (a + b) / (a + b + c + d)\n",
    "* And, the probability of a randomly chosen observation being classified as no is (P2): (c + d) / (a + b + c + d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Reality:\n",
    "* (a + c) observations as yes\n",
    "* (b + d) observations as no\n",
    "\n",
    "\n",
    "* Hence, the probability of a randomly chosen observation being classified as yes is (P3): (a + c) / (a + b + c + d)\n",
    "* And, the probability of a randomly chosen observation being classified as no is (P4): (b + d) / (a + b + c + d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Probability of a randomly chosen sample being *CORRECTLY* classified is \n",
    "\n",
    "* when the classifier classifies it as yes AND it is in reality yes: P1*P3\n",
    "* when the classifier classifies it as no AND it is in reality no: P2*P4\n",
    "\n",
    "Hence, Expected probability of a randomly chosen sample being *CORRECTLY* classified is: P1 x P3 + P2 x P4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](../images/image20.png)\n",
    "![](../images/image23.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](../images/image20.png)\n",
    "![](../images/image23.png)\n",
    "![](../images/image22.png)\n",
    "![](../images/image21.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Great! now that we understand $P_o$ and $P_e$, Cohen's kappa is defined as\n",
    "\n",
    "![](../images/image24.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* In essence, the kappa statistic is a measure of how closely the instances classified by the machine learning classifier matched the data labeled as ground truth, controlling for the accuracy of a random classifier as measured by the expected accuracy. \n",
    "* Not only can this kappa statistic shed light into how the classifier itself performed, the kappa statistic for one model is directly comparable to the kappa statistic for any other model used for the same classification task.\n",
    "* For any arbitrary accuracy value, more the Kappa value, better the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Q: Here is an example of 2 classifiers with given confusion matrix. Which one do you think is performing better? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Classifier A**\n",
    "\n",
    "![](../images/image25.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**CLassifier B**\n",
    "\n",
    "![](../images/image27.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A: It is apparent that for given accuracy, classifier A is doing a much better job at classifying than classifier B, which is also reflected in higher Kappa value attained by classifier A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\"Wow! that is an interesting metric to use!\" said John. What else we can do? Lucius again went searching the attics of his mind, looking for some other techniques he had studies. And started recalling the resampling techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "## Resampling Techniques\n",
    "***\n",
    "\n",
    "Resampling methods are techniques in which , we try to **reduce the proportion of the dominant class by undersampling** from it, or we try to **increase the proportion of the minor class by oversampling** from it.\n",
    "However, some of the more successful approaches combine both oversampling and undersampling.\n",
    "\n",
    "Let's have a look at them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "## Undersampling:\n",
    "***\n",
    "Undersampling techniques try to balance out the classes by reducing the number of observations in the dominant classes. \n",
    "\n",
    "![](../images/image26.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Undersampling\n",
    "***\n",
    "There are many undersampling techniques. Let's look at some of them.\n",
    "\n",
    "* Random Undersampling\n",
    "* Cluster Centroids\n",
    "* Tomek Links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Technical-Stuff.png\" alt=\"Technical-Stuff\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "## Undersampling - Random Undersampling\n",
    "***\n",
    "Random undersampling is one of the most intuitive and naive methods for undersampling. This method works by randomly choosing the samples from dominant classes. Let's understand the random undersampling by a few examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Example 1**\n",
    "* Total number of observations: 1000\n",
    "* Total number of classes: 2 (A, B)\n",
    "* Size of class A: 975\n",
    "* Size of class B: 25\n",
    "* Proportion of Minor class: 2.5%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Sampled Dataset**\n",
    "* From A, select 475 points randomly\n",
    "* From B, select all 25 points\n",
    "* Proportion of Minor class: 5%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Example 2**\n",
    "* Total number of observations: 1000\n",
    "* Total number of classes: 3 (A, B, C)\n",
    "* Size of class A: 925\n",
    "* Size of class B: 25\n",
    "* Size of class C: 50\n",
    "* Proportion of Minor classes: 2.5% (B) and 5% (C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Sampled Dataset**\n",
    "* From A, select 425 points randomly\n",
    "* From B, select all 25 points\n",
    "* From C, select all 50 points\n",
    "* Proportion of Minor class: 5% (B) and 10% (C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "loan_predition = pd.read_csv(\"../data/loan_prediction.csv\", )\n",
    "loan_predition.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "label_enc = LabelEncoder()\n",
    "for column in loan_predition.select_dtypes(include=[\"object\"]).columns.values:\n",
    "    loan_predition[column] = label_enc.fit_transform(loan_predition[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sns.countplot(loan_predition.Loan_Status);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "loan_predition.replace({0:1, 1:0}, inplace=True)\n",
    "# index_values = loan_predition[loan_predition.Loan_Status == 1][100:].index.values\n",
    "# loan_predition = loan_predition.drop(loan_predition.index[list(index_values)])\n",
    "sns.countplot(loan_predition.Loan_Status);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(loan_predition.iloc[:,:-1], \n",
    "                                                    loan_predition.iloc[:,-1], \n",
    "                                                    random_state=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=9)\n",
    "rf.fit(X_train, y_train)\n",
    "print(\"f1_score\", f1_score(y_test, rf.predict(X_test)))\n",
    "print(precision_score(y_test, rf.predict(X_test)))\n",
    "print(recall_score(y_test, rf.predict(X_test)))\n",
    "print(roc_auc_score(y_test, rf.predict(X_test)))\n",
    "print(confusion_matrix(y_test, rf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(random_state=9)\n",
    "lr.fit(X_train, y_train)\n",
    "print(\"f1_score\", f1_score(y_test, lr.predict(X_test)))\n",
    "print(precision_score(y_test, lr.predict(X_test)))\n",
    "print(recall_score(y_test, lr.predict(X_test)))\n",
    "print(roc_auc_score(y_test, lr.predict(X_test)))\n",
    "print(confusion_matrix(y_test, lr.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/Technical-Stuff.png\" alt=\"Technical-Stuff\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br />\n",
    "\n",
    "## Undersampling with `imblearn`:\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Create the samplers\n",
    "rus = RandomUnderSampler(random_state=9)\n",
    "X_sample2, y_sample2 =  rus.fit_sample(X_train, y_train)\n",
    "sns.countplot(y_sample2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "rf2 = RandomForestClassifier(random_state=9)\n",
    "rf2.fit(X_sample2, y_sample2)\n",
    "print(\"f1_score\", f1_score(y_test, rf2.predict(X_test)))\n",
    "print(precision_score(y_test, rf2.predict(X_test)))\n",
    "print(recall_score(y_test, rf2.predict(X_test)))\n",
    "print(roc_auc_score(y_test, rf2.predict(X_test)))\n",
    "print(confusion_matrix(y_test, rf2.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "rf2 = LogisticRegression(random_state=9)\n",
    "rf2.fit(X_sample2, y_sample2)\n",
    "print(\"f1_score\", f1_score(y_test, rf2.predict(X_test)))\n",
    "print(precision_score(y_test, rf2.predict(X_test)))\n",
    "print(recall_score(y_test, rf2.predict(X_test)))\n",
    "print(roc_auc_score(y_test, rf2.predict(X_test)))\n",
    "print(confusion_matrix(y_test, rf2.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/Technical-Stuff.png\" alt=\"Technical-Stuff\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br />\n",
    "\n",
    "## Undersampling - Cluster Centroids\n",
    "***\n",
    "* This technique undersampled by creation of new samples. \n",
    "* Let’s understand how\n",
    "    * Size of minority class: 200\n",
    "    * Size of majority class: 1000\n",
    "* Cluster centroids method works by creating 200 clusters of the majority class and returns the centroids of each of the clusters. Hence, rather than sampling from the original data points we get new representative sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import ClusterCentroids\n",
    "\n",
    "cc = ClusterCentroids(random_state=9)\n",
    "X_sample3, y_sample3 = cc.fit_sample(X_train, y_train)\n",
    "sns.countplot(y_sample3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "rf3 = RandomForestClassifier(random_state=9)\n",
    "rf3.fit(X_sample3, y_sample3)\n",
    "print(\"f1_score\", f1_score(y_test, rf3.predict(X_test)))\n",
    "print(precision_score(y_test, rf3.predict(X_test)))\n",
    "print(recall_score(y_test, rf3.predict(X_test)))\n",
    "print(roc_auc_score(y_test, rf3.predict(X_test)))\n",
    "print(confusion_matrix(y_test, rf3.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "rf3 = LogisticRegression(random_state=9)\n",
    "rf3.fit(X_sample3, y_sample3)\n",
    "print(\"f1_score\", f1_score(y_test, rf3.predict(X_test)))\n",
    "print(precision_score(y_test, rf3.predict(X_test)))\n",
    "print(recall_score(y_test, rf3.predict(X_test)))\n",
    "print(roc_auc_score(y_test, rf3.predict(X_test)))\n",
    "print(confusion_matrix(y_test, rf3.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/Technical-Stuff.png\" alt=\"Technical-Stuff\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br />\n",
    "\n",
    "## Undersampling - Tomek Links\n",
    "***\n",
    "* Tomek Links are pairs of instances of opposite classes who are their own nearest neighbors.\n",
    "* This technique identifies Tomek Links and gets rid of the majority samples.\n",
    "* The idea is to clarify the border between the minority and majority classes, making the minority region(s) more distinct. \n",
    "\n",
    "![](../images/image31.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "tl = TomekLinks(sampling_strategy='not minority')\n",
    "X_sample4, y_sample4 = tl.fit_sample(X_train, y_train)\n",
    "sns.countplot(y_sample4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "rf4 = RandomForestClassifier()\n",
    "rf4.fit(X_sample4, y_sample4)\n",
    "print(\"f1_score\", f1_score(y_test, rf4.predict(X_test)))\n",
    "print(precision_score(y_test, rf4.predict(X_test)))\n",
    "print(recall_score(y_test, rf4.predict(X_test)))\n",
    "print(roc_auc_score(y_test, rf4.predict(X_test)))\n",
    "print(confusion_matrix(y_test, rf4.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "rf4 = LogisticRegression()\n",
    "rf4.fit(X_sample4, y_sample4)\n",
    "print(\"f1_score\", f1_score(y_test, rf4.predict(X_test)))\n",
    "print(precision_score(y_test, rf4.predict(X_test)))\n",
    "print(recall_score(y_test, rf4.predict(X_test)))\n",
    "print(roc_auc_score(y_test, rf4.predict(X_test)))\n",
    "print(confusion_matrix(y_test, rf4.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Oversampling:\n",
    "***\n",
    "As opposed to undersamping, oversampling techniques try to make the classes balanced by enhancing the minority class using different techniques.\n",
    "\n",
    "![](../images/image29.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/Technical-Stuff.png\" alt=\"Technical-Stuff\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br />\n",
    "\n",
    "## Oversampling - Random Oversampling\n",
    "***\n",
    "Random oversampling selects the samples with replacement from the minority class.\n",
    "\n",
    "**Example 1**\n",
    "\n",
    "* Total number of observations: 1000\n",
    "* Total number of classes: 2 (A, B)\n",
    "* Size of class A: 975\n",
    "* Size of class B: 25\n",
    "* Proportion of Minor class: 2.5%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Sampled Dataset\n",
    "* From A, select all 975 points\n",
    "* From B, select 225 points with replacement\n",
    "* Proportion of Minor class: 18.75%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Example 2**\n",
    "\n",
    "* Total number of observations: 1000\n",
    "* Total number of classes: 3 (A, B, C)\n",
    "* Size of class A: 925\n",
    "* Size of class B: 25\n",
    "* Size of class C: 50\n",
    "* Proportion of Minor classes: 2.5% (B) and 5% (C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Sampled Dataset**\n",
    "* From A, select all 925 points\n",
    "* From B, select 225 points with replacement\n",
    "* From C, select 450 points with replacement\n",
    "* Proportion of Minor class: ~14% (B) and ~28% (C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "ros = RandomOverSampler(random_state=9)\n",
    "X_sample5, y_sample5 = ros.fit_sample(X_train, y_train)\n",
    "sns.countplot(y_sample5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "rf5 = RandomForestClassifier(random_state=9)\n",
    "rf5.fit(X_sample5, y_sample5)\n",
    "print(\"f1_score\", f1_score(y_test, rf5.predict(X_test)))\n",
    "print(precision_score(y_test, rf5.predict(X_test)))\n",
    "print(recall_score(y_test, rf5.predict(X_test)))\n",
    "print(roc_auc_score(y_test, rf5.predict(X_test)))\n",
    "print(confusion_matrix(y_test, rf5.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "rf5 = LogisticRegression(random_state=9)\n",
    "rf5.fit(X_sample5, y_sample5)\n",
    "print(\"f1_score\", f1_score(y_test, rf5.predict(X_test)))\n",
    "print(precision_score(y_test, rf5.predict(X_test)))\n",
    "print(recall_score(y_test, rf5.predict(X_test)))\n",
    "print(roc_auc_score(y_test, rf5.predict(X_test)))\n",
    "print(confusion_matrix(y_test, rf5.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/Technical-Stuff.png\" alt=\"Technical-Stuff\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br />\n",
    "\n",
    "## Oversampling - SMOTE (Synthetic Minority Oversampling Technique)\n",
    "***\n",
    "* The original paper: [SMOTE: Synthetic Minority Over-sampling Technique](https://arxiv.org/pdf/1106.1813.pdf)\n",
    "* The minority class is over-sampled by creating synthetic examples rather than by over-sampling with replacement.\n",
    "* It generates synthetic examples in a less application-specific manner, by operating in feature space rather than data space.\n",
    "* The minority class is over-sampled by taking each minority class sample and introducing synthetic examples along the line segments joining any/all of the k minority class nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Technical-Stuff.png\" alt=\"Technical-Stuff\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br />\n",
    "\n",
    "## How SMOTE works:\n",
    "***\n",
    "* Take the difference between the feature vector (sample) under consideration and its nearest neighbor.\n",
    "* Multiply this difference by a random number between 0 and 1, and add it to the feature vector under consideration.\n",
    "* This causes the selection of a random point along the line segment between two specific features.\n",
    "![](../images/image32.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Depending upon the amount of over-sampling required, neighbors from the k nearest neighbors are randomly chosen.\n",
    "* For instance, if the amount of over-sampling needed is 200%, only two neighbors from the five nearest neighbors are chosen and one sample is generated in the direction of each.\n",
    "* NOTE: this k becomes a hyperparameter for SMOTE algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Limitation: Because it operates by interpolating between rare examples.\n",
    "* Hence, it can only generate examples within the body of available examples—never outside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "\n",
    "smote = BorderlineSMOTE(random_state=9, kind=\"borderline-2\")\n",
    "X_sample6, y_sample6 = smote.fit_sample(X_train, y_train)\n",
    "sns.countplot(y_sample6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "rf6 = RandomForestClassifier(random_state=9)\n",
    "rf6.fit(X_sample6, y_sample6)\n",
    "print(\"f1_score\", f1_score(y_test, rf6.predict(X_test)))\n",
    "print(precision_score(y_test, rf6.predict(X_test)))\n",
    "print(recall_score(y_test, rf6.predict(X_test)))\n",
    "print(roc_auc_score(y_test, rf6.predict(X_test)))\n",
    "print(confusion_matrix(y_test, rf6.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "rf6 = LogisticRegression(random_state=9)\n",
    "rf6.fit(X_sample6, y_sample6)\n",
    "print(\"f1_score\", f1_score(y_test, rf6.predict(X_test)))\n",
    "print(precision_score(y_test, rf6.predict(X_test)))\n",
    "print(recall_score(y_test, rf6.predict(X_test)))\n",
    "print(roc_auc_score(y_test, rf6.predict(X_test)))\n",
    "print(confusion_matrix(y_test, rf6.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Algorithmic Approach\n",
    "***\n",
    "In algorithmic approach, we use different techniques to tweak the algorithms to make them learn minority classes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Algorithmic Approach - Cost Sensitive Training (Penalised Training): \n",
    "***\n",
    "* One way to do so is to create a custom metric which penalizes wrong predictions in the minority class.\n",
    "* Recall the metric the we defined while discussing the metrics of evaluation:\n",
    "* metric=(5 ∗ false negative + 1 ∗ false positive) / 6\n",
    "* Such metrics could be used in handling the imbalanced datasets.\n",
    "* sklearn provides a method to device custom metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/Technical-Stuff.png\" alt=\"Technical-Stuff\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br />\n",
    "\n",
    "## Algorithmic Approach - Choice of Algorithm:\n",
    "***\n",
    "* Ensemble methods, especially Random Forests are found to be good at handling imbalanced datasets\n",
    "* These methods are able to learn classes based on importance assigned to them.\n",
    "* sklearn's implementations of these algorithms provides option to handle imbalanced dataset by setting the **`class_weight`** parameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class_wts = range(50)\n",
    "f1s = []\n",
    "auc = []\n",
    "for wt in class_wts:\n",
    "    rf7 = RandomForestClassifier(random_state=9, class_weight={0:wt,1:1})\n",
    "    rf7.fit(X_train, y_train)\n",
    "    f1s.append(f1_score(y_test, rf7.predict(X_test)))\n",
    "    auc.append(roc_auc_score(y_test, rf7.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "max_scorer = f1s.index(np.max(f1s))\n",
    "rf7 = RandomForestClassifier(random_state=9, class_weight={0:max_scorer,1:1})\n",
    "rf7.fit(X_train, y_train)\n",
    "print(\"f1_score\", f1_score(y_test, rf7.predict(X_test)))\n",
    "print(precision_score(y_test, rf7.predict(X_test)))\n",
    "print(recall_score(y_test, rf7.predict(X_test)))\n",
    "print(roc_auc_score(y_test, rf7.predict(X_test)))\n",
    "print(confusion_matrix(y_test, rf7.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(class_wts, f1s, label=\"F1 scores\")\n",
    "plt.plot(class_wts, auc, label=\"AUC scores\")\n",
    "plt.xlabel(\"class weight\")\n",
    "plt.ylabel(\"scores\")\n",
    "plt.title(\"Effect of Class Wt. in Imbalanced Classes\")\n",
    "plt.ylim(0.45, 0.8)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Some Useful tips:\n",
    "***\n",
    "* While carrying out cross-validation, make stratified folds to make sure the presence of minority class in all folds\n",
    "* Instead of predictions, get probabilities from the trained classifier.\n",
    "* Study the AUC-ROC curve and adjust the prediction threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\"Phew! that was a lot of techniques to understand in one go!\", John already looked overwhelmed. Lucius, smiling mildly, just added: \"and we just scratched the surface\". There are a lot more techniques that can be employed. It might be worth checking out `imblearn's` official documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Off to other challenges\n",
    "***\n",
    "After concluding the discussion on imbalanced datasets, John started wondering about some more problems that can come along the way. He remembered being told again and again how important the data is for any ML problem. He immediately started thinking what if there is too little data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "## Dealing with Smaller Datasets:\n",
    "***\n",
    "Sometimes, challenge arises not because of too much data, but because of too less data. Such a scenario is known as  **the curse of dimensionality**, which essentially means **number of features >> number of observations**\n",
    "\n",
    "\n",
    "* In case of such small datasets, following are some of the techniques that could come in handy\n",
    "    * Exploit Bootstrapping\n",
    "    * Use Simpler, Regularized Models\n",
    "    * Use Ensemble Techniques\n",
    "    * Use Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Value of K in Koolness\n",
    "***\n",
    "What is the value of k in k-fold validation that should be used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Optimum Value of K in K-Fold Validation:\n",
    "***\n",
    "* Refresher: Why do we use cross-validation?\n",
    "* Trade-off:\n",
    "    * Higher K: More samples to train, more cross-validation, results in less bias, high variance but requires more computations\n",
    "    * Lower K: Less samples to train, less cross-validation, results in more bias, low variance but requires less computations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Optimum Value of K in K-Fold Validation:\n",
    "***\n",
    "* According to paper [A Study of Cross Validation and Bootstrap for Accuracy Estimation and Model Selection](http://robotics.stanford.edu/~ronnyk/accEst.pdf), value of k=10 is a good balance between accuracy and training time\n",
    "* Stratified k-fold seems to perform better\n",
    "![](../images/image33.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Optimum Value of K in K-Fold Validation:\n",
    "***\n",
    "\n",
    "* [Here](https://vinhkhuc.github.io/2015/03/01/how-many-folds-for-cross-validation.html) is a python implementation of the same experiment for iris dataset.\n",
    "* For smaller datasets, usually leave-one-out validation works fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "## The Age old Question - Which Algorithm to Use?\n",
    "***\n",
    "Having studied a bunch of algorithms is good, but choosing which one to use is not! Let's understand which algorithms perform better in which scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Which Algorithm to Use? - A perspective from a Research Paper\n",
    "***\n",
    "In [this](http://jmlr.org/papers/volume15/delgado14a/delgado14a.pdf) paper,\n",
    "The researchers evaluated **179 classifiers** arising from **17 families**, implemented in Weka, R, C and Matlab.\n",
    "They used **121 datasets**, which represent the whole UCI database and other real problems, in order to achieve significant conclusions about the classifier behavior, not dependent on the data set collection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Which Algorithm to Use? - A perspective from a Research Paper\n",
    "***\n",
    "**Key Findings:**\n",
    "\n",
    "* The classifiers most likely to be the bests are the random forest (RF) versions, the best of which achieves 94.1% of the maximum accuracy overcoming 90% in the 84.3% (102 out of 121) of the data sets.\n",
    "* The SVM with Gaussian kernel (implemented in C using LibSVM) achieves 92.3% of the maximum accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Which Algorithm to Use? - A perspective from a Research Paper\n",
    "***\n",
    "A few models are clearly better than the remaining ones:\n",
    "* Random forest\n",
    "* SVM with Gaussian and polynomial kernels\n",
    "* C5.0 decision tree\n",
    "* avNNet (the multi-layer perceptron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Which Algorithm to Use? - A perspective from a Research Paper\n",
    "***\n",
    "**Paper Summary**\n",
    "\n",
    "* The random forest was found to be clearly the best family of classifiers (3 out of 5 bests classifiers are RF), followed by SVM (4 classifiers in the top-10), neural networks and boosting ensembles (5 and 3 members in the top-20, respectively)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Which Algorithm to Use? - Some practical Tips\n",
    "***\n",
    "So far we have learnt about a bunch of algorithms. We have learnt about 2 families of algorithm\n",
    "    * Linear Models\n",
    "    * Ensemble Models\n",
    "\n",
    "The question is then which algorithm to use and when? Let’s have a look at some quick ideas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Which Algorithm to Use? - Some practical Tips\n",
    "***\n",
    "Penalized linear regression methods have the advantage that they train very quickly. That helps us for 2 reasons\n",
    "* Training times on large data sets can extend to hours, days, or even weeks.\n",
    "* Long training times can stall development and deployment on large problems.\n",
    "Training usually needs to be done several times before a deployable solution is arrived at."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Which Algorithm to Use? - Some practical Tips\n",
    "***\n",
    "* Hence, rapid training time for penalized linear methods makes them useful for the obvious reason that shorter is better. \n",
    "* However, Depending on the problem, these methods may suffer some performance disadvantages relative to ensemble methods.\n",
    "* Therefore, penalized linear methods can be a useful first step in your development process even in the circumstance where they yield inferior performance to ensemble methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Which Algorithm to Use? - Some practical Tips\n",
    "***\n",
    "* Besides enjoying a training time advantage, penalized linear methods generate predictions much faster than ensemble methods. \n",
    "* Generating a prediction involves using the trained model. The trained model for penalized linear regression is simply a list of real numbers—one for each feature being used to make the predictions. \n",
    "* The number of floating‐point operations involved is the number of variables being used to make predictions. \n",
    "* For highly time‐sensitive predictions such as high‐speed trading or Internet ad insertions, computation time makes the difference between making money and losing money. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Which Algorithm to Use? - Some practical Tips\n",
    "***\n",
    "* On the other hand ensemble methods bring to the table the ability to work with nonlinear data\n",
    "* We can also easily control the complexity of ensemble models by tuning the hyperparameters\n",
    "* Also, ensemble methods come with the ability to tell apart important features from relatively redundant ones. Which is one of the huge advantages of ensemble methdos\n",
    "* Hence, ensemble methods could be used as the final predictors after feature engineering and feature selection has been carried out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Which Algorithm to Use? - Some practical Tips\n",
    "***\n",
    "![](../images/image34.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Which Algorithm to Use? - Some practical Tips\n",
    "***\n",
    "![](../images/image35.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lucius's Encounter to Unsupervised Machine Learning\n",
    "***\n",
    "Now that John and Lucius had ventured into the Supervised Machine Learning for some time, they grew curious more about the part they had not focused on a lot - Unsupervised Machine Learning.\n",
    "\n",
    "\"One of my professors had mentioned that Unsupervised Machine Learning techniques are extremely useful in solving the problem pertaining to **clustering, dimensionality reduction and predicting PDF of a sample**\" recalled Lucius, hardly understanding what that meant.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Applications of Unsupervised Machine Learning\n",
    "***\n",
    "Lucius quickly went and fetched his laptop computer, and started searching these words. The first link that showed up was that of Wikipedia, which said this about **dimensionality reductions**\n",
    "\n",
    "> In machine learning and statistics, dimensionality reduction or dimension reduction is the process of reducing the number of random variables under consideration, via obtaining a set of principal variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Applications of Unsupervised Machine Learning\n",
    "***\n",
    "\"Sounds interesting!\" said Lucius, searching for the next key-word - **density estimation**\n",
    "\n",
    "> In statistics, kernel density estimation (KDE) is a non-parametric way to estimate the probability density function of a random variable. Kernel density estimation is a fundamental data smoothing problem where inferences about the population are made, based on a finite data sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Applications of Unsupervised Machine Learning\n",
    "***\n",
    "\"Wow! This is something even more interesting! I wonder what Wiki has to say about clustering\" muttered Lucius pressing the enter for the search string - **Clustering**\n",
    "\n",
    "> Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense or another) to each other than to those in other groups (clusters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Ohh! the clustering sounds easy! Let's dig a little bit more and what clustering is all about.\" Some more searches and Lucius was reading:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "## So What is Clustering Anyway?\n",
    "***\n",
    "Clustering is Organizing data into clusters such that there is\n",
    "* high intra-cluster similarity\n",
    "* low inter-cluster similarity\n",
    "\n",
    "Informally, finding natural groupings among objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**But, Finding similarities is not always that easy!**\n",
    "\n",
    "![](../images/image01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So What is Clustering Anyway?\n",
    "***\n",
    "Lucius read further:\n",
    "\n",
    "* Cluster analysis itself is not one specific algorithm, but the general task to be solved.\n",
    "* It can be achieved by various algorithms that differ significantly in their notion of \n",
    "    * what constitutes a cluster, and\n",
    "    * how to efficiently find them.\n",
    "    \n",
    "Let's see how some of them work. But before that, let's understand some applications and types of clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applications of Clustering - Unsupervised Machine Learning\n",
    "***\n",
    "Clustering is an extremely useful technique in **both supervised and unsupervised machine learning**\n",
    "\n",
    "\n",
    "\n",
    "* Clustering is one of the most popular techniques for spotting the underlying pattern in a dataset\n",
    "* Some of the use cases are:\n",
    "    * Customer segmentation\n",
    "    * Locating an optimum location for a business outlet\n",
    "    * Clustering web-pages/documents based on their content "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applications of Clustering - Supervised Machine Learning\n",
    "***\n",
    "\n",
    "#### Exploratory Data Analysis\n",
    "* Which observations are nearer to each-other\n",
    "\n",
    "#### Feature Engineering\n",
    "* Missing Value Imputation\n",
    "* Outlier Detection\n",
    "* As Independent Variables / Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "## Types of Clustering:\n",
    "***\n",
    "**Flat or Partitional clustering**, in which, we try to partition the dataset into predefined different number of groups. These partitions are independent of each other. Some of the examples are:\n",
    "* K-means\n",
    "* Gaussian Mixture\n",
    "    \n",
    "    \n",
    "![](../images/image03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Clustering:\n",
    "***\n",
    "**Hierarchical clustering**, in which,\n",
    "\n",
    "* Partitions can be **visualized using a tree structure** (aka dendrogram)\n",
    "* Does not need the number of clusters as input\n",
    "* Possible to view partitions at different levels of granularities (i.e., can refine/coarsen clusters) using different K\n",
    "    \n",
    "![](../images/image02.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! now that we understand basic things about clustering, let's go on to understanding the first clustering algorithm - **K-means**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means Clustering\n",
    "***\n",
    "Many clustering algorithms are available in `Scikit-Learn` and elsewhere, but perhaps the simplest to understand is known as k-means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "## K-means Clustering\n",
    "***\n",
    "k-means clustering aims to partition **n observations** into **k clusters** in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clusters in \"k-means clustering\" follow these two underlying rules \n",
    "* The \"cluster center\" is the arithmetic mean of all the points belonging to the cluster.\n",
    "* Each point is closer to its own cluster center than to other cluster centers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sounds easy? Let's try and understand this in a bit more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/Maths-Insight.png\" alt=\"Maths-Insight\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "## K-means: A bit of math\n",
    "***\n",
    "The K-means objective function for k-means as follows:\n",
    "* Let µ1, . . . , µK be the K cluster centroids (means)\n",
    "* Let $r_{nk}$ ∈ {0, 1} be indicator denoting whether point $x_n$ belongs to cluster k\n",
    "\n",
    "\n",
    "K-means objective minimizes the total distortion (sum of distances of points from their cluster centers)\n",
    "\n",
    "$$J(µ,r) = \\sum_{n=1}^{N}\\sum_{k=1}^{K}r_{nk}||X_n − µ_k ||^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/Maths-Insight.png\" alt=\"Maths-Insight\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "## K-means: A bit of math\n",
    "***\n",
    "The exact optimization of the K-means objective is **NP-hard**, which means that actually solving the problem is computationally very expensive, however, given a solution, checking whether it is correct or not is relatively easy!\n",
    "\n",
    "\n",
    "This means that actually solving the problem and coming up with cluster centroids is computationally very expensive. However, given a set of cluster centroids, checking if they are good approximations is computationally cheap. Therefore, the **K-means algorithm is performed using a heuristic** that helps us converge to a solution faster. Let's see how that works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/Maths-Insight.png\" alt=\"Maths-Insight\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "## K-means: Expectation–Maximization\n",
    "***\n",
    "\n",
    "k-means is a particularly simple and easy-to-understand application of an iterative algorithm known as **Expectation–Maximization**, and we will walk through it briefly here. \n",
    "\n",
    "\n",
    "The expectation–maximization approach here consists of the following procedure:\n",
    "1. Guess some cluster centers\n",
    "2. Repeat until converged\n",
    "    * E-Step: assign points to the nearest cluster center\n",
    "    * M-Step: set the cluster centers to the mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More on Expectation–Maximization\n",
    "***\n",
    "\n",
    "* Here the \"E-step\" or \"Expectation step\" is so-named because it involves updating our expectation of which cluster each point belongs to. \n",
    "* The \"M-step\" or \"Maximization step\" is so-named because it involves maximizing some fitness function that defines the location of the cluster centers — in this case, that maximization is accomplished by taking a simple mean of the data in each cluster.\n",
    "\n",
    "This algorithm can be summarized as follows: **Under typical circumstances, each repetition of the E-step and M-step will always result in a better estimate of the cluster characteristics.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now, let's see how this heuristic is performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means Clustering Algorithm\n",
    "***\n",
    "* **Step 1**: Start by making a guess on where the central points of each cluster are. Let’s call these pseudo-centers, since we do not yet know if they are actually at the center of their clusters.\n",
    "* **Step 2**: Assign each data point to the nearest pseudo-center. By doing so, we have just formed clusters, with each cluster comprising all data points associated with its pseudo-center.\n",
    "* **Step 3**: Update the location of each cluster’s pseudo-center, such that it is now indeed in the center of all its members.\n",
    "* **Step 4**: Repeat the steps of re-assigning cluster members (Step 2) and re-locating cluster centers (Step 3), until there are no more changes to cluster membership."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/image23.png)\n",
    "\n",
    "Let's understand the k-means clustering algorithm using following **gifs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**gif 1: 2 Clusters Example**\n",
    "\n",
    "\n",
    "![](../images/image20.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**gif 2: Multiple Clusters Example**\n",
    "\n",
    "![](https://datasciencelab.files.wordpress.com/2013/12/p_n2000_k15_.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Popularity of k-means\n",
    "***\n",
    "K-Means is the 'go-to' clustering algorithm for many because it is \n",
    "* Fast\n",
    "* Easy to understand\n",
    "* Available everywhere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Popularity of k-means\n",
    "***\n",
    "To demonstrate the popularity of k-means, here are some variants. And no, we are not going to cover them!\n",
    "* K-Means\n",
    "* K-Means++ (only changes how to initialize centroids)\n",
    "* Online K-Means\n",
    "* Spherical K-Means\n",
    "* K-Medoids\n",
    "* Kernel K-Means\n",
    "* K-Modes\n",
    "* Bisecting K-Means\n",
    "* Fuzzy C-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/Technical-Stuff.png\" alt=\"Technical-Stuff\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "## K-means clustering in `sklearn`\n",
    "***\n",
    "Many clustering algorithms are available in Scikit-Learn and elsewhere, but perhaps the simplest to understand is k-means clustering, which is implemented in **`sklearn.cluster.KMeans`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(init=\"random\", n_clusters=5)\n",
    "km.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km.predict(X[1, np.newaxis])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/Technical-Stuff.png\" alt=\"Technical-Stuff\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "# Synthetic data for testing clustering algorithms\n",
    "***\n",
    "We'll create (and plot) synthetic datasets to understand k-means clustering better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn.cluster as cluster\n",
    "import time\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_context('poster')\n",
    "sns.set_color_codes()\n",
    "plot_kwds = {'alpha' : 0.25, 's' : 80, 'linewidths':0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate two clusters: a with 100 points, b with 50:\n",
    "np.random.seed(9)  # for repeatability of this tutorial\n",
    "a = np.random.multivariate_normal([10, 0], [[3, 1], [1, 4]], size=[100,])\n",
    "b = np.random.multivariate_normal([0, 20], [[3, 1], [1, 4]], size=[50,])\n",
    "X = np.concatenate((a, b),)\n",
    "print(X.shape)  # 150 samples with 2 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0], X[:,1], c='b', **plot_kwds)\n",
    "frame = plt.gca()\n",
    "frame.axes.get_xaxis().set_visible(False)\n",
    "frame.axes.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.random.multivariate_normal([40, 40], [[20, 1], [1, 30]], size=[200,])\n",
    "d = np.random.multivariate_normal([80, 80], [[30, 1], [1, 30]], size=[200,])\n",
    "e = np.random.multivariate_normal([0, 100], [[100, 1], [1, 100]], size=[200,])\n",
    "X2 = np.concatenate((X, c, d, e),)\n",
    "plt.scatter(X2[:,0], X2[:,1],  c='b', **plot_kwds)\n",
    "frame = plt.gca()\n",
    "frame.axes.get_xaxis().set_visible(False)\n",
    "frame.axes.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters(data, algorithm, args, kwds):\n",
    "    start_time = time.time()\n",
    "    labels = algorithm(*args, **kwds).fit_predict(data)\n",
    "    end_time = time.time()\n",
    "    palette = sns.color_palette('deep', np.unique(labels).max() + 1)\n",
    "    colors = [palette[x] if x >= 0 else (0.0, 0.0, 0.0) for x in labels]\n",
    "    plt.scatter(data.T[0], data.T[1], c=colors, **plot_kwds)\n",
    "    frame = plt.gca()\n",
    "    frame.axes.get_xaxis().set_visible(False)\n",
    "    frame.axes.get_yaxis().set_visible(False)\n",
    "    plt.title('Clusters found by {}'.format(str(algorithm.__name__)), fontsize=24)\n",
    "    plt.text(5, 10, 'Clustering took {:.2f} s'.format(end_time - start_time), fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(X, cluster.KMeans, (),{'n_clusters':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(X2, cluster.KMeans, (), {'n_clusters':5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, now that we understand about k-means better, there a few things we need to keep in mind as well. K-means have some shortcomings which we need to understand if want to use the technique successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortcomings of k-means\n",
    "***\n",
    "**The globally optimal result may not be achieved**\n",
    "* There is no assurance that it will lead to the global best solution. \n",
    "* For example, if we use a different random seed in our simple procedure, the particular starting guesses lead to poor results\n",
    "\n",
    "**Way out**\n",
    "* For this reason, it is common for the algorithm to be run for multiple starting guesses, as indeed Scikit-Learn does by default (set by the n_init parameter, which defaults to 10).\n",
    "* Use better initialization strategies like k-means++\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/Technical-Stuff.png\" alt=\"Technical-Stuff\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "## Shortcomings of k-means\n",
    "***\n",
    "**You need to specify exactly how many clusters you expect**\n",
    "* If you know a lot about your data then that is something you might expect to know\n",
    "* If, on the other hand, you are simply exploring a new dataset, then 'number of clusters' is a hard parameter to have any good intuition for\n",
    "\n",
    "**Way out**\n",
    "\n",
    "The usually proposed solution is to run K-Means for many different 'number of clusters' values and score each clustering with some 'cluster goodness' measure (usually a variation on intra-cluster vs inter-cluster distances) and attempt to find an 'elbow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k means determine k\n",
    "distortions = []\n",
    "K = range(1,8)\n",
    "for k in K:\n",
    "    kmeanModel = KMeans(n_clusters=k)\n",
    "    kmeanModel.fit(X)\n",
    "    distortions.append(kmeanModel.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the elbow\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k means determine k\n",
    "distortions = []\n",
    "K = range(1,12)\n",
    "for k in K:\n",
    "    kmeanModel = KMeans(n_clusters=k)\n",
    "    kmeanModel.fit(X2)\n",
    "    distortions.append(kmeanModel.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the elbow\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, as we can see the there is an elbow created at k=2 and k=4, which marks sudden drop in distortions, which indicates that k=2 and 4 might be the correct number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortcomings of k-means\n",
    "***\n",
    "**k-means is limited to linear cluster boundaries**\n",
    "* The fundamental model assumptions of k-means (points will be closer to their own cluster center than to others) means that the algorithm will often be ineffective if the clusters have complicated geometries.\n",
    "* In particular, the boundaries between k-means clusters will always be linear, which means that it will fail for more complicated boundaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "X3, y3 = make_moons(200, noise=.05, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = KMeans(2, random_state=0).fit_predict(X3)\n",
    "plt.scatter(X3[:, 0], X3[:, 1], c=labels, s=50, cmap='viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/Technical-Stuff.png\" alt=\"Technical-Stuff\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "### Way out\n",
    "***\n",
    "One way to overcome this is through kernelized k-means, which is implemented in `Scikit-Learn` within the `SpectralClustering` estimator. It uses the graph of nearest neighbors to compute a higher-dimensional representation of the data, and then assigns labels using a k-means algorithm\n",
    "\n",
    "We will not be covering these topics. However, here is [sklearn's page on various clustering techniques](http://scikit-learn.org/stable/modules/clustering.html). It is good starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "model = SpectralClustering(n_clusters=2, affinity='nearest_neighbors', assign_labels='kmeans')\n",
    "labels = model.fit_predict(X3)\n",
    "plt.scatter(X3[:, 0], X3[:, 1], c=labels, s=50, cmap='viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortcomings of k-means\n",
    "***\n",
    "**It is a partitioning algorithm**\n",
    "* It partitions your dataset into as many (assumed to be globular) chunks as you ask for by attempting to minimize intra-partition distances\n",
    "* Can not accommodate clusters of different shapes (other than the one determined by its distance metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortcomings of k-means\n",
    "***\n",
    "2. Inability to identify non-linear boundaries\n",
    "\n",
    "![](../images/image22.1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Shortcomings of k-means\n",
    "***\n",
    "![](../images/image25.1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortcomings of k-means\n",
    "***\n",
    "**k-means can be slow for large numbers of samples**\n",
    "\n",
    "* Because each iteration of k-means must access every point in the dataset, the algorithm can be relatively slow as the number of samples grows. \n",
    "\n",
    "**Way out**\n",
    "* One way to overcome this problem is through relaxing requirement to use all data at each iteration can be relaxed.\n",
    "* For example, we might just use a subset of the data to update the cluster centers at each step. \n",
    "* This is the idea behind **batch-based k-means algorithms**, one form of which is implemented in **`sklearn.cluster.MiniBatchKMeans`**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortcomings of k-means\n",
    "***\n",
    "Read [this fabulous post on cross-validated](https://stats.stackexchange.com/questions/133656/how-to-understand-the-drawbacks-of-k-means) for a more detailed understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice. Seems like K-means is one heck of a clustering algorithm. Now, let's understand a thing or two about another clustering algorithm family known as Hierarchical Clustering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "## Hierarchical Clustering\n",
    "***\n",
    "\n",
    "\n",
    "Hierarchical clustering is where we build a cluster tree (aka dendrogram) to represent data, where each group (or “node”) is linked to two or more successor groups. \n",
    "* The groups are nested and organized as a tree, which ideally ends up as a meaningful classification scheme.\n",
    "* Each node in the cluster tree contains a group of similar data; Nodes are placed on the graph next to other, similar nodes. \n",
    "* Clusters at one level are joined with clusters in the next level up, using a degree of similarity; \n",
    "* The process carries on until all nodes are in the tree, which gives a visual snapshot of the data contained in the whole set. \n",
    "* The total number of clusters is not predetermined before you start the tree creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://www.statisticshowto.com/wp-content/uploads/2016/11/clustergram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Hierarchical Clustering\n",
    "***\n",
    "\n",
    "There are two major ways in which hierarchical clustering can be carried out:\n",
    "\n",
    "1. Agglomerative or Bottom-Up Clustering\n",
    "2. Divisive or Top-Down Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agglomerative (bottom-up) Clustering\n",
    "***\n",
    "1. Start with each example in its own singleton cluster\n",
    "2. At each time-step, greedily merge 2 most similar clusters\n",
    "3. Stop when there is a single cluster of all examples, else go to 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divisive (top-down) Clustering\n",
    "***\n",
    "\n",
    "1. Start with all examples in the same cluster\n",
    "2. At each time-step, remove the “outsiders” from the least cohesive cluster\n",
    "3. Stop when each example is in its own singleton cluster, else go to 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "## Which one to Use?\n",
    "***\n",
    "* Agglomerative Clustering is simpler to implement because for top-down clustering, we need a second, flat clustering algorithm as a ``subroutine''. \n",
    "* However, top-Down routine has the advantage of being more efficient if we do not generate a complete hierarchy all the way down to individual document leaves. \n",
    "* For a fixed number of top levels, using an efficient flat algorithm like  $K$-means, top-down algorithms are linear in the number of documents and clusters.\n",
    "* There is also evidence that divisive algorithms produce more accurate hierarchies than bottom-up algorithms in some circumstances according to [this Stanford University Review](https://nlp.stanford.edu/IR-book/html/htmledition/references-and-further-reading-17.html#sec:hclstfurther) . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deciding (Dis)similarity between clusters\n",
    "***\n",
    "In both techniques above, we discussed finding out the similarity or dissmilarity between two cluster. The question is -- How measure them?\n",
    "\n",
    "\n",
    "Before any clustering is performed, it is required to determine the **proximity matrix containing the distance between each point using a distance function**. \n",
    "Then, the matrix is updated to display the distance between each cluster. \n",
    "The following three methods differ in how the distance between each cluster is measured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Linkage\n",
    "***\n",
    "In single linkage hierarchical clustering, the distance between two clusters is defined as the shortest distance between two points in each cluster. For example, the distance between clusters “r” and “s” to the left is equal to the length of the arrow between their two closest points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/image05.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Linkage\n",
    "***\n",
    "In complete linkage hierarchical clustering, the distance between two clusters is defined as the longest distance between two points in each cluster. For example, the distance between clusters “r” and “s” to the left is equal to the length of the arrow between their two furthest points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/image06.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Linkage\n",
    "***\n",
    "In average linkage hierarchical clustering, the distance between two clusters is defined as the average distance between each point in one cluster to every point in the other cluster. For example, the distance between clusters “r” and “s” to the left is equal to the average length each arrow between connecting the points of one cluster to the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/image07.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Scipy` has a really convenient api for carrying out hierarchical clustering. Let's see how it works. We start with necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed imports\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the linkage matrix\n",
    "Z = linkage(X, 'average')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate full dendrogram\n",
    "plt.figure(figsize=(25, 10))\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('sample index')\n",
    "plt.ylabel('distance')\n",
    "dendrogram(\n",
    "    Z,\n",
    "    leaf_rotation=90.,  # rotates the x axis labels\n",
    "    leaf_font_size=8.,  # font size for the x axis labels\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate full dendrogram\n",
    "Z2 = linkage(X2, 'average')\n",
    "plt.figure(figsize=(25, 10))\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('sample index')\n",
    "plt.ylabel('distance')\n",
    "dendrogram( Z2, \n",
    "           leaf_rotation=90.,  # rotates the x axis labels\n",
    "           leaf_font_size=8.\n",
    "           )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single vs Complete vs Average Linkage:\n",
    "***\n",
    "* A concern of using single\n",
    "linkage is that it can sometimes produce chaining\n",
    "amongst the clusters. This means that several clusters\n",
    "may be joined together simply because one of their\n",
    "cases is within close proximity of case from a separate\n",
    "cluster. This problem is specific to single linkage due to\n",
    "the fact that the smallest distance between pairs is the\n",
    "only value taken into consideration. Because the steps\n",
    "in agglomerative hierarchical clustering are\n",
    "irreversible, this chaining effect can have disastrous\n",
    "effects on the cluster solution. \n",
    "* In\n",
    "complete linkage, outlying cases prevent close clusters\n",
    "to merge together because the measure of the furthest\n",
    "neighbour exacerbates the effects of outlying data.\n",
    "* Average Linkage is supposed to represent a\n",
    "natural compromise between the linkage measures to\n",
    "provide a more accurate evaluation of the distance\n",
    "between clusters. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortcomings\n",
    "***\n",
    "One of the biggest drawbacks of hierarchical clustering is it is extremely calculation heavy. Hence they are not scalable. That also means that they are not very useful for larger datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flat vs Hierarchical Clustering\n",
    "***\n",
    "* Flat clustering produces a single partitioning\n",
    "* Hierarchical Clustering can give different partitions depending on the level-of-resolution we are looking at\n",
    "\n",
    "\n",
    "* Flat clustering needs the number of clusters to be specified\n",
    "* Hierarchical clustering doesn’t need the number of clusters to be specified\n",
    "\n",
    "\n",
    "* Flat clustering is usually more efficient run-time wise\n",
    "* Hierarchical clustering can be slow (has to make several merge/split decisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-class Activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credit Card Data analysis\n",
    "\n",
    "Segmentation in marketing is a technique used to divide customers or other entities into groups based on attributes such as behaviour or demographics.\n",
    "\n",
    "The top challenge faced by marketers is understanding who they are selling to. Once you know your buyer personas, you can tailor your targeting and offerings to increase their satisfaction and your revenue as a result. When you already have a pool of customers and plenty of data, it can be incredibly useful to segment them.\n",
    "\n",
    "Here we will be using Credit card data to segment the customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the dataset\n",
    "The credit card data has 18 attributes for each customer, which include the balance (credit owed by the customer), cash advance (when a customer withdraws cash using the credit card), the customer’s credit limit, minimum payment, percentage of full payments and tenure. A complete data dictionary info is given below:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "|Feature|Description|\n",
    "|-----|-----|\n",
    "|CUST_ID| Identification of Credit Card holder (Categorical)| \n",
    "|BALANCE | Balance amount left in their account to make purchases| \n",
    "|BALANCE_FREQUENCY | How frequently the Balance is updated, score between 0 and 1(1 = frequently updated, 0 = not frequently updated |\n",
    "|PURCHASES | Amount of purchases made from account| \n",
    "|ONEOFF_PURCHASES | Maximum purchase amount done in one-go| \n",
    "|INSTALLMENTS_PURCHASES | Amount of purchase done in installment| \n",
    "|CASH_ADVANCE | Cash in advance given by the user |\n",
    "|PURCHASES_FREQUENCY | How frequently the Purchases are being made, score between 0 and 1 (1 = frequently purchased, 0 = not frequently purchased) |\n",
    "|ONEOFFPURCHASESFREQUENCY | How frequently Purchases are happening in one-go (1 = frequently purchased, 0 = not frequently purchased) |\n",
    "|PURCHASESINSTALLMENTSFREQUENCY | How frequently purchases in installments are being done (1 = frequently done, 0 = not frequently done) |\n",
    "|CASHADVANCEFREQUENCY | How frequently the cash in advance being paid |\n",
    "|CASHADVANCETRX | Number of Transactions made with \"Cash in Advanced\" |\n",
    "|PURCHASES_TRX | Numbe of purchase transactions made |\n",
    "|CREDIT_LIMIT | Limit of Credit Card for user| \n",
    "|PAYMENTS | Amount of Payment done by user |\n",
    "|MINIMUM_PAYMENTS | Minimum amount of payments made by user| \n",
    "|PRCFULLPAYMENT | Percent of full payment paid by user |\n",
    "|TENURE | Tenure of credit card service for user|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import scale\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/customer_seg.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets check the descriptive Statistics of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code starts here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with missing values\n",
    "Lets check the number of missing values in the given dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code starts here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Impute these missing values with mean and remove `CUST_ID` which is not useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code starts here\n",
    "\n",
    "# Imputing for Minimum_payments column\n",
    "\n",
    "\n",
    "# Imputing for Credit_Limit column\n",
    "\n",
    "\n",
    "# Dropping the CUST_ID \n",
    "\n",
    "\n",
    "# Checking again if any missing values are there or not \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform log transformation on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.copy()\n",
    "cols =  ['BALANCE',\n",
    "         'PURCHASES',\n",
    "         'ONEOFF_PURCHASES',\n",
    "         'INSTALLMENTS_PURCHASES',\n",
    "         'CASH_ADVANCE',\n",
    "         'CASH_ADVANCE_TRX',\n",
    "         'PURCHASES_TRX',\n",
    "         'CREDIT_LIMIT',\n",
    "         'PAYMENTS',\n",
    "         'MINIMUM_PAYMENTS',\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code starts here\n",
    "\n",
    "# Note: Adding 1 for each value to avoid inf values\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Detect outliers in the continuous columns \n",
    "\n",
    "As this is a clustering problem, I decided to test without outlier's replacement because to get the meaningful clusters and should make sense after plotting the pair graph.  \n",
    "\n",
    "We will be Using IRQ Score to identify outliers values in dataset. IRQ method is used in boxplot to identify possible outliers values.\n",
    "\n",
    "```python\n",
    "The interquartile range (IQR), also called the midspread or middle 50%, or technically H-spread, is a measure of statistical dispersion, being equal to the difference between 75th and 25th percentiles, or between upper and lower quartiles, IQR = Q3 − Q1. In other words, the IQR is the first quartile subtracted from the third quartile; these quartiles can be clearly seen on a box plot on the data. It is a measure of the dispersion similar to standard deviation or variance, but is much more robust against outliers.\n",
    "```\n",
    "For now, we`ll do nothing with outliers because this may harm the clustering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code starts here\n",
    "\n",
    "# Function to detect outliers in every feature\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the outliers using box plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code starts here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale the features using scale function. This function will put all variables at the same scale, with mean zero and standard deviation equals to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code starts here\n",
    "\n",
    "# Scale All features\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the elbow method find the optimal number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the elbow method to find the optimal number of clusters\n",
    "\n",
    "X = np.array(features)\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Code starts here\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the graph to visualize the Elbow Method to find the optimal number of cluster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the graph to visualize the Elbow Method to find the optimal number of cluster  \n",
    "\n",
    "\n",
    "# Code starts here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying KMeans to the dataset with the optimal number of cluster and store the clusters in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code starts here\n",
    "\n",
    "# Applying KMeans to the dataset with the optimal number of cluster\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding clusters to main dataframe\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the different clusters\n",
    "\n",
    "# Code starts here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/Recap.png\" alt=\"Recap\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "# In-session Recap Time\n",
    "***\n",
    "* Imbalanced Data\n",
    "* Resampling Techniques\n",
    "* Undersampling & Oversampling\n",
    "* Algorithmic Approach\n",
    "* Dealing with smaller data sets\n",
    "* Which Algorithm to use\n",
    "* Applications of Unsupervised Learning\n",
    "* Clustering\n",
    "* Types of Clustering\n",
    "* K-means Clustering\n",
    "* Shortcomings of K-means Clustering\n",
    "* Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Thank You"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
