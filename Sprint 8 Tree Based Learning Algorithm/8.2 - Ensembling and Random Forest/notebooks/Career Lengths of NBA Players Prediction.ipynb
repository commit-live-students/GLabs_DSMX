{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/Ensemble_methods.jpg\" alt=\"ensemble methods\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Program so far\n",
    "\n",
    "***\n",
    "\n",
    "- Basics of Python\n",
    "- Descriptive and Inferential Statistics\n",
    "- Linear Regression\n",
    "- L1/L2 Regularization\n",
    "- Basic data cleaning and Preprocessing\n",
    "- Feature extraction and Feature engineering\n",
    "- Logistic Regression\n",
    "- Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Agenda\n",
    "***\n",
    "- What is ensembling?\n",
    "- Types of ensembling\n",
    "- Naive aggregation or voting\n",
    "- Bootstrap Aggregating or Bagging\n",
    "- Stacking\n",
    "- Introducing Random Forests\n",
    "- Hyperparameters for Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Lucius goes to John\n",
    "***\n",
    "Lucius goes on a pilot trip to visit certain universities in Brooklyn. On his way back he decided to pay a visit to John. He got a six-pack of Heineken a rushed to John's place.\n",
    "\n",
    "A much more confident John, played an excellent host to Lucius. While discussing, Lucius discussed a few issues that he was facing. Lucius was through with Decision Trees, but being an Applied Math enthusiast, he was not satisfied with the result. (\"You know how they are!\") He was wondering if anything better could be done.\n",
    "\n",
    "John invited Jay over as he lived only a couple of blocks away."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Jay Helps\n",
    "***\n",
    "Jay patiently paid an ear to Lucius's problem, and beamed with excitement, **what's better than one learner? -- multiple learners!**\n",
    "\n",
    "Let's understand more, but before that, let's get over with the routine stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dataframe = pd.read_csv('../data/loan_prediction.csv')\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Split the original data into train & test data, with the column `Loan Status` as the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = dataframe.iloc[:, :-1]\n",
    "y = dataframe.iloc[:, -1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## More is better than one! (1/2)\n",
    "***\n",
    "The idea of **the wisdom of crowds** has been popular since as far back we go, and for a reason. It has been found that the collective intelligence of many often surpasses the intelligence of a single expert.\n",
    "\n",
    "So we don't need to be experts to understand that using multiple learners to make predictions would help!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## More is better than one! (2/2)\n",
    "***\n",
    "\n",
    "Methods of improving model performance by aggregating predictions over several learners are known as **ensemble methods.** Ensembling helps improvise on the stability and predictive power of the model.\n",
    "\n",
    "Ensemble modeling is a powerful way to improve the performance of your model. It usually pays off to apply ensemble learning over and above various models you might be building. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Introduction to Ensemble Methods\n",
    "***\n",
    "Let's explore this technique of Ensembling using a philosophical thought process.\n",
    "\n",
    "#### Condorcet’s Jury Theorem\n",
    "Let's say a jury of voters need to make a decision regarding a binary outcome (for example to convict a defendant or not).\n",
    "\n",
    "If each voter has a probability p of being correct and the probability of a majority of voters being correct is L, then **L > p if p > 0.5** if the voters as independent from each other. Interestingly, **L approaches 1 as the number of voters approaches infinity**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Introduction to Ensemble Methods\n",
    "***\n",
    "#### Condorcet’s Jury Theorem\n",
    "\n",
    "In human language, p > 0.5 means that the individual judgments (votes) are at least a little better than random chance.\n",
    "\n",
    "Now, let's take this analogy to the world of ML:\n",
    "\n",
    "* Verdict --> classification prediction\n",
    "* Jury members --> ML models\n",
    "* votes --> individual predictions\n",
    "\n",
    "This means that employing multiple ML models should improve the performance according to the Condorcet's theorem, and it does!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Building an intuition (1/2)\n",
    "***\n",
    "Let's say I am interested in buying the new iphone model since I have an extra kidney anyway.\n",
    "\n",
    "However, I am not sure if it is a good model or not. So I ask a bunch of people\n",
    "\n",
    "1. A mobile shopkeeper, whose opinions about a mobile phone model are 80% times correct\n",
    "2. A YouTube gadget reviewer, who is 70% times correct in her opinions about a gadget\n",
    "3. My friend, who is 60% of times correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Building an intuition (2/2)\n",
    "***\n",
    "I decide that I will buy the phone if all of them recommend it, and all of them do! In such a case, what is the probability of the new phone turning out to be a bad model?\n",
    "\n",
    "It will be same as the probability of all of them being wrong simultaneously \n",
    "\n",
    ">* P = (1 - 0.8) x (1 - 0.7) x (1 - 0.6)\n",
    ">* P = 0.024\n",
    "\n",
    "Which means that there is 97.6% chance that the phone will be good, given their opinions are independent from each other.\n",
    "\n",
    "Ensemble works on similar principles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "## Strong Learners vs Weak Learners\n",
    "***\n",
    "We discussed how we only need a large number of learners, whose predictive power is just slightly better than random chance (tossing a coin in case of binary classification problem!) for ensembling to work. Such learners have a special name -- \"weak learners\".\n",
    "\n",
    "* **Weak Learner:**\n",
    "Given a labeled dataset, a Weak Learner produces a classifier which is at least a little more accurate than random classification.\n",
    "* **Strong Learner:**\n",
    "We call a machine learning model a Strong Learner which, given a labeled dataset, can produce a classifier to arbitrary accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "## Ensemble of Weak Learner\n",
    "***\n",
    "Given the formulation above, the question we want to ask is this:\n",
    "      \n",
    "**Can an ensemble of weak classifiers produce a single strong classifier?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "## Ensemble of Wise Learners (1/2)\n",
    "***\n",
    "To build ensemble models, we combine multiple models using different methods and hope that the wisdom of the crowd outperforms any individual model. \n",
    "\n",
    "Naturally, not all crowds are wise (for example, greedy investors of a stock market bubble). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "## Ensemble of Wise Learners (2/2)\n",
    "***\n",
    "**Surowiecki** presents a simple framework to evaluate if a given crowd is wise:\n",
    "* **Independence :** Members’ opinions are not determined by the opinions of those around them.\n",
    "* **Diversity of opinion :** Each member should have private information even if it is just an eccentric interpretation of the known facts.\n",
    "* **Decentralization :** Members are able to specialize and draw conclusions based on local knowledge.\n",
    "* **Aggregation :** Some mechanism exists for turning private judgments into a collective decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Surowiecki's framework helps us make sure that the ensemble of ML learners improve overall performance by\n",
    "\n",
    "* Decreasing the variance \n",
    "* Decreasing the bias\n",
    "* Improving the predictive force"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How Ensemble Method Work\n",
    "***\n",
    "Lucius, being in a hurry was keen to jump on how to do things rather than fundamental concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## So How does Ensemble Method Work\n",
    "***\n",
    "Every ensemble algorithm consists of two steps:\n",
    "\n",
    "* Producing a cohort of predictions using simple ML algorithms.\n",
    "* Combining the predictions into one \"aggregated\" model.\n",
    "\n",
    "Ensemble can be achieved through several techniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Back to Lucius\n",
    "***\n",
    "As Lucius has a background of applied maths, he has built a strong intuition over the years. Lucius pointed out that the most obvious and intuitive way would be to average out all the possibilities and that would be the final output. Indeed, it was.\n",
    "\n",
    "He was referring to an ensemble method called **Aggregation** or **Voting Ensemble**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "## Naive Aggregation\n",
    "***\n",
    "Naive aggregation works by aggregating the final output through averaging (regression) or voting (classification).\n",
    "\n",
    "- A more sophisticated ensemble might assign weights to the predictions by different learners while aggregating.\n",
    "\n",
    "- Works best with algorithms which learn very differently from each other, thereby complementing each others' decisions\n",
    "\n",
    "**Brain teaser :**\n",
    "When does a voting classifier out do its base learner and when it doesn’t?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Technical-Stuff.png\" alt=\"Technical-Stuff\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "## Soft Voting vs Hard Voting\n",
    "***\n",
    "Since, every classification algorithm first calculates the probabilities of each outcome, and them produces the prediction, the aggregation could be done either on calculated probabilities, or final predictions.\n",
    "\n",
    "* In **hard voting**, the voting classifier takes majority of its base learners’ predictions\n",
    "* In **soft voting**, the voting classifier takes into account the probability values by its base learners \n",
    "\n",
    "In general, Soft voting has been observed to perform better than hard voting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's apply Soft Voting and Hard Voting on the loan prediction dataset. We can pass in a number of classifiers to it. We also check the accuracy by both these methods. We will be using the `VotingClassifier` class in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "decision_clf1 = DecisionTreeClassifier()\n",
    "decision_clf2 = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Hard Voting\n",
    "voting_clf_hard = VotingClassifier(estimators = [('Logistic Regression', log_clf),\n",
    "                                                 ('Decision Tree 1', decision_clf1),\n",
    "                                                 ('Decision Tree 2', decision_clf2)],\n",
    "                                   voting = 'hard')\n",
    "\n",
    "voting_clf_hard.fit(X_train, y_train)\n",
    "y_pred_hard = voting_clf_hard.predict(X_test)\n",
    "accuracy_hard = accuracy_score(y_test, y_pred_hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Soft Voting\n",
    "voting_clf_soft = VotingClassifier(estimators = [('Logistic Regression', log_clf),\n",
    "                                                 ('Decision Tree 1', decision_clf1),\n",
    "                                                 ('Decision Tree 2', decision_clf2)],\n",
    "                                   voting = 'soft')\n",
    "voting_clf_soft.fit(X_train, y_train)\n",
    "y_pred_soft = voting_clf_soft.predict(X_test)\n",
    "accuracy_soft = accuracy_score(y_test, y_pred_soft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Hard voting accuracy: %.4f\" %(accuracy_hard))\n",
    "print(\"Soft voting accuracy: %.4f\" %accuracy_soft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The idea of specialists:\n",
    "***\n",
    "Jay was thinking silently the whole time. After a while, he said, **what could better than the wisdom of crowds? --> wisdom of diverse experts!**.\n",
    "\n",
    "This reminded him of the *mantri mandal* of the kings in the older times, where each minister used be an expert of a particular area and the king would ask for opinions from them before taking any major decisions.\n",
    "\n",
    "Unknowingly, Jay was stumbling over a technique called **Bagging**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example (1/2)\n",
    "***\n",
    "Let’s understand this idea better through an example of a multi-speciality hospital. Let’s say the hospital has 3 medical interns which the management can train. Moreover, the hospital deals with 3 types of cases: \n",
    "* Heart diseases\n",
    "* Broken bones\n",
    "* Cancer\n",
    "\n",
    "Now, the management has 2 ways in which they can be trained:\n",
    "* All the interns handle all types of cases \n",
    "* Each intern handle a specific type of case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## We know how interns are..\n",
    "***\n",
    "![hospital](../images/hospital.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example (2/2)\n",
    "***\n",
    "If the management chooses option 1, they will create 3 doctors who can take care of a wide range of cases but since their exposure to each type of case is limited, they will be generalists: MBBS doctors\n",
    "\n",
    "On the other side, if the management lets each intern work on a single category of cases, they can become specialists in their respective areas: A cardiologist, an orthopedist and an oncologist.\n",
    "\n",
    "**Similarly, in bagging, by training each base learner on different sample of data, we make specialist base learners.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "## Bagging\n",
    "***\n",
    "Bagging stands for **B**ootstrap **Agg**regat**ing**.\n",
    "\n",
    "In ensemble algorithms, bagging methods form a class of algorithms which build several instances of a black-box estimator on random subsets of the original training set and then aggregate their individual predictions to form a final prediction. \n",
    "\n",
    "Unlike naive aggregator, bagging uses a single type of base learner. Bagging is a method that involves manipulating the training set by resampling. We learn k base classifiers on k different samples of training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The B in Bagging -- Bootstrapping\n",
    "***\n",
    "* The samples are independently created by resampling the training data using uniform weights\n",
    "\n",
    "* This means that the sampling of data points happens with replacement. The process of **sampling with replacement is called Bootstrapping.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bias - Variance trade-off\n",
    "***\n",
    "Because of bootstrapping, each individual predictor has **a higher bias** than if it were trained on the original training set. \n",
    "\n",
    "However, a large number of such biases will cancel each other out when aggregated, hence the bias of the resulting bagging is only slightly higher than a comparable single predictor strong learner. \n",
    "\n",
    "At the same time, because bagging provides a way to reduce overfitting, the variance of resulting strong learner reduced significantly.\n",
    "\n",
    "Generally, the net result is that the ensemble has a similar bias but a lower variance than a single predictor trained on the original training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bagging reduces Variance\n",
    "***\n",
    "![bagging](../images/image22.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Technical-Stuff.png\" alt=\"Technical-Stuff\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "## And bagging is practical!\n",
    "***\n",
    "* The learners can all be trained in parallel, via different CPU cores or even different servers. \n",
    "* Similarly, predictions can be made in parallel. \n",
    "* This is one of the reasons why bagging ensembles are such popular methods: **they scale very well.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now, lets apply Bagging to the above dataset.\n",
    "We will compare the accuracy of a single learner against an Ensemble of learners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "decision_clf = DecisionTreeClassifier()\n",
    "\n",
    "# Fitting single decision tree\n",
    "decision_clf.fit(X_train, y_train)\n",
    "y_pred_decision = decision_clf.predict(X_test)\n",
    "score_dt = accuracy_score(y_test, y_pred_decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Fitting single logistic reg\n",
    "\n",
    "log_clf.fit(X_train, y_train)\n",
    "y_pred_decision = log_clf.predict(X_test)\n",
    "score_lr = accuracy_score(y_test, y_pred_decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Fitting bagging classifier with Logisitc Regression\n",
    "bagging_clf1 = BaggingClassifier(LogisticRegression(), n_estimators=100, max_samples=100, \n",
    "                                bootstrap=True, random_state=9)\n",
    "\n",
    "bagging_clf1.fit(X_train, y_train)\n",
    "y_pred_bagging = bagging_clf1.predict(X_test)\n",
    "score_bc_lr = accuracy_score(y_test, y_pred_bagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Fitting bagging classifier with Decision Tree\n",
    "bagging_clf2 = BaggingClassifier(DecisionTreeClassifier(), n_estimators=100, max_samples=100, \n",
    "                                bootstrap=True, random_state=9)\n",
    "\n",
    "bagging_clf2.fit(X_train, y_train)\n",
    "y_pred_bagging = bagging_clf2.predict(X_test)\n",
    "score_bc_dt = accuracy_score(y_test, y_pred_bagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"decision tree accuracy %.4f\" %(score_dt))\n",
    "print(\"bagging classifier (decision tree) accuracy %.4f\" %(score_bc_dt))\n",
    "print(\"======================================\")\n",
    "print(\"logistic regression accuracy %.4f\" %(score_lr))\n",
    "print(\"bagging classifier (logistic regression) accuracy %.4f\" %(score_bc_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can easily observe that the accuracy of a BaggingClassifier is much better then that of a single decision tree, but not in case of logistic regression, why? -- more on this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "## Pasting\n",
    "***\n",
    "Just as in bagging we create samples through repeated resampling with replacement, we can create samples **with repeated resampling without replacement** for each base learner. Ensemble on such samples is known as **pasting**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bagging vs Pasting (1/2)\n",
    "***\n",
    "* Both bagging and pasting allow training instances to be sampled several times across multiple predictors\n",
    "* But only bagging allows training instances to be sampled several times for the same predictor.\n",
    "* Bootstrapping introduces a bit more diversity in the subsets that each predictor is trained on, so bagging ends up with a slightly higher bias than pasting\n",
    "* This also means that predictors end up being less correlated so the ensemble’s variance is reduced. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bagging vs Pasting (2/2)\n",
    "***\n",
    "* Overall, bagging often results in better models, which explains why it is generally preferred. \n",
    "* However, given spare time and CPU power it is worth using cross- validation to evaluate both bagging and pasting and select the one that works best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Out of Bag Evaluation (1/2)\n",
    "***\n",
    "Lucius, being a mathematician, was wondering something about bootstrapping:\n",
    "\n",
    "For bootstrapping with n samples,\n",
    "\n",
    "* The probability of each sample being selected is 1/n\n",
    "* Hence, the probability of not being selected is (1-1/n)\n",
    "* If m such samples are created, then the probability of a sample never being selected is p =  (1-1/n)<sup>m</sup>\n",
    "* When n and m approach large numbers, p ~ e-1 ~ 0.368"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Out of Bag Evaluation (2/2)\n",
    "***\n",
    "\"Aha!\" said, Lucius, \"This means that for each bootstrapping, around 3rd of the original sample will end up not being selected!\"\n",
    "\n",
    "Jay started following the idea: Since a predictor never sees the OOB instances during training, it can be evaluated on these instances, without the need for a separate validation set or cross-validation\n",
    "\n",
    "And since this unselected sample is not in the bag, this validation called **out-of-bag evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Similar to the above approach, we just add a new parameter `oob_score` and set it to True, whose default value is False. We also, check out its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "out_of_bag_clf = BaggingClassifier(DecisionTreeClassifier(random_state=9),\n",
    "                                  n_estimators=100,\n",
    "                                  max_samples=100,\n",
    "                                  bootstrap=True,\n",
    "                                  oob_score=True,\n",
    "                                  random_state=9)\n",
    "out_of_bag_clf.fit(X_train, y_train)\n",
    "y_pred = out_of_bag_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Special Mention: The Random Forest\n",
    "***\n",
    "* Random Forest is a bagging algorithm with decision tree as base classifier/regressor\n",
    "* The Random Forest algorithm introduces extra randomness when growing trees \n",
    "* Instead of searching for the very best feature when splitting a node, it searches for the best feature among a random subset of features. \n",
    "* This bootstrapping results in a greater tree diversity, which trades a higher bias for a lower variance, generally yielding an overall better model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## John catches up\n",
    "***\n",
    "It took John a bit of time to take in all the \"wisdom\" imparted on it, but once it all settled, John came up with an idea of his own. Let's hear it:\n",
    "\n",
    "So far we have used methods like **averaging** and **voting** to aggregate the predictions by all the base learners, thereby we have assigned same weights to the predictions made by the base learners.\n",
    "\n",
    "However, it might be possible that some base learners might be better at predicting than the others. So, a better aggregation scheme could be to assign some kind of weights to the predictions made by base learners!\n",
    "\n",
    "And since we have been learning machine learning to predict things anyway,... you see where we are going with this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "## Stacking\n",
    "***\n",
    "\n",
    "Stacking is an ensemble learning technique to combine multiple classification models via a meta-classifier\n",
    "It is based on a simple idea: instead of using trivial functions to aggregate the predictions of all predictors in an ensemble, we train a model to perform this aggregation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "## Stacking\n",
    "***\n",
    "\n",
    "![stacking](../images/image23.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How Stacking works?\n",
    "***\n",
    "* First, the training set is split in two subsets. \n",
    "* The first subset is used to train the predictors in the first layer\n",
    "* Next, the first layer predictors are used to make predictions on the second (held-out) set\n",
    "* This ensures that the predictions are “clean,” since the predictors never saw these instances during training.\n",
    "* The meta-classifier is trained on this new training set, so it learns to predict the target value given the first layer’s predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Technical-Stuff.png\" alt=\"Technical-Stuff\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "## Stacking with `mlxtend`\n",
    "***\n",
    "Sadly, stacking is not implemented in sklearn.\n",
    "\n",
    "Scikit-Learn does not support stacking directly, luckily but it is not very hard to create your own stacking ensemble. Alternatively a python library called mlextend supports stacking and has very similar api as sklearn!\n",
    "\n",
    "Check out the link for `Stacking` implementation in a python library called **[mlxtend](https://rasbt.github.io/mlxtend/user_guide/classifier/StackingClassifier/)**.\n",
    "\n",
    "You can install it using the `pip` command: `pip install mlxtend`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from mlxtend.classifier import StackingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "decision_tree_clf = DecisionTreeClassifier() \n",
    "models = [log_clf, decision_tree_clf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "stacking_clf = StackingClassifier(classifiers = models,\n",
    "                                 meta_classifier = decision_tree_clf)\n",
    "\n",
    "stacking_clf.fit(X_train, y_train)\n",
    "y_pred = stacking_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lucius & New York\n",
    "***\n",
    "- After spending a few days at John's in Brooklyn, Lucius was going to the Grand Central Railway Station to head back home \n",
    "\n",
    "- He was in a cab and as usual, it was taking a lot time traveling from Brooklyn to Grand Central\n",
    "\n",
    "- In the cab, he realized that his cab can take multiple routes to the SAME destination\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"../images/rf1.png\" alt=\"Drawing\" style=\"width: 400px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lucius & New York\n",
    "***\n",
    " - He pondered upon this a lot while he was on the train, returning home \n",
    " \n",
    " - He thought that what if he could use multiple Decision Trees to help in out in his model? \n",
    " \n",
    " - He doesn't HAVE to pick the best one (like how Google Maps picks the best route) \n",
    " \n",
    " - He could use the average of all predictions\n",
    " \n",
    " - Intuitively, this does seem like a good approach. More models will give a more robust prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lucius' Approach \n",
    "***\n",
    "- He thought to use multiple trees as using just 1 tree limits his predictions to constrained by the specified hyper-parameters \n",
    "\n",
    "- Thus, the Decision Tree by itself did lack a few important characteristics\n",
    "\n",
    "- Let's see what they were and understand what Lucius was trying to achieve \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revisiting Decision Trees \n",
    "***\n",
    "- We went through the shortcomings of decision trees\n",
    "    - They overfit\n",
    "    - They are structurally unstable since we need to find the optimal hyper-parameters in order to interpret the result\n",
    "    \n",
    "    \n",
    "- Therefore, by using a greater number of Trees a.k.a a \"Forest\", Lucius thought that this is 1 way of getting a better fitting model and prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-ALert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "## Random Forest \n",
    "***\n",
    "- In Random Forest, we learned that 'bagging' is used to create multiple Decision Trees with minimal correlation\n",
    "\n",
    "- In bagging, a random subset of the training data is selected to train each tree\n",
    "\n",
    "- Furthermore, the model randomly restricts the variables which may be used at the splits of each tree. Hence, the trees grown are dissimilar, but they still retain certain predictive power.\n",
    "\n",
    "- Let's build our intuition through an easy example and then return back to Lucius' Approach\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example - People's first preference of Entertainment\n",
    "***\n",
    " - A random sample of people were given 3 options when asked what their preferred mode of entertainment were. These options were :\n",
    "\n",
    "\n",
    " 1. Watching TV/Movies Online and at home (Netflix, etc) \n",
    " 2. Going to the Movie Theaters \n",
    " 3. Watching a live Play \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entertainment\n",
    "***\n",
    "- The information (variables) available in the dataset for each individual is the following:\n",
    "\n",
    "1. Age , 2. Gender,  3. Highest educational qualification, 4. Working in Industry, 5. Residence in Metro/Non-metro\n",
    "\n",
    "We need to come up with an algorithm to give an accurate prediction for an individual who has following traits:\n",
    "\n",
    "1. Age : 35 years , 2, Gender : Male , 3. Highest Educational Qualification : Diploma holder, 4. Industry : Manufacturing, 5. Residence : Metro\n",
    "\n",
    "We will only talk about random forest to make this prediction in our example\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-ALert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "## How the RF Algorithm works\n",
    "*** \n",
    " - Deviating from our example, say, we have a random 1000 observations of Mexicans with 10 variables\n",
    " \n",
    " - Random forest tries to build multiple CART models with different samples and different initial variables. \n",
    " \n",
    " - For instance, it will take a random sample of 100 observation and 5 randomly chosen initial variables to build a CART model \n",
    " \n",
    " - It will repeat the process (say) 10 times and then make a final prediction on each observation\n",
    " \n",
    " - **Final prediction is a function of each prediction. This final prediction can simply be the mean of each prediction**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to Entertainment\n",
    "*** \n",
    " - Say, the algorithm Random forest picks up 10,000 observations with only one variable (for simplicity) to build each CART model. In total, we are looking at 5 CART model being built with different variables.\n",
    "\n",
    "- Remember: we have 5 variables in this dataset as described in the previous slides\n",
    "\n",
    "- In a real life problem, you will have more number of population sample and different combinations of  input variables. Let's have a look at the salary bands again:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to Entertainment\n",
    "***\n",
    " Salary bands :\n",
    "\n",
    " Band 1) Watching TV/Movies Online and at home (Netflix, etc) \n",
    " \n",
    " Band 2) Going to the Movie Theaters \n",
    " \n",
    " Band 3) Watching a live Play \n",
    "\n",
    "Following are the outputs of the 5 different CART models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "\n",
    "## Entertainment, Entertainment, Entertainment ;)\n",
    "***\n",
    "#### Note: These are outputs from the dataset and have been arranged in Tabular format for easy comprehension\n",
    "\n",
    "**CART 1: VARIABLE AGE**\n",
    "***\n",
    "<center><img src=\"../images/rf3.png\" alt=\"Drawng\" style=\"width: 400px;\"/></center>\n",
    "\n",
    "**CART 2: Variable Gender**\n",
    "***\n",
    "<center><img src=\"../images/rf4.png\" alt=\"Drawing\" style=\"width: 400px;\"/></center>\n",
    "\n",
    "**CART 3: Variable Education**\n",
    "***\n",
    "<center><img src=\"../images/rf5.png\" alt=\"Drawing\" style=\"width: 400px;\"/></center>\n",
    "\n",
    "**CART 4: Variable Residence**\n",
    "***\n",
    "<center><img src=\"../images/rf6.png\" alt=\"Drawing\" style=\"width: 400px;\"/></center>\n",
    "\n",
    "**CART 5: Variable Industry**\n",
    "***\n",
    "<center><img src=\"../images/rf7.png\" alt=\"Drawing\" style=\"width: 400px;\"/></center>\n",
    "\n",
    "## Entertainment \n",
    "***\n",
    "Using these 5 CART models, we need to come up with set of probabilities that belong to each of the classes.\n",
    "\n",
    "- For simplicity, we will just take a mean of probabilities in this case study. \n",
    "\n",
    "- Other than simple mean, we also consider vote method to come up with the final prediction. To come up with the final prediction let’s locate the following profile in each CART model :\n",
    "\n",
    "    \n",
    "\n",
    " 1. Age : 35 years \n",
    " \n",
    " 2. Gender : Male \n",
    " \n",
    " 3. Highest Educational Qualification : Diploma holder\n",
    " \n",
    " 4. Industry : Manufacturing\n",
    " \n",
    " 5. Residence : Metro\n",
    "\n",
    "For each of these CART model, following is the distribution across entertainment bands :\n",
    "***\n",
    "<center><img src=\"../images/rf8.png\" alt=\"Drawing\" style=\"width: 400px;\"/></center>\n",
    "\n",
    " - The final probability is simply the average of the probability in the same salary bands in different CART models\n",
    " \n",
    " - As you can see from this analysis, that there is 70% chance of this individual falling in class 1 (Watching TV/Movies Online at home using Netflix, etc) and around 24% chance of the individual falling in class 2 (Going to the Movie Theater)\n",
    "\n",
    "## Wisdom of the Crowd \n",
    "***\n",
    " - A random forest is an example of an ensemble, which is a combination of predictions from different models.\n",
    " \n",
    " - In an ensemble, predictions could be combined either by majority-voting or by taking averages.\n",
    " \n",
    " - Provided in the next slide is an illustration of how an ensemble formed by majority-voting yields more accurate predictions than the individual models it is based on:\n",
    "\n",
    "#### Note: The illustration below is a GIF not an image. Please see the whole image\n",
    "\n",
    "<center><img src=\"../images/rf2.gif\" alt=\"Drawing\" style=\"width: 550px;\"/></center>\n",
    "\n",
    "## Lucius' Approach\n",
    "***\n",
    "- After Jay's discussion on Ensembling with Lucius and John, Lucius had already been pretty intrigued by Random Forest \n",
    "\n",
    "- He expanded his knowledge and learnt how to build a Random Forest Classifier in Python\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, make_scorer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data & seperating it into train and test\n",
    "# Splitting the data into training and target set\n",
    "\n",
    "dataframe = pd.read_csv('../data/loan_prediction.csv')\n",
    "X = dataframe.iloc[:,0:4]\n",
    "y = dataframe.iloc[:,4]\n",
    "np.random.seed(9)\n",
    "            \n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Curious to practice his new found skills, Lucius firsts plays around with the Loan Prediction dataset by first, creating 2 Random Forest models with different parameters\n",
    "\n",
    "**One with min_samples_leaf set to 2**\n",
    "\n",
    "**One with max_depth set to 5**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model for tuning the min samples of leaf\n",
    "clf_1 = RandomForestClassifier(min_samples_leaf=2, random_state=9)\n",
    "clf_1.fit(X_train,y_train)\n",
    "\n",
    "#model for tuning the max depth\n",
    "clf_2 = RandomForestClassifier(max_depth = 5, random_state=9)\n",
    "clf_2.fit(X_train,y_train)\n",
    "\n",
    "# predicting for min_samples 0f leaf as 2.\n",
    "y_clf_1 = clf_1.predict(X_test)\n",
    "\n",
    "# predicting for max depth 5.\n",
    "y_clf_2 = clf_2.predict(X_test)\n",
    "\n",
    "#calculate accuracy and roc auc score for min leaf samples\n",
    "\n",
    "accuracy_clf_1 = accuracy_score(y_test, y_clf_1)\n",
    "auc_roc1 = roc_auc_score(y_test, y_clf_1)\n",
    "print(\"Accuracy score of the model with min sample leaf 2 is: {}\".format(accuracy_clf_1))\n",
    "print(\"roc_auc_score of the model1 is: {}\".format(auc_roc1))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "#calculate accuracy and roc auc score for max depth 5\n",
    "\n",
    "accuracy_clf_2 = accuracy_score(y_test,y_clf_2)\n",
    "auc_roc2 = roc_auc_score(y_test, y_clf_2)\n",
    "print(\"Accuracy score of the model with max depth 5 is: {}\".format(accuracy_clf_2))\n",
    "print(\"roc_auc_score of the model2 is: {}\".format(auc_roc2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Variation in the performance of the model\n",
    "***\n",
    "- In the above code Lucius used 2 different parameters for checking the performance of the models\n",
    "\n",
    "- The variation in the **accuracy** and the **roc_auc score** of the models indicates the increase in the performance of the model by tweaking the parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-ALert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "## The main weaknesses of using a random forest are:\n",
    "***\n",
    "They're difficult to interpret \n",
    "- Because we've averaging the results of many trees, it can be hard to figure out why a random forest is making predictions the way it is.\n",
    "- They take longer to create - Making two trees takes twice as long as making one, making three takes three times as long, and so on. \n",
    "- Let's look into this further\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `n_jobs` saves the day \n",
    "***\n",
    "-  Fortunately, we can exploit multicore processors to parallelize tree construction\n",
    "\n",
    "- Scikit allows us to do this through the n_jobs parameter on RandomForestClassifier\n",
    "\n",
    "In-class Activity - See the performance by changing the **n_jobs** in parameters input to see the difference for parallel construction of trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-ALert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "## Deciding the number of trees to build \n",
    "***\n",
    "* When we build a RandomForestClassifier, we pass in an **n_estimators** parameter that indicates **how many trees to build**\n",
    "\n",
    "* While adding more trees usually improves accuracy, it also increases the overall time the model takes to train.\n",
    "\n",
    "* RandomForestClassifier has a similar interface to DecisionTreeClassifier, and we can use the fit() and predict() methods to train and make predictions.\n",
    "\n",
    "- More trees, in general, does lead to better predictions but *TOO MANY* trees don't necessarily improve the prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/Technical-Stuff.png\" alt=\"Technical-Stuff\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "## Criterion to split the nodes\n",
    "***\n",
    "- **Criterion** is the function to measure the quality of a split. \n",
    "- Supported criteria are **“gini”** for the Gini impurity and **“entropy”** for the information gain. \n",
    " \n",
    "- We have studied these in detail in Decision Trees. If you need to, please go back and review how Trees are split based on these scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-ALert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "## Some Caution - Hyperparameters\n",
    "***\n",
    "- Random forests can still be prone to overfitting, though, so it's important to tune parameters like **maximum depth** and **minimum samples per leaf**\n",
    "\n",
    "- We got an intuition behind n_jobs, no. of trees, and we already know about the Criterion used to split the trees\n",
    "\n",
    "- ALL of these are treated as Hyperparameters and it's important to get optimal values since we want to fit the *BEST* possible model to our data! \n",
    "\n",
    "\n",
    "- Thus, Lucius turned through tuning these hyperparameters through a process already known to us: **Grid Searching! **\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/Technical-Stuff.png\" alt=\"Technical-Stuff\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "## Hyper parameter tuning\n",
    "***\n",
    "\n",
    "* These are the hyperparameters that Lucas is going to focus on tuning and get optimal values\n",
    "***\n",
    "     parameter = {\n",
    "                'n_estimators': [ ], \n",
    "                  'max_features': [], \n",
    "                  'criterion': [],\n",
    "                  'max_depth': ,\n",
    "                  'min_samples_split': [ ],\n",
    "                  'min_samples_leaf': [ ] \n",
    "               } \n",
    "                 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tweaking parameters can increase the accuracy of the forest. \n",
    "* These parameters apply to the individual trees in the model, and change how they are constructed. \n",
    "\n",
    "\n",
    "- Lucius defined a Grid Search Function. Let's see how he went about this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined a function for implementing hyper parameter tuning\n",
    "def gridfunc(classifier, parameter, X_train, y_train):\n",
    "        \n",
    "    clf = classifier\n",
    "    np.random.seed(9)\n",
    "    parameters = parameter\n",
    "    acc_scorer = make_scorer(accuracy_score)\n",
    "    \n",
    "    # Run the grid search\n",
    "    grid_obj = GridSearchCV(clf, parameters, scoring=acc_scorer)\n",
    "    grid_obj = grid_obj.fit(X_train, y_train)\n",
    "    \n",
    "    return grid_obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot mean scores for a particular grid object\n",
    "\n",
    "def hp_cv_scores(grid_obj):\n",
    "    grid_obj.cv_results_\n",
    "    mean_test_scores = grid_obj.cv_results_['mean_test_score']\n",
    "    plt.figure(figsize=(10,6))\n",
    "    \n",
    "    param_values =[str(x) for x in list(grid_obj.param_grid.items())[0][1]]\n",
    "    x = np.arange(1, len(param_values)+1)\n",
    "    \n",
    "    plt.plot(x,mean_test_scores,c='g', label='Test set')\n",
    "    plt.xlabel(list(grid_obj.param_grid.items())[0][1])\n",
    "    plt.ylabel('mean scores')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(random_state=9)\n",
    "\n",
    "# plotting the graph to find the variation in model for given n_estimators -- NEEDS LEGEND\n",
    "\n",
    "grid = gridfunc(classifier, {'n_estimators': [2, 3, 4]} , X_train, y_train)\n",
    "hp_cv_scores(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the graph to find the variation in model for given max features\n",
    "\n",
    "grid = gridfunc(classifier,{'max_features': ['log2', 'sqrt']} , X_train, y_train)\n",
    "hp_cv_scores(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the graph to find the variation in model for given criterion\n",
    "\n",
    "grid = gridfunc(classifier,{'criterion': ['entropy', 'gini']} , X_train, y_train)\n",
    "hp_cv_scores(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the graph to find the variation in model for given max depth\n",
    "\n",
    "grid = gridfunc(classifier,{'max_depth': [2,4,6,8,16]} , X_train, y_train)\n",
    "hp_cv_scores(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the graph to find the variation in model for given min samples split\n",
    "\n",
    "grid = gridfunc(classifier,{'min_samples_split': [2, 3, 5, 10, 15,20,25,30,35]} , X_train, y_train)\n",
    "hp_cv_scores(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the graph to find the variation in model for given min samples leaf\n",
    "\n",
    "grid = gridfunc(classifier,{'min_samples_leaf': [1,3,5,8,50,100]} , X_train, y_train)\n",
    "hp_cv_scores(grid)\n",
    "\n",
    "classifier = RandomForestClassifier(random_state=9)\n",
    "parameter = {'n_estimators': [10,50,100], \n",
    "              'max_features': ['log2', 'sqrt'], \n",
    "              'criterion': ['entropy', 'gini'],\n",
    "              'max_depth': [2,4,6], \n",
    "              'min_samples_split': [2, 3, 5],\n",
    "              'min_samples_leaf': [1,3,5]\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#task find the results for the given params\n",
    "grid = gridfunc(classifier, parameter, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the clf to the best combination of parameters\n",
    "clf = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the best algorithm to the data. \n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction of Career Lengths of NBA Players\n",
    "\n",
    "The dataset details the performance metrics of NBA players and whether their careers span more than 5 years or not. \n",
    "\n",
    "### Aim\n",
    "The task is to predict whether the careers of these players will span for more than 5 years or less.\n",
    "\n",
    "\n",
    "### Feature descriptions:\n",
    "The dataset contains columns giving values for points attained by NBA players in their careers. The description of the features is as given below\n",
    "\n",
    "\n",
    "|Features|Description|\n",
    "|----|----|\n",
    "|Name| Name of the Player|\n",
    "|Games Played|Number of Games Played by the player|\n",
    "|Minutes| Minutes Played\n",
    "|Points| Points Per Game|\n",
    "|Field Goals Made| Successful field goals made|\n",
    "|Field Goals Attempted | Field goals attempted |\n",
    "|Field Goals Percentage | Percentage of successful field goals out of the field goals attempted|\n",
    "|3 Pointers Made| Successful 3 pointer attempts|\n",
    "|3 Pointers Attempted | Total 3 pointers attempted|\n",
    "|3 Pointers Percentage | Percentage of successful 3 pointers out of those attempted|\n",
    "|Free Throws Made| Successful number of free throws | \n",
    "|Free Throws Attempted| Total attempted number of free throws|\n",
    "|Free Throw Percentage| Successful number of free throws out of those attempted|\n",
    "|OREB|Offesnisve Rebounds|\n",
    "|DREB|Defensive Rebounds |\n",
    "|REB|Rebounds|\n",
    "|AST|Assists|\n",
    "|STL|Steals|\n",
    "|BLK|Number of blocks|\n",
    "|TOV |Number of Turnovers|\n",
    "|Play for more than 5 years or less| 1 if career length is greater than 5 years else 0 if career length is less than 5 years|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score,accuracy_score,classification_report,roc_curve,confusion_matrix\n",
    "pd.set_option('display.max_columns',None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data. List the categorical and continuous features in the dataset. Find the number of null values in every feature and impute them appropriately. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "df = pd.read_csv('../data/NBA_players.csv')\n",
    "\n",
    "df.head()\n",
    "\n",
    "# Code starts here\n",
    "\n",
    "# Categorical and Continuous features\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Check for null values\n",
    "\n",
    "\n",
    "\n",
    "# The null values in the feature `3 Pointers Percentage` are present since\n",
    "# they indicate that these players haven't attempted any three pointer throws. So we shall replace them with 0\n",
    "\n",
    "# Fill the null-values with 0\n",
    "\n",
    "\n",
    "\n",
    "# Drop the feature Name\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seperate the independent features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code starts here\n",
    "\n",
    "# target\n",
    "\n",
    "\n",
    "# Features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for class distribution in the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code starts here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Class distribution doesn't seem to be imbalanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the distribution of each feature, so that we have a better understanding about the distribution of each feature. Draw the number of values for each category feature and the histogram of the values for each continuous feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code starts here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can infer from the above plots that there exists some amount of skewness in our predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check the amount of skewness in our continuous predictors and depending on the type of skewness, let's apply the necessary transformations for removing the skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as scs\n",
    "\n",
    "\n",
    "# Code starts here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# If skewness is greater than 1 the feature is highly positively skewed\n",
    "\n",
    "\n",
    "# If the skewness is less than -1 the feature is highly negatively skewed.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Let's remove the skewness in the positively skewed variables by using a log transform\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train and test data into X_train ,y_train,X_test and y_test data and apply a vanilla logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code starts here\n",
    "\n",
    "# Split into training and testing\n",
    "\n",
    "\n",
    "\n",
    "# Apply a Logistic Regression model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets now train our data with Decision tree model. We will then predict our test data and compute the accuracy as well as the `roc_auc_score`.\n",
    "\n",
    "- Train a decision tree (DecisionTreeClassifier) with a maximum depth of 3, and evaluate the accuracy metric i.e `roc_auc_score` on the test data. Use parameter `random_state = 42` for results reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code starts here\n",
    "\n",
    "\n",
    "\n",
    "# Fit the model on train data\n",
    "\n",
    "\n",
    "# accuracy\n",
    "\n",
    "\n",
    "# Predicted values for test data\n",
    "\n",
    "\n",
    "# ROC score\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that Decision Tree is giving us an accuracy of 64% while the `AUC_ROC_score` is around 59%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next let's implement a Stacking Classifier consisting of one Decision Tree and Logistic regression and a  decision tree model as a meta classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.classifier import StackingClassifier\n",
    "\n",
    "classifier1 = LogisticRegression(C=0.1)\n",
    "classifier2= DecisionTreeClassifier(random_state=2,criterion='entropy',max_depth=4)\n",
    "\n",
    "# classifier\n",
    "\n",
    "classifier_list=[classifier1,classifier2]\n",
    "\n",
    "# meta classifier\n",
    "\n",
    "m_classifier=DecisionTreeClassifier(random_state=42,criterion='entropy',max_depth=6)\n",
    "\n",
    "\n",
    "# Code starts here\n",
    "\n",
    "# Stacking Classifier\n",
    "\n",
    "\n",
    "# Fit & Predict\n",
    "\n",
    "\n",
    "\n",
    "# Check the score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Now let's apply a Random Forest model\n",
    "- Let the parameters for the model be `criterion='gini'`, `max_depth= 6`,`max_features= 'auto'`,`n_estimators= 11`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Code starts here\n",
    "\n",
    "# Initialize RandomForrest model to variable rfc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "\n",
    "\n",
    "# Store the predicted values of test data\n",
    "\n",
    "\n",
    "# accuracy\n",
    "\n",
    "\n",
    "# roc score\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try improving upon the accuracy of the Random Forest using hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(random_state=2)\n",
    "param_grid = { \n",
    "'n_estimators': [11,12,13],\n",
    "'max_features': ['auto', 'sqrt', 'log2'],\n",
    "'max_depth' : [4,5,6,7,8],\n",
    "'criterion' :['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Code starts here\n",
    "\n",
    "# grid search cv\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize RandomForrest model to variable rfc\n",
    "rfc = RandomForestClassifier(criterion='entropy', max_depth= 6,\n",
    "                             max_features= 'auto', n_estimators= 12)\n",
    "\n",
    "# Code starts here\n",
    "\n",
    "# Fit the model\n",
    "\n",
    "\n",
    "# Store the predicted values of test data\n",
    "\n",
    "\n",
    "# accuracy\n",
    "\n",
    "\n",
    "# roc score\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's apply a Voting Classifier that consists of three models - Logistic Regression, Decision Tree and a Random Forest with a soft voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Various models\n",
    "clf_1 = LogisticRegression()\n",
    "clf_2 = DecisionTreeClassifier(random_state=4)\n",
    "clf_3 = RandomForestClassifier(random_state=4)\n",
    "\n",
    "model_list = [('lr',clf_1),('DT',clf_2),('RF',clf_3)]\n",
    "\n",
    "# Code starts here\n",
    "\n",
    "# Initialize voting classifier\n",
    "\n",
    "\n",
    "# Fit the model on training data\n",
    "\n",
    "\n",
    "# predict on test\n",
    "\n",
    "\n",
    "# accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check how a bagging classifier using a Decision Tree performs on our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Bagging Classifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Initialize Bagging Classifier\n",
    "bagging_clf = BaggingClassifier(DecisionTreeClassifier(), random_state=0,n_estimators=100,max_samples=100)\n",
    "\n",
    "# Code starts here\n",
    "\n",
    "# Fit the model on training data\n",
    "\n",
    "\n",
    "# Predicted values of X_test\n",
    "\n",
    "\n",
    "# accuracy \n",
    "\n",
    "\n",
    "# roc_score\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Let's now visualize the AUC_ROC scores for all the models in one plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a result table as a DataFrame\n",
    "result_table = pd.DataFrame(columns=['classifiers', 'fpr','tpr','auc'])\n",
    "\n",
    "classifiers = [dt,rfc,bagging_clf,voting_clf_hard,sclf]\n",
    "\n",
    "# Code starts here\n",
    "\n",
    "\n",
    "# Create result_table\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# Set name of the classifiers as index labels\n",
    "\n",
    "\n",
    "\n",
    "# Plot\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "## Overview\n",
    "***\n",
    "* Random Forest is a bagging algorithm with decision tree as base classifier/regressor\n",
    "* The Random Forest algorithm introduces extra randomness when growing trees \n",
    "* Instead of searching for the very best feature when splitting a node it searches for the best feature among a random subset of features. \n",
    "* This results in a greater tree diversity, which trades a higher bias for a lower variance, generally yielding an overall better model. \n",
    "\n",
    "- Random Forests can be easily deployed in a distributed fashion due to the fact that they can run in parallel and for imbalanced data Random Forest stratifys the sampling.\n",
    "- And it is true that they are more robust to overfitting and require less tuning to avoid it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/Recap.png\" alt=\"Recap\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "# In-session Recap Time\n",
    "***\n",
    "* Decision Trees and their weakness\n",
    "* Enseble of decision trees\n",
    "* Hyperparameter tuning of random forest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank You\n",
    "***\n",
    "### Next Session: Boosting Ensemble and XGBoost\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
