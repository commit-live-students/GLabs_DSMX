{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h1> Decision Trees </h1></center>\n",
    "\n",
    "***\n",
    "\n",
    "![decision_trees](../images/DT.1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Program so far\n",
    "***\n",
    "- Python Basics and Intermediate\n",
    "- Descreptive and Inferential Statistics\n",
    "- Feature Engineering\n",
    "- Linear Regression\n",
    "- Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A Tale of Two Friends\n",
    "***\n",
    "- To John's surprise Lucius was moving to New York City as well! \n",
    "- John & Lucius knew each other since Kindergarten and were probably the best of friends\n",
    "- He was going to be John's roommate too so this was quite exciting for both of them \n",
    "- But, (there's always a but) Lucius had a financial problem..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A Tale of Two Friends\n",
    "***\n",
    "- It's established that New York City is one of the most expensive cities in the world\n",
    "- Lucius was moving to study Applied Mathematics at New York University, where college tuition can really put a dent in ones bank account \n",
    "    - It was crystal clear to John and Lucius that he would move to NYC only if he'd get a loan for his college tuition \n",
    "\n",
    "\n",
    "- Looking into this, John sent Lucius a dataset which consisted of prevoious observations on Loan Predictions at PNC Bank (Lucius' bank) and whether they we were approved or not \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lucius imagining having a good time \n",
    "***\n",
    "<center><img src=\"../images/twofriends.jpg\" alt=\"Drawing\" style=\"width: 500px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Lucius' Curiosity\n",
    "*** \n",
    " - Lucius, unlike John, did possess extensive knowledge in Machine Learning algorithms and decided to make the most of this dataset and figure where he stands\n",
    " - After all, if, he thought, asked for a smaller loan amount he could have time to figure out how to get the rest of the sum and whether he should work the summer before school or not. This is quite common amongst aspiring college students in the USA, for undergrad and grad school\n",
    " - Lucius wanted to check out the data: (check it out yourself in the cell below:) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## By now you should be familar with importing data sets and checking the first few rows using the head() function too\n",
    "## Enter your code to print the first few rows of loan prediction data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What type of problem is this? \n",
    "***\n",
    "- As you can tell, this is a Classification problem where the Target Variable is Categorical \n",
    "    - 1: Yes (Loan Approved)\n",
    "    - O: No  (Loan Not Approved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Lucius' Direct Approach \n",
    "***\n",
    "- Through previous experiences, Lucius was confident that using Decision Trees to tackle Classification problems was quick and easy \n",
    "\n",
    "- But, we haven't really developed the intuition on what the Decision Tree does and how it treats the data. Also, we need to know the terminology used in Decision Trees too as these will be extensively used throughout this notebook and in other lectures as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees\n",
    "***\n",
    "\n",
    "* Decision trees are one of the most intuitive family of algorithms.\n",
    "* Extremely easy to understand\n",
    "* Checkout the decision tree below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Decision Trees\n",
    "***\n",
    "\n",
    "<center><img src=\"../images/tree2.png\" alt=\"Drawing\" style=\"width: 600px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision trees are awesome\n",
    "***\n",
    "- Apart from being intuitive, we use decision trees because they can handle non-linearity in data\n",
    "\n",
    "- Since our observations tend to get jumbled up when we plot them, it's impossible to separate the data points linearly (this is what a simple Logistic Regression model would do) \n",
    "\n",
    "- In the image below, is the data Linearly Separable? \n",
    "\n",
    "<center><img src=\"../images/linearlyins.png\" alt=\"Drawing\" style=\"width: 400px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Answer: NO\n",
    "\n",
    "- This is where Decision Trees are very useful. \n",
    "- Through easy computational methods, Decision Trees are capable of complex methods of separation\n",
    "\n",
    "- But this also leads to ***overfitting***! We will see how to tackle that by tuning some parameters later on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees\n",
    "***\n",
    "Types of decision tree are based on the type of target variable we have. It can be of two types:\n",
    "\n",
    "- **Categorical Variable Decision Tree**: Decision Tree which has categorical target variable then it called as categorical variable decision tree. Example:- In above scenario of student problem, where the target variable was “Student will play cricket or not” i.e. YES or NO.  (***CLASSIFICATION***)\n",
    "\n",
    "- **Continuous Variable Decision Tree**: Decision Tree has continuous target variable then it is called as Continuous Variable Decision Tree.  (***REGRESSION***)\n",
    "\n",
    "\n",
    "Before diving deep into the mathematics, let's get a better understanding of the Terminology used in Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees\n",
    "***\n",
    "Let's look at the basic terminology used with Decision trees:\n",
    "\n",
    "- **Root Node**: It represents entire population or sample and this further gets divided into two or more homogeneous sets.\n",
    "\n",
    "- **Splitting**: It is a process of dividing a node into two or more sub-nodes.\n",
    "\n",
    "- **Decision Node**: When a sub-node splits into further sub-nodes, then it is called decision node.\n",
    "\n",
    "- **Leaf/ Terminal Node**: Nodes do not split is called Leaf or Terminal node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"../images/DT98.png\" alt=\"Drawing\" style=\"width: 600px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Decision Trees\n",
    "***\n",
    "- **Pruning**: When we remove sub-nodes of a decision node, this process is called pruning. You can say opposite process of splitting\n",
    "- **Branch / Sub-Tree**: A sub section of entire tree is called branch or sub-tree.\n",
    "- **Parent and Child Node**: A node, which is divided into sub-nodes is called parent node of sub-nodes where as sub-nodes are the child of parent node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "## SPLITS IN DECISION TREES\n",
    "***\n",
    "* It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. \n",
    "\n",
    "* A binary split is made on a feature.\n",
    "\n",
    "* The final result is a tree with decision nodes and leaf nodes.\n",
    "\n",
    "* The topmost decision node in a tree which corresponds to the best predictor called root node.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Building our Intuition\n",
    "***\n",
    "- As usual, let's build our intuition on a problem based on a sort-of \"toy dataset\"\n",
    "\n",
    "- What is this dataset about?\n",
    "    - Let's say that the movie Dunkirk is running in the theaters\n",
    "    - Demographically, this movie has got a lot of mixed reviews\n",
    "\n",
    "\n",
    "- Let's have a look at the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "films = pd.read_csv('../data/films.csv')\n",
    "films.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "## Decision Trees: How does it work?\n",
    "***\n",
    "We'll start with a very simple example: \n",
    "- We have a sample of 50 people with three variables Gender (M/F), employment status( Student/ Working) and Age (years)\n",
    "- Some of these 50 are planning to watch the movie.\n",
    "- Now, we want to create a model to predict who will watch the movie? In this problem, we need to segregate the sample into who will watch the movie based on highly significant input variable among all three"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "## Decision Trees: How does it work?\n",
    "***\n",
    "- A Decision Tree will segregate the movie watchers based on all values of three variable and identify the variable, which creates the sets so that all the members in the same groups are homogeneous to each other and heterogeneous to the other group.\n",
    "\n",
    "- Let's start with the target variable. As per the dataset, there are 26 people who want to watch the movie and 24 who don't."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](../images/DT_master.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "## Decision Trees: How does it work?\n",
    "***\n",
    "- Now, let's see how the three variables affect a person's movie watching decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](../images/DT_MF.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](../images/DT_age.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](../images/DT_emp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "## Decision Trees: How does it work?\n",
    "***\n",
    "- As you can see from the images that `gender` splits the sample into most homogeneous groups\n",
    "- We can keep splitting our decision trees in the similar fashion, but as it turns out, mathematicians are smart! They have figured out a better way to split the decision trees. Let's see how."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "## How does a tree decide where to split?\n",
    "***\n",
    "- Decision trees use multiple algorithms to decide to split a node in two or more sub-nodes.\n",
    "\n",
    "- The creation of sub-nodes increases the homogeneity of resultant sub-nodes\n",
    "\n",
    "- Decision tree **splits the nodes on all available variables** and then selects the split which results in most homogeneous sub-nodes\n",
    "\n",
    "** splits are done based on **\n",
    "1. Gini index\n",
    "2. Entropy\n",
    "3. Chi Squared\n",
    "***\n",
    "\n",
    "* We will understand the calculations behind these 3 concepts using our Film Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/Technical-Stuff.png\" alt=\"Technical Stuff\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "## Gini Index\n",
    "***\n",
    "Gini index says, if we select two items from a population at random then they must be of same class and probability for this is 1 if population is pure\n",
    "\n",
    "* It works with categorical target variable “Success” or “Failure”\n",
    "* A Gini score gives an idea of how good a split is by how mixed the classes are in the two groups created by the split.\n",
    "* It measures how often a randomly chosen element would be incorrectly identified.\n",
    "* A perfect separation results in a Gini score of 0, whereas the worst case split that results in 50/50 classes in each group results in a Gini score of 1.0 (for a 2 class problem).\n",
    "* Higher the value of Gini higher the homogeneity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to Calculate Gini Index?\n",
    "***\n",
    "1. Calculate Gini for sub-nodes, using formula sum of square of probability for success and failure \n",
    "$$(p^2+ (1-p)^2)$$.\n",
    "2. Calculate Gini for split using weighted Gini score of each node of that split\n",
    "\n",
    "Quickly, let's use the \"Students\" example to build our intuition before we build..well, a better intuition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Before starting to split our Film Dataset based on a variable, let's see how many watched Dunkirk from our sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Viewers who watched the movie:{}\".format(len(films[films['watching'] == 'yes'])))\n",
    "print(\"Viewers who did not watched the movie:{}\".format(len(films[films['watching'] == 'no'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## SPLIT BASED ON GENDER :\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "crosstab1 = pd.crosstab(index=films[\"watching\"], columns=films[\"gender\"])\n",
    "crosstab1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Probabilities Calculation of male and female:**\n",
    "\n",
    "***\n",
    " $$ Females\\hspace{0.3cm}watched\\hspace{0.3cm}yes = \\frac{females\\hspace{0.3cm}who\\hspace{0.3cm}watched\\hspace{0.3cm}movies}{total\\hspace{0.3cm}females}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "male_watched_yes = (12/float(28))\n",
    "female_watched_yes = (14/float(22))\n",
    "\n",
    "print(\"Probability of males that watched Dunkirk:{:.3f}\".format(male_watched_yes))\n",
    "print(\"Probability of females that watched Dunkirk:{:.3f}\".format(female_watched_yes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Gini Index Calculation for Males & Females**\n",
    "***\n",
    "$$gini(females)=(females\\hspace{0.3cm}watched\\hspace{0.3cm}yes)^2+(1-females\\hspace{0.3cm}watched\\hspace{0.3cm}yes)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "subnode_male = (male_watched_yes)**2 + (1-male_watched_yes)**2\n",
    "subnode_female = (female_watched_yes)**2 + (1-female_watched_yes)**2\n",
    "\n",
    "print(\"Gini(female):{:.3f}\".format(subnode_female))\n",
    "print(\"Gini(male):{:.3f}\".format(subnode_male))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Weighted Gini Index Calculation for Gender Split:**\n",
    "***\n",
    "$weighted\\hspace{0.3cm}gini(gender)=\\frac{males}{total}x(gini(males))+\\frac{females}{total}x(gini(females))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "calculated_wt_gender = (28/float(50))*subnode_male + (22/float(50))*subnode_female\n",
    "print(\"Weighted Gini for Gender:{:.4f}\".format(calculated_wt_gender))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## SPLIT BASED ON EMPLOYMENT\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "crosstab2 = pd.crosstab(index=films[\"watching\"], columns=films[\"employment_status\"])\n",
    "crosstab2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Probabilities Calculation :**\n",
    "$$(working\\hspace{0.3cm}watched\\hspace{0.3cm}yes)=\\frac{(working\\hspace{0.3cm}professionals\\hspace{0.3cm}who\\hspace{0.3cm}watched\\hspace{0.3cm}movie)}{(total\\hspace{0.3cm}working)}$$\n",
    "* Probability of students and employees who watched the movie vs who didnot watch the movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "student_watched_yes = (4/float(9))\n",
    "working_watched_yes = (22/float(41))\n",
    "print(\"Probability of students that watched:{:.3f}\".format(student_watched_yes))\n",
    "print(\"Probability of working people that watched:{:.3f}\".format(working_watched_yes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Gini Index Calculation :**\n",
    "![](../images/img2.png)\n",
    "$$gini(working)=(working\\hspace{0.3cm}watched\\hspace{0.3cm}yes)^2+(1-working\\hspace{0.3cm}watched\\hspace{0.3cm}yes)^2$$\n",
    "* calculating Gini index for students who watched\n",
    "* calculating Gini index for employees who watched "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "subnode_student = (student_watched_yes)**2 + (1 - student_watched_yes)**2\n",
    "subnode_working = (working_watched_yes)**2 + (1 - working_watched_yes)**2\n",
    "\n",
    "print(\"Gini(student):{:.3f}\".format(subnode_student))\n",
    "print(\"Gini(working):{:.3f}\".format(subnode_working))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Weighted Gini Index for Employment Split :** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "calculated_wt_emp = (41/float(50))*subnode_working + (9/float(50))*subnode_student\n",
    "print(\"Weighted Gini(employment):{:.4f}\".format(calculated_wt_emp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Since weighted gini(gender) > weighted gini(employment), the node split will take on Gender\n",
    "***\n",
    "- And now, you have developed a very strong basis on how splits take place based on the Gini Index/Score of Variables \n",
    "- Can you imagine how time consuming it would be if we had around 50+ variables? Not practical \n",
    "- Luckily, Sci-Kit Learn makes this easy for us \n",
    "\n",
    "- This also helps us judging the importance of variables. There's definitely a function to display the variables to and this could aid in Feature Selection as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## In-class Activity\n",
    "***\n",
    "Calculate the weighted gini index if we split the node by age? Would it be more or less than our previous splits?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "## Chi-Square\n",
    "***\n",
    "It is an algorithm to find out the statistical significance between the differences between sub-nodes and parent node. We measure it by sum of squares of standardized differences between observed and expected frequencies of target variable.\n",
    "\n",
    "1. It works with categorical target variable “Success” or “Failure”\n",
    "2. It can perform two or more splits\n",
    "3. Higher the value of Chi-Square higher the statistical significance of differences between sub-node and Parent node.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "## Chi-Square\n",
    "***\n",
    "4. Chi-Square of each node is calculated using formula,\n",
    "5. Chi-square = $((Actual – Expected)^2 / Expected)^{1/2}$\n",
    "6. It generates tree called CHAID (Chi-square Automatic Interaction Detector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "## Chi-Square\n",
    "***\n",
    "### Steps to Calculate Chi-Square: \n",
    "\n",
    "1. Calculate Chi-square for individual node by calculating the deviation for Success and Failure both\n",
    "2. Calculated Chi-square of Split using Sum of all Chi-square of success and Failure of each node of the split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Result: \n",
    "***\n",
    "Above, you can see that Chi-square scores also identify the Gender split to be more significant compared to Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Coming back to Dunkirk \n",
    "***\n",
    " - Let's now perform similar calculation in Python on our Dunkirk example.\n",
    " - By now you should get a good idea how this notebook is laid out. \n",
    "     - We're first building our intuition on an easy example (Students) \n",
    "     - Then we're directly calculating, in Python, the different scores for our Dunkirk Dataset\n",
    "     - We're going to go back to Lucius and John's life and see how they used Sci-kit learn to see if Lucius would get the loan or not! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Dunkirk - Split on Gender\n",
    "***\n",
    "**Gender Node**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "crosstab1 = pd.crosstab(index=films[\"gender\"], columns=films[\"watching\"])\n",
    "crosstab1[\"Total\"] = crosstab1.no + crosstab1.yes\n",
    "crosstab1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "**Calculate the expected women who watch movie**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "# calculate the expected women who watch movie\n",
    "\n",
    "crosstab1[\"Expected watch\"] = crosstab1.Total/2\n",
    "crosstab1[\"Expected not watch\"] = crosstab1.Total/2\n",
    "crosstab1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Calculating deviation\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "crosstab1[\"E - O (Watch)\"] = crosstab1[\"Expected watch\"] - crosstab1.yes\n",
    "crosstab1[\"E - O (Not Watch)\"] = crosstab1[\"Expected not watch\"] - crosstab1.no\n",
    "crosstab1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "** Formula to calculate chi-square : **\n",
    "***\n",
    "$$\\tilde{\\chi}^2=(\\frac{(actual-expected)^2}{expected})^{1/2}$$\n",
    "\n",
    "* calculating the chi square value for women"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "crosstab1[\"chi2_watch\"] = np.sqrt(crosstab1[\"E - O (Watch)\"]**2/crosstab1[\"Expected watch\"])\n",
    "crosstab1[\"chi2_not_watch\"] = np.sqrt(crosstab1[\"E - O (Not Watch)\"]**2/crosstab1[\"Expected not watch\"])\n",
    "crosstab1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "chi2_gender = (crosstab1[\"chi2_watch\"] + crosstab1[\"chi2_not_watch\"]).sum()\n",
    "chi2_gender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Split on Employment Status**\n",
    "***\n",
    "* We will perform Similar Calculations for splits on the *Employment Status* node\n",
    "* Get the total chi-squared value & compare it to our previous result to see which split is more effective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "crosstab2 = pd.crosstab(index=films[\"employment_status\"], columns=films[\"watching\"])\n",
    "crosstab2[\"Total\"] = crosstab2.no + crosstab2.yes\n",
    "\n",
    "crosstab2[\"Expected watch\"] = crosstab2.Total/2\n",
    "crosstab2[\"Expected not watch\"] = crosstab2.Total/2\n",
    "\n",
    "crosstab2[\"E - O (Watch)\"] = crosstab2[\"Expected watch\"] - crosstab2.yes\n",
    "crosstab2[\"E - O (Not Watch)\"] = crosstab2[\"Expected not watch\"] - crosstab2.no\n",
    "\n",
    "crosstab2[\"chi2_watch\"] = np.sqrt(crosstab2[\"E - O (Watch)\"]**2/crosstab2[\"Expected watch\"])\n",
    "crosstab2[\"chi2_not_watch\"] = np.sqrt(crosstab2[\"E - O (Not Watch)\"]**2/crosstab2[\"Expected not watch\"])\n",
    "\n",
    "crosstab2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "chi2_emp = (crosstab2[\"chi2_watch\"] + crosstab2[\"chi2_not_watch\"]).sum()\n",
    "chi2_emp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "** Chi-squared values for GENDER is more than in employment status**\n",
    "***\n",
    "\n",
    "$\\tilde{\\chi}^2$ test also agrees with the Gini Index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "## Entropy / Information Gain\n",
    "***\n",
    "From the image below, which of the three options can be described ***easily***?\n",
    "***\n",
    "<center><img src=\"../images/dt7.png\" alt=\"Drawing\" style=\"width: 650px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "## Entropy / Information Gain\n",
    "***\n",
    "- C because it requires less information as all the values are similar\n",
    "- On the other hand, B requires more information to describe it \n",
    "- A requires the maximum amount of information\n",
    "\n",
    "In other words, we can say that C is a Pure node, B is less Impure and A is more impure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "## Entropy / Information Gain\n",
    "***\n",
    "- Now, we can conclude that less impure node requires less information to describe it\n",
    "\n",
    "- Concretely, a very impure node requires more information\n",
    "\n",
    "- **Information theory is a measure to define this degree of disorganization in a system known as Entropy.**\n",
    "\n",
    "- If the sample is completely homogeneous, then the entropy is zero and if the sample is an equally divided (50% – 50%), it has entropy of one. Let's understand how this is calculated and see what Information Gain really means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "## Entropy / Information Gain\n",
    "***\n",
    "Entropy can be calculated using formula:-\n",
    "<center><img src=\"../images/dt8.png\" alt=\"Drawing\" style=\"width: 250px;\"/></center>\n",
    "\n",
    "Here p and q are the probabilities of success and failure respectively in that node\n",
    "- Entropy is also used with categorical target variables. \n",
    "- It chooses the split which has lowest entropy compared to parent node and other splits\n",
    "- The lesser the entropy, the better it is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "## Entropy / Information Gain\n",
    "***\n",
    "Information Gain = Entropy of Parent node - [Weighted Avg]Entropy of Sub-nodes\n",
    "\n",
    "Steps to calculate entropy for a split:\n",
    "\n",
    "1. Calculate entropy of parent node\n",
    "2. Calculate entropy of each individual node of split and calculate weighted average of all sub-nodes available in split.\n",
    "\n",
    "** Now, let's build our intuition on the Students example, really quickly** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "## Entropy / Information Gain\n",
    "***\n",
    "Above, you can see that entropy for Split on Gender is the lowest among all, so the tree will split on Gender\n",
    "\n",
    "Can you calculate the Information Gain for each split? It's pretty easy \n",
    "   - Remember: We are trying to **Maximize** Information Gain \n",
    "   - Which is **WHY**, mathematically, we choose the *minimum variable of Entropy* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Coming back to Dunkirk\n",
    "***\n",
    "Let's try and calculate the Entropy of the Parent node for the Dunkirk Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As usual, we're first going to try and split the node by Gender and then based on their Employment Status \n",
    "\n",
    "Let's calculate the entropy for the parent node: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Calculating the parent entropy\n",
    "p = 26/float(50)\n",
    "q = 24/float(50)\n",
    "parent_entropy = -p*np.log2(p) - q*np.log2(q)\n",
    "parent_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Such a high entropy value suggest that this is almost an **impure** node.\n",
    "\n",
    "- Now, since we're splitting on the basis of Gender, let's calculate the Entropy for the Female and Male nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "crosstab1.iloc[:,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Female node entropy\n",
    "p = 14/float(22)\n",
    "q = 8/float(22)\n",
    "female_entropy = -p*np.log2(p) - q*np.log2(q)\n",
    "female_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Male node entropy\n",
    "p = 12/float(28)\n",
    "q = 16/float(28)\n",
    "male_entropy = -p*np.log2(p) - q*np.log2(q)\n",
    "male_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Weighted entropy for gender\n",
    "weighted_gender = (28/float(50))*male_entropy + (22/float(50))*female_entropy\n",
    "weighted_gender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Can you calculate the Information Gain now? \n",
    " - You have the Entropy of the Parent Node \n",
    " - You have Weighted Average Entropy of the Female and Male nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## Calculate here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now, let's do similar calculations if we were splitting our dataset on the basis of *Employment Status*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "crosstab2.iloc[:,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "#entropy for students\n",
    "p = 4/float(9)\n",
    "q = 5/float(9)\n",
    "working_entropy = -p*np.log2(p) - q*np.log2(q)\n",
    "working_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "# entropy for working people\n",
    "p = 22/float(41)\n",
    "q = 19/float(41)\n",
    "\n",
    "student_entropy = -p*np.log2(p) - q*np.log2(q)\n",
    "student_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "weighted_emp = (41/float(50))*working_entropy + (9/float(50))*student_entropy\n",
    "weighted_emp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Since gender has least entropy, we will split the decision tree at gender. This is again in agreement with previous methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reduction in Variance\n",
    "***\n",
    "- Reduction in variance is an algorithm used for continuous target variables.\n",
    "- This algorithm uses standard formula of variance to choose the split. \n",
    "\n",
    "- The split with lower variance is selected as the criteria to split the problem :  \n",
    "***\n",
    "<center><img src=\"../images/dt10.png\" alt=\"Drawing\" style=\"width: 150px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Above X-bar is mean of the values, X is actual/observed value and n is number of values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Steps to calculate Variance:\n",
    "***\n",
    "1) Calculate variance for each node\n",
    "\n",
    "2) Calculate variance for each split as weighted average of each node variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Above, you can see that the Gender split has lower variance compared to the parent node, so the split would take place on Gender variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Coming back to Dunkirk, again\n",
    "***\n",
    "\n",
    "* Let's assign 1 if someone is watching the film and 0 if he/she isn't watching the film \n",
    "* Based on this, let's perform the Variance Calculations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "mean_root = (26*1 + 24*0)/float(50)\n",
    "mean_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "var_root = (26*(1-0.52)**2 + 24*(0-0.52)**2)/50\n",
    "var_root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Similarly, calculating variance for gender split we get: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "crosstab1.iloc[:,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "# Calculate mean_women\n",
    "\n",
    "mean_women = (14*1 + 8*0)/float(22)\n",
    "mean_women"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "# variance for women\n",
    "var_women = (14*(1-0.636)**2 + 8*(0-0.636)**2)/22\n",
    "var_women"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "# Calculate mean_men\n",
    "\n",
    "mean_men = (12*1 + 16*0)/float(28)\n",
    "mean_men"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# variance for men\n",
    "\n",
    "var_men = (12*(1 - mean_men)**2 + 16* (0 - mean_men)**2)/28\n",
    "var_men"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "# Weighted variance for gender split\n",
    "weighted_variance = (28/float(50)*var_men) + (22/float(50)*var_women)\n",
    "weighted_variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Now let's do the needful if we're splitting our parent node based on *Employment Status*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "crosstab_var = crosstab2.iloc[:,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "crosstab_var[\"average\"] = crosstab_var.yes*1/crosstab_var.Total\n",
    "crosstab_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# variance\n",
    "\n",
    "crosstab_var[\"variance\"] = (crosstab_var.yes * (1 - crosstab_var.average)**2 \n",
    "                            + crosstab_var.no * (0 - crosstab_var.average)**2) / crosstab_var.Total\n",
    "\n",
    "crosstab_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "weighted_emp = (41/float(50))*crosstab_var.loc[\"working\", \"variance\"]\\\n",
    "                    + (9/float(50))*crosstab_var.loc[\"student\", \"variance\"]\n",
    "weighted_emp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Here, variance (weighted_emp) split is slightly higher than variance (weighted_gender) split.**\n",
    "***\n",
    "- Above, you can see that Gender split has lowest variance compare to parent node, so the split would take place on Gender variable.\n",
    "- Let's get back to what Lucius does now! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Lucius' Approach \n",
    "***\n",
    "- We have learned, to quite some detail, how the algorithm decides to split the parent node\n",
    "- Let's see how Lucius implemented a decision tree model in Python! \n",
    "\n",
    "- Let's import the necessary libraries, etc. We already know how to do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Importing necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Reading data & seperating it into train and test\n",
    "# Splitting the data into training and target set\n",
    "\n",
    "dataframe = pd.read_csv('../data/loan_prediction.csv')\n",
    "X = dataframe.iloc[:,0:5]\n",
    "y = dataframe.iloc[:,5]\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Decision Tree using the Gini Index criterion**\n",
    "***\n",
    "- Here, let's check the accuracy of the Gini Index method on our training set. \n",
    "- Do you remember how Accuracy is calculated? \n",
    "    - If not, now is a good time to pause and ponder. It has something to do with the Confusion Matrix.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "clf_gini = DecisionTreeClassifier(criterion='gini')\n",
    "clf_gini.fit(X_train,y_train)\n",
    "\n",
    "y_prediction_gini = clf_gini.predict(X_test)\n",
    "accuracy_gini = accuracy_score(y_test,y_prediction_gini)\n",
    "\n",
    "accuracy_gini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Decision Tree using the Information Gain / Entropy criterion**\n",
    "***\n",
    "- Similar calculations here too.\n",
    "\n",
    "- We saw that for the Gini Index method we got an accuracy of ~64.5% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "clf_entropy = DecisionTreeClassifier(criterion='entropy')\n",
    "clf_entropy.fit(X_train,y_train)\n",
    "y_prediction_entropy = clf_entropy.predict(X_test)\n",
    "accuracy_entropy = accuracy_score(y_test,y_prediction_entropy)\n",
    "accuracy_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Plotting a decision tree\n",
    "\n",
    "Decision tree can be plotted using `pydotplus`. Here is an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import pydotplus\n",
    "from IPython.display import Image\n",
    "X = 'ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term', 'Credit_History'\n",
    "y = 'Loan_Status'\n",
    "dot_data = tree.export_graphviz(clf_entropy, out_file=None,\n",
    "                               feature_names=X,\n",
    "                               class_names=y,\n",
    "                               filled=True, rounded=True,\n",
    "                               special_characters=True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lucius' Thoughts\n",
    "***\n",
    "- Lucius was curious to alter some parameters of his Decision Tree in order to prevent over-fitting and get a better accuracy. (we got accuracies below ~ 70% which suggest that our models were not that good) \n",
    "\n",
    "So, what are the key parameters of tree modeling and how can we avoid over-fitting in decision trees?\n",
    "\n",
    " - Let's discuss this and build our intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Shortcomings decision trees\n",
    "\n",
    "- Overfitting is one of the key challenges faced while modeling decision trees\n",
    "- If there is no limit set of a decision tree, it will give you 100% accuracy on training set because in the worse case it will end up making 1 leaf for each observation\n",
    "- Thus, preventing overfitting is pivotal while modeling a decision tree and it can be done in 2 ways:\n",
    "\n",
    "    - Setting constraints on tree size\n",
    "    - Tree pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br />\n",
    "## Setting Constraints \n",
    "***\n",
    "- First, lets look at the general structure of a decision tree:\n",
    "\n",
    "<center><img src=\"../images/dt12.png\" alt=\"Drawing\" style=\"width: 750px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Minimum samples for a node split**\n",
    " - Defines the minimum number of samples (or observations) which are required in a node to be considered for splitting.\n",
    " - Used to control over-fitting. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.\n",
    " - Too high values can lead to under-fitting hence, it should be tuned using CV.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Minimum samples for a terminal node (leaf)**\n",
    " - Defines the minimum samples (or observations) required in a terminal node or leaf.\n",
    " - Used to control over-fitting similar to min_samples_split.\n",
    " - Generally lower values should be chosen for imbalanced class problems because the regions in which the minority class will be in majority will be very small.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Maximum depth of tree (vertical depth)**\n",
    "\n",
    "- Used to control over-fitting as higher depth will allow model to learn relations very specific to a particular sample.\n",
    "- Should be tuned using CV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Maximum number of terminal nodes**\n",
    " - The maximum number of terminal nodes or leaves in a tree.\n",
    " - Can be defined in place of max_depth. Since binary trees are created, a depth of ‘n’ would produce a maximum of $2^n$ leaves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Maximum features to consider for split**\n",
    " - The number of features to consider while searching for a best split. These will be randomly selected.\n",
    " - As a thumb-rule, square root of the total number of features works great but we should check upto 30-40% of the total number of features.\n",
    " - Higher values can lead to over-fitting but depends on case to case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lucius Approach -2 : Effect of varying max_depth\n",
    "***\n",
    "\n",
    "Lucius chose **Depths of 2 and 5 respectively **and then compared the results to see which is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "clf_1 = DecisionTreeClassifier(max_depth = 2)\n",
    "clf_1.fit(X_train,y_train)\n",
    "\n",
    "clf_2 = DecisionTreeClassifier(max_depth = 5)\n",
    "clf_2.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# predicting for both max depth 2 and 5.\n",
    "\n",
    "y_clf_1 = clf_1.predict(X_test)\n",
    "y_clf_2 = clf_2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#calculate accuracy\n",
    "accuracy_clf_1 = accuracy_score(y_test,y_clf_1)\n",
    "accuracy_clf_2 = accuracy_score(y_test,y_clf_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "accuracy_clf_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "accuracy_clf_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Better Results: \n",
    "***\n",
    "- Lucius compared these results with what he got before \n",
    "- We can notice that the accuracies of both these models is better than the models before ( ~ 64%) \n",
    "\n",
    "**NOTE** : In Python, the default criterion to make the splits is the Gini Index unless specified otherwise!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Figuring out his Future\n",
    "***\n",
    "- Now, using this, Lucius wanted to check if he would get a loan too\n",
    "- Therefore, for the sake of simplicity let's assume that the loan amounts in the dataset are in Thousands of dollars\n",
    "\n",
    "- Each semester, NYU charges a whopping $26,500 for his course\n",
    "- His program is 6 semesters long and he needs the fees for all the 6 semesters \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## Enter your code here: Predict Lucius future, well, somewhat!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Advantages of Decision Trees: \n",
    "***\n",
    "1) Easy to Understand: \n",
    "- Decision tree output is very easy to understand even for people from non-analytical background. \n",
    "- It does not require any statistical knowledge to read and interpret them.\n",
    "- Its graphical representation is very intuitive and users can easily relate their hypothesis.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "2) Useful in Data exploration: \n",
    "- Decision tree is one of the fastest way to identify most significant variables and relation between two or more variables. - With the help of decision trees, we can create new variables / features that has better power to predict target variable. -  It can also be used in data exploration stage. For example, we are working on a problem where we have information available in hundreds of variables, there decision tree will help to identify most significant variable.\n",
    "\n",
    "3) Less data cleaning required: \n",
    "- It requires less data cleaning compared to some other modeling techniques.\n",
    "- It is not influenced by outliers and missing values to a fair degree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "4) Data type is not a constraint:\n",
    "- It can handle both numerical and categorical variables.\n",
    "\n",
    "5) Non Parametric Method: \n",
    "- Decision tree is considered to be a non-parametric method. \n",
    "- This means that decision trees have no assumptions about the space distribution and the classifier structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Drawbacks of Decision Trees\n",
    "***\n",
    "* Decision-tree learners can create over-complex trees that do not generalise the data well. This is called overfitting.\n",
    "\n",
    "* Decision trees can be unstable because small variations in the data might result in a completely different tree being generated.\n",
    "\n",
    "* Decision tree learners create biased trees if some classes dominate. It is therefore recommended to balance the dataset prior to fitting with the decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## In-class Activity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset description:\n",
    "Dataset UCI Adult: classify people using demographical data - whether they earn more than $50,000 per year or not.\n",
    "\n",
    "### Feature descriptions:\n",
    "```python\n",
    "Age – continuous feature\n",
    "Workclass – continuous feature\n",
    "fnlwgt – final weight of object, continuous feature\n",
    "Education – categorical feature\n",
    "Education_Num – number of years of education, continuous feature\n",
    "Martial_Status – categorical feature\n",
    "Occupation – categorical feature\n",
    "Relationship – categorical feature\n",
    "Race – categorical feature\n",
    "Sex – categorical feature\n",
    "Capital_Gain – continuous feature\n",
    "Capital_Loss – continuous feature\n",
    "Hours_per_week – continuous feature\n",
    "Country – categorical feature\n",
    "Target – earnings level, categorical (binary) feature.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (10, 8)\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import collections\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.metrics import accuracy_score\n",
    "from ipywidgets import Image\n",
    "from io import StringIO\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pydotplus #pip install pydotplus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Train and Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('../data/adult_train.csv')\n",
    "data_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.read_csv('../data/adult_test.csv')\n",
    "data_test.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove rows with 'nan' values from test dataset with respect to target variable and encode target variable as `0` and `1` for train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "# necessary to remove rows with incorrect labels in test dataset\n",
    "data_test = data_test[(data_test['Target'] == ' >50K.') | (data_test['Target']==' <=50K.')]\n",
    "\n",
    "# encode target variable as integer\n",
    "data_train.loc[data_train['Target']==' <=50K', 'Target'] = 0\n",
    "data_train.loc[data_train['Target']==' >50K', 'Target'] = 1\n",
    "\n",
    "data_test.loc[data_test['Target']==' <=50K.', 'Target'] = 0\n",
    "data_test.loc[data_test['Target']==' >50K.', 'Target'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the statistics of the test data and count the distribution of target variable in train data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "data_test.describe(include='all').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "data_train['Target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the distribution of each feature, so that we have a better understanding about the distribution of each feature. Draw the number of values for each category feature and the histogram of the values for each continuous feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "fig = plt.figure(figsize=(25, 15))\n",
    "cols = 5\n",
    "rows = np.ceil(float(data_train.shape[1]) / cols)\n",
    "for i, column in enumerate(data_train.columns):\n",
    "    ax = fig.add_subplot(rows, cols, i + 1)\n",
    "    ax.set_title(column)\n",
    "    if data_train.dtypes[column] == np.object:\n",
    "        data_train[column].value_counts().plot(kind=\"bar\", axes=ax)\n",
    "    else:\n",
    "        data_train[column].hist(axes=ax)\n",
    "        plt.xticks(rotation=\"vertical\")\n",
    "plt.subplots_adjust(hspace=0.7, wspace=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the plots above our data is mostly concentrated in the USA with mostly male white \n",
    "people.This is a good thing to notice, as it may impact the conclusions we come to later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the percentage of adults from each country in the data *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "(data_train[\"Country\"].value_counts() / data_train.shape[0]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Indeed! 89% of the samples are for people from the US. Mexico comes next with less than 2%.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the data types of train and test data *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "print(data_train.dtypes)\n",
    "print(data_test.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "As we see, in the test data, age is treated as type object. We need to fix this.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As we have seen in the earlier task, the data type of age in the test data is object type, so we will have to convert the data type of `Age` column in the test data to `int` type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "data_test['Age'] = data_test['Age'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Also cast all float features to int type to keep types consistent between our train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "data_test['fnlwgt'] = data_test['fnlwgt'].astype(int)\n",
    "data_test['Education_Num'] = data_test['Education_Num'].astype(int)\n",
    "data_test['Capital_Gain'] = data_test['Capital_Gain'].astype(int)\n",
    "data_test['Capital_Loss'] = data_test['Capital_Loss'].astype(int)\n",
    "data_test['Hours_per_week'] = data_test['Hours_per_week'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have to treat categorical and numerical data in different ways for cleaning the data and encoding, so we will have to store these in two different columns. Choose categorical and continuous features from train data and store in different variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "# choose categorical and continuous features from data\n",
    "\n",
    "categorical_columns = [c for c in data_train.columns \n",
    "                       if data_train[c].dtype.name == 'object']\n",
    "numerical_columns = [c for c in data_train.columns \n",
    "                     if data_train[c].dtype.name != 'object']\n",
    "\n",
    "print('categorical_columns:', categorical_columns)\n",
    "print('numerical_columns:', numerical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As we have already separated the numerical and categorical columns, we will fill in missing data for continuous features with their median values, for categorical features with their mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "# fill missing data\n",
    "\n",
    "for c in categorical_columns:\n",
    "    data_train[c].fillna(data_train[c].mode()[0], inplace=True)\n",
    "    data_test[c].fillna(data_train[c].mode()[0], inplace=True)\n",
    "    \n",
    "for c in numerical_columns:\n",
    "    data_train[c].fillna(data_train[c].median(), inplace=True)\n",
    "    data_test[c].fillna(data_train[c].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machines only understand numbers, so we will have to represent the categorical variables in a way so that the computer understands it, so we will first label encode and then one hot encode the categorical variables. We'll dummy code some categorical features: Workclass, Education, Martial_Status, Occupation, Relationship, Race, Sex, Country. It can be done via pandas method get_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "for x in categorical_columns:\n",
    "    data_train[x] = le.fit_transform(data_train[x])\n",
    "    data_test[x] = le.transform(data_test[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternate way with a single line of code to encode data with categorical data without complicating if you want to use.\n",
    "\n",
    "pd.get_dummies(data=data_train, columns = categorical_columns).shape\n",
    "\n",
    "data_train = pd.concat([data_train[numerical_columns],\n",
    "    pd.get_dummies(data_train[categorical_columns])], axis=1)\n",
    "\n",
    "data_test = pd.concat([data_test[numerical_columns],\n",
    "    pd.get_dummies(data_test[categorical_columns])], axis=1)\n",
    "\n",
    "data_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train and test data into X_train ,y_train,X_test and y_test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "X_train = data_train.drop(['Target'], axis=1)\n",
    "y_train = data_train['Target']\n",
    "\n",
    "X_test = data_test.drop(['Target'], axis=1)\n",
    "y_test = data_test['Target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets now train our data with Decision tree model. We will then predict our test data and compute the accuracy.\n",
    "\n",
    "* Train a decision tree (DecisionTreeClassifier) with a maximum depth of 3, and evaluate the accuracy metric on the test data. Use parameter random_state = 17 for results reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=3, random_state=17)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "tree_predictions = tree.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "# Check the accuracy score\n",
    "\n",
    "print(\"Decision tree accuracy: \",accuracy_score(y_test,tree_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We see that the accuracy we got is 84% without using parameter tuning Now lets see if we can increase our model accuracy by using Decision tree with parameter tuning\n",
    "* Create a dictionary with Key as `'max_depth'` and value as `range(2,11)` and save it as tree_params.\n",
    "* Train a decision tree (DecisionTreeClassifier(random_state = 17),tree_params). Find the optimal maximum depth using 5-fold cross-validation (GridSearchCV)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_params = {'max_depth': range(2, 11)}\n",
    "\n",
    "locally_best_tree = GridSearchCV(DecisionTreeClassifier(random_state=17),\n",
    "                                 tree_params, cv=5)                  \n",
    "\n",
    "\n",
    "# Your code here\n",
    "\n",
    "locally_best_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print out optimal maximum depth(i.e. best_params_ attribute of GridSearchCV) and best_score_  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "print(\"Best params:\", locally_best_tree.best_params_)\n",
    "print(\"Best cross validaton score\", locally_best_tree.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a decision tree with maximum depth of 9 (it is the best max_depth in my case), and compute the test set accuracy. Use parameter random_state = 17 for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "tuned_tree = DecisionTreeClassifier(max_depth=9, random_state=17)\n",
    "tuned_tree.fit(X_train, y_train)\n",
    "tuned_tree_predictions = tuned_tree.predict(X_test)\n",
    "print(\"Decision tree Accuracy after tuning: \",accuracy_score(y_test, tuned_tree_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets plot the decision tree and see its distibution of classes\n",
    "\n",
    "* Use the tree.export_graphviz() module to create the graphic visualization object of the learned tree classifier .Input the parameters tuned_tree, out_file=None, feature_names=X_train.columns, filled = True, class_names=['class_no','class_yes'] and save it as dot_data\n",
    "\n",
    "* Use the pydotplus.graph_from_dot_data() module to draw the graph. Save it as graph\n",
    "\n",
    "* To display the graph we save the above created image into a file and display it using matplotlib as  Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image  \n",
    "from sklearn import tree\n",
    "import pydotplus\n",
    "\n",
    "# Create DOT data\n",
    "dot_data = tree.export_graphviz(tuned_tree, out_file=None, \n",
    "                                feature_names=X_train.columns, filled = True,  \n",
    "                                class_names=['class_yes','class_no'])\n",
    "\n",
    "# Draw graph\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)  \n",
    "\n",
    "# Show graph\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PNG\n",
    "graph.write_png(\"graph.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/Recap.png\" alt=\"Recap\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "\n",
    "<br />\n",
    "\n",
    "# In-session Recap Time\n",
    "***\n",
    "- What is Decision Trees and how to use it?\n",
    "- Splits in Decision Trees\n",
    "    - Gini Index\n",
    "    - Entropy\n",
    "    - Chi Squared\n",
    "- Advantages of Decision Trees\n",
    "- Drawbacks of Decsion Trees\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Thank You\n",
    "***\n",
    "### Coming up next...\n",
    "***\n",
    "- Ensembling"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
