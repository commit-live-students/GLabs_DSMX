{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vi3gIFAQRS9H"
   },
   "source": [
    "# POS tagging using spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3XMpr2s4K20i"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.pipeline import SentenceSegmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AHa5eaZgK2hU"
   },
   "outputs": [],
   "source": [
    "# Construction via create_pipe\n",
    "sentencizer = nlp.create_pipe(\"sentencizer\")\n",
    "\n",
    "# Construction from class\n",
    "from spacy.pipeline import Sentencizer\n",
    "sentencizer = Sentencizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 919,
     "status": "ok",
     "timestamp": 1583034343614,
     "user": {
      "displayName": "Pradeepta Mishra",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj8snChTjRe3VK7AdFX9bDDfyleHyR_jgZamFg1WcQ=s64",
      "userId": "08371374182622470276"
     },
     "user_tz": -330
    },
    "id": "TYNWpp6XK2eo",
    "outputId": "16844004-5d76-455e-fb96-11810148dc11"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "sentencizer = nlp.create_pipe(\"sentencizer\")\n",
    "nlp.add_pipe(sentencizer)\n",
    "doc = nlp(\"A simple pipeline component, to allow custom sentence boundary detection logic that doesn’t require the dependency parse. By default, sentence segmentation is performed by the DependencyParser, so the Sentencizer lets you implement a simpler, rule-based strategy that doesn’t require a statistical model to be loaded. The component is also available via the string name sentencizer. After initialization, it is typically added to the processing pipeline using nlp.add_pipe.\")\n",
    "len(list(doc.sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2160,
     "status": "ok",
     "timestamp": 1583034456906,
     "user": {
      "displayName": "Pradeepta Mishra",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj8snChTjRe3VK7AdFX9bDDfyleHyR_jgZamFg1WcQ=s64",
      "userId": "08371374182622470276"
     },
     "user_tz": -330
    },
    "id": "i03_pWZoRMRS",
    "outputId": "ae6253de-4514-42b3-f978-d5d16d637d07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 'DET', 'simple': 'ADJ', 'pipeline': 'NOUN', 'component': 'NOUN', ',': 'PUNCT', 'to': 'PART', 'allow': 'VERB', 'custom': 'NOUN', 'sentence': 'NOUN', 'boundary': 'ADJ', 'detection': 'NOUN', 'logic': 'NOUN', 'that': 'DET', 'does': 'VERB', 'n’t': 'ADV', 'require': 'VERB', 'the': 'DET', 'dependency': 'NOUN', 'parse': 'NOUN', '.': 'PUNCT'}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp('A simple pipeline component, to allow custom sentence boundary detection logic that doesn’t require the dependency parse.')\n",
    "\n",
    "pos_tags={}\n",
    "for token in doc:\n",
    "    pos_tags[token.text]=token.pos_\n",
    "\n",
    "print(pos_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dPl96oodPpd6"
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "79ids17bPlG2"
   },
   "outputs": [],
   "source": [
    "# Grammar defined for our task   \n",
    "\n",
    "adhoc_grammar = nltk.CFG.fromstring(\"\"\"\n",
    "  S  -> NP VP\n",
    "  NP -> Det Nom | PropN\n",
    "  Nom -> Adj Nom | N\n",
    "  VP -> V Adj | V NP | V S | V NP PP\n",
    "  PP -> P NP\n",
    "  PropN -> 'Adam' | 'Nicole' | 'Sam'\n",
    "  Det -> 'the' | 'a'\n",
    "  N -> 'bear' | 'squirrel' | 'tree' | 'fish' | 'log'| 'cat'|'dog' \n",
    "  Adj  -> 'angry' | 'frightened' |  'little' | 'tall'\n",
    "  V ->  'chased'  | 'saw' | 'said' | 'thought' | 'was' | 'put'| 'rescued'\n",
    "  P -> 'on'\n",
    "  \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UIeh2cQpPk-J"
   },
   "outputs": [],
   "source": [
    "# Sentence 1\n",
    "sentence_1 = 'the angry dog chased the frightened little cat'.split()\n",
    "\n",
    "# Sentence\n",
    "sentence_2= 'Adam rescued the squirrel'.split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Sv-pe14Pk62"
   },
   "outputs": [],
   "source": [
    "# Function for parsing\n",
    "\n",
    "def parse(sentence):\n",
    "    \n",
    "    # List for parsed tree\n",
    "    parsed_tree = []  \n",
    "    \n",
    "    # Parsing done by \n",
    "    parser = nltk.ChartParser(adhoc_grammar)\n",
    "    \n",
    "\n",
    "    # Loop for creating appending the trees\n",
    "    for tree in parser.parse(sentence):\n",
    "        parsed_tree.append(tree)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Return the parsed tree\n",
    "    return(str(parsed_tree[0])) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 839,
     "status": "ok",
     "timestamp": 1583035291112,
     "user": {
      "displayName": "Pradeepta Mishra",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj8snChTjRe3VK7AdFX9bDDfyleHyR_jgZamFg1WcQ=s64",
      "userId": "08371374182622470276"
     },
     "user_tz": -330
    },
    "id": "XfhLNX72QPqx",
    "outputId": "d42e9bf2-07a8-42f9-c006-f8b812d6f7bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Adam', 'rescued', 'the', 'squirrel']\n",
      "(S\n",
      "  (NP (PropN Adam))\n",
      "  (VP (V rescued) (NP (Det the) (Nom (N squirrel)))))\n"
     ]
    }
   ],
   "source": [
    "# Calling the parse function for sentence 1\n",
    "pt2=(parse(sentence_2))\n",
    "print(sentence_2)\n",
    "print(pt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 870,
     "status": "ok",
     "timestamp": 1583035188459,
     "user": {
      "displayName": "Pradeepta Mishra",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj8snChTjRe3VK7AdFX9bDDfyleHyR_jgZamFg1WcQ=s64",
      "userId": "08371374182622470276"
     },
     "user_tz": -330
    },
    "id": "rYyeXooJPknG",
    "outputId": "f685d178-53d9-4971-efde-be98dd7df524"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'angry', 'dog', 'chased', 'the', 'frightened', 'little', 'cat']\n",
      "(S\n",
      "  (NP (Det the) (Nom (Adj angry) (Nom (N dog))))\n",
      "  (VP\n",
      "    (V chased)\n",
      "    (NP\n",
      "      (Det the)\n",
      "      (Nom (Adj frightened) (Nom (Adj little) (Nom (N cat)))))))\n"
     ]
    }
   ],
   "source": [
    "# Calling the parse function for sentence 1   \n",
    "pt1=(parse(sentence_1))\n",
    "print(sentence_1)\n",
    "print(pt1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tVbB_ILNRlA0"
   },
   "source": [
    "## Context free grammer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 905,
     "status": "ok",
     "timestamp": 1583035300262,
     "user": {
      "displayName": "Pradeepta Mishra",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj8snChTjRe3VK7AdFX9bDDfyleHyR_jgZamFg1WcQ=s64",
      "userId": "08371374182622470276"
     },
     "user_tz": -330
    },
    "id": "lBCPq72HRq56",
    "outputId": "0d34422f-425c-499e-a245-9bb035fcfc09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP (Det the) (Nom (Adj angry) (Nom (N dog))))\n",
      "  (VP\n",
      "    (V chased)\n",
      "    (NP\n",
      "      (Det the)\n",
      "      (Nom (Adj frightened) (Nom (Adj little) (Nom (N cat)))))))\n",
      "(S\n",
      "  (NP (PropN Adam))\n",
      "  (VP (V rescued) (NP (Det the) (Nom (N squirrel)))))\n"
     ]
    }
   ],
   "source": [
    "# Grammar defined for our task   \n",
    "\n",
    "adhoc_grammar = nltk.CFG.fromstring(\"\"\"\n",
    "  S  -> NP VP\n",
    "  NP -> Det Nom | PropN\n",
    "  Nom -> Adj Nom | N\n",
    "  VP -> V Adj | V NP | V S | V NP PP\n",
    "  PP -> P NP\n",
    "  PropN -> 'Adam' | 'Nicole' | 'Sam'\n",
    "  Det -> 'the' | 'a'\n",
    "  N -> 'bear' | 'squirrel' | 'tree' | 'fish' | 'log'| 'cat'|'dog' \n",
    "  Adj  -> 'angry' | 'frightened' |  'little' | 'tall'\n",
    "  V ->  'chased'  | 'saw' | 'said' | 'thought' | 'was' | 'put'| 'rescued'\n",
    "  P -> 'on'\n",
    "  \"\"\")\n",
    "\n",
    "# Sentence 1\n",
    "sentence_1 = 'the angry dog chased the frightened little cat'.split()\n",
    "\n",
    "# Sentence\n",
    "sentence_2= 'Adam rescued the squirrel'.split()\n",
    "\n",
    "\n",
    "# Function for parsing\n",
    "\n",
    "def parse(sentence):\n",
    "    \n",
    "    # List for parsed tree\n",
    "    parsed_tree = []  \n",
    "    \n",
    "    # Parsing done by \n",
    "    parser = nltk.ChartParser(adhoc_grammar)\n",
    "    \n",
    "\n",
    "    # Loop for creating appending the trees\n",
    "    for tree in parser.parse(sentence):\n",
    "        parsed_tree.append(tree)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Return the parsed tree\n",
    "    return(str(parsed_tree[0])) \n",
    "\n",
    "\n",
    "# Calling the parse function for sentence 1   \n",
    "pt1=(parse(sentence_1))\n",
    "print(pt1)\n",
    "\n",
    "# Calling the parse function for sentence 1\n",
    "pt2=(parse(sentence_2))\n",
    "print(pt2)\n",
    "\n",
    "# Cleaning of the trees for better visualization purposes\n",
    "\n",
    "pt1=pt1.replace('(','[')\n",
    "pt1=pt1.replace(')',']')\n",
    "\n",
    "\n",
    "pt2=pt2.replace('(','[')\n",
    "pt2=pt2.replace(')',']')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A-ubqmjESTc0"
   },
   "source": [
    "## Statistical parsing using pcgf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1031,
     "status": "ok",
     "timestamp": 1583036717217,
     "user": {
      "displayName": "Pradeepta Mishra",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj8snChTjRe3VK7AdFX9bDDfyleHyR_jgZamFg1WcQ=s64",
      "userId": "08371374182622470276"
     },
     "user_tz": -330
    },
    "id": "XTbEzNMiRyPW",
    "outputId": "63ec376d-5168-4995-de8f-ff1e4d807fa8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A PCFG production: VP -> V NP [0.7]\n",
      "pcfg_prod.lhs()  => VP\n",
      "pcfg_prod.rhs()  => (V, NP)\n",
      "pcfg_prod.prob() => 0.7\n",
      "('telescope',)\n",
      "('man',)\n",
      "('man',)\n"
     ]
    }
   ],
   "source": [
    "# Header Files\n",
    "\n",
    "from nltk import PCFG\n",
    "from nltk.probability import DictionaryProbDist\n",
    "from nltk import nonterminals, Nonterminal, Production\n",
    "from nltk.corpus import treebank\n",
    "from nltk import treetransforms\n",
    "from nltk import induce_pcfg\n",
    "from nltk.parse import pchart\n",
    "\n",
    "# PCFG Grammar\n",
    "\n",
    "toy_pcfg1 = PCFG.fromstring(\"\"\"\n",
    "    S -> NP VP [1.0]\n",
    "    NP -> Det N [0.5] | NP PP [0.25] | 'John' [0.1] | 'I' [0.15]\n",
    "    Det -> 'the' [0.8] | 'my' [0.2]\n",
    "    N -> 'man' [0.5] | 'telescope' [0.5]\n",
    "    VP -> VP PP [0.1] | V NP [0.7] | V [0.2]\n",
    "    V -> 'ate' [0.35] | 'saw' [0.65]\n",
    "    PP -> P NP [1.0]\n",
    "    P -> 'with' [0.61] | 'under' [0.39]\n",
    "\"\"\")\n",
    "\n",
    "# Saving all the rules of the grammar in a variable\n",
    "pcfg_prods = toy_pcfg1.productions()\n",
    "\n",
    "\n",
    "# Selecting one probability grammar rule\n",
    "pcfg_prod = pcfg_prods[10]\n",
    "\n",
    "\n",
    "print('A PCFG production:', pcfg_prod)\n",
    "print('pcfg_prod.lhs()  =>', pcfg_prod.lhs())\n",
    "print('pcfg_prod.rhs()  =>', pcfg_prod.rhs())\n",
    "print('pcfg_prod.prob() =>', pcfg_prod.prob())\n",
    "\n",
    "# Taking all productions where LHS=N\n",
    "n_productions = toy_pcfg1.productions(Nonterminal('N'))\n",
    "\n",
    "dict = {}\n",
    "for pr in n_productions: dict[pr.rhs()] = pr.prob()\n",
    "n_probDist = DictionaryProbDist(dict)\n",
    "\n",
    "# Generates random samples depending on the prob.\n",
    "\n",
    "print(n_probDist.generate())\n",
    "\n",
    "print(n_probDist.generate())\n",
    "\n",
    "print(n_probDist.generate())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "parsing_code_along.ipynb",
   "provenance": [
    {
     "file_id": "1XaygP7zoE380AfOwiR5NN9S6aYDSAICt",
     "timestamp": 1564396731598
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
