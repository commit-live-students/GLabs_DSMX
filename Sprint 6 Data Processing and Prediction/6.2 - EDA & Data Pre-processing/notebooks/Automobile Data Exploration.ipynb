{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exploratory Data Analysis\n",
    "![EDA_open](../images/eda_open.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Program so far \n",
    "***\n",
    "* Python Data Science Tool box\n",
    "* Introduction to machine learning\n",
    "* Summarizing the Data\n",
    "* Art of Statistical Inference\n",
    "* Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# What are we going to learn today?\n",
    "***\n",
    "- Initial Exploration\n",
    "- Introduction to Seaborn\n",
    "- Univariate Analysis\n",
    "- Multi-variate Analysis\n",
    "- Scaling, Centering, Skewness \n",
    "- Basic data cleaning and Preprocessing\n",
    "- Feature extraction and Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now all the data that we have seen so far has been clean or pre-cleaned. In real-life we rarely get such clean datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## John's Concerns\n",
    "***\n",
    "After dealing with outliers, John realised the significance of a clean data set. So he decided to learn more about data cleaning and data manipulation.\n",
    "\n",
    "He used the data he had collected so far.\n",
    "\n",
    "Let's see how John proceeds!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt  \n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "#For some Statistics\n",
    "from scipy.stats import norm, skew\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import the set\n",
    "df = pd.read_csv('../data/train.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It’s always a good idea to take a closer look at the data itself. \n",
    "\n",
    "With the help of the `head()` and `tail()` functions of the Pandas library, he easily checked out the first few and last few lines of the DataFrame, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code starts here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code starts here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also a good practice to know the columns and their corresponsding data types, along with finding whether they contain null value or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code starts here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `describe()` function helped him to get various summary statistics that exclude NaN values.\n",
    "\n",
    "This function returns the **count, mean, standard deviation, minimum and maximum** values and the quantiles of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code starts here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "He also found out the total number of rows and columns in the dataset using the Syntax below:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code starts here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to find the Numerical Features of our Dataset?\n",
    "***\n",
    "Now, let's see how John found out and listed the number of Numerical Features in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code starts here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to find the Categorical Features in our Dataset?\n",
    "***\n",
    "As in the case of Numerical Features, let's find out the number of Categorical Features in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code starts here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br /> \n",
    "# John's Introduction to Seaborn\n",
    "***\n",
    "John got a good a glimpse of his data. But that's the thing with Data Science the more you get involved the harder is it for you to stop exploring.\n",
    "\n",
    "Now, John wanted to **analyze** his data in order to extract some insights. And after a lot of consultation and google search, John learned about Seaborn.\n",
    "\n",
    "John used Seaborn to do both **Univariate and Multivariate analysis**. How? we will see soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## So what is Seaborn? (1/2)\n",
    "***\n",
    "Seaborn is a Python visualization library based on matplotlib. \n",
    "\n",
    "It provides a high-level interface for drawing attractive statistical graphics.\n",
    "\n",
    "Some of the features that seaborn offers are : \n",
    "\n",
    "* Several built-in themes for styling matplotlib graphics\n",
    "* Tools for choosing color palettes to make beautiful plots that reveal patterns in your data\n",
    "* Functions for visualizing univariate and bivariate distributions or for comparing them between subsets of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## So what is Seaborn? (2/2)\n",
    "***\n",
    "* Tools that fit and visualize linear regression models for different kinds of independent and dependent variables\n",
    "* Functions that visualize matrices of data and use clustering algorithms to discover structure in those matrices\n",
    "* A function to plot statistical timeseries data with flexible estimation and representation of uncertainty around the estimate\n",
    "* High-level abstractions for structuring grids of plots that let you easily build complex visualizations\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">**You can import Seaborn as below :**</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br /> \n",
    "## What is Univariate Analysis and how did John use it?\n",
    "***\n",
    "** What is Univariate Analysis?**\n",
    "\n",
    "* Univariate analysis is the simplest form of analyzing data.\n",
    "\n",
    "* “Uni” means “one”, so in other words your data has only one variable. \n",
    "\n",
    "* It doesn’t deal with causes or relationships (unlike regression) and it’s major purpose is to describe; it takes data, summarizes that data and finds patterns in the data.\n",
    "\n",
    "Univariate Analysis can be done either on **Numerical or Categorical** features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Numerical Features in Univariate Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br /> \n",
    "### Histogram\n",
    "***\n",
    "When dealing with a set of data, often the first thing we want to do is get a sense for how the variables are distributed. \n",
    "\n",
    "John started by identifying a few variables of interest and checking their distribution. \n",
    "\n",
    "A histogram represents the distribution of data by forming bins along the range of the data and then drawing bars to show the number of observations that fall in each bin.\n",
    "\n",
    "The bin can be of any size.\n",
    "\n",
    "Let's plot a histogram : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "x = np.random.normal(size=100)\n",
    "sns.distplot(x, kde=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In order to make any prediction John needed to fit a linear regression model, so he made sure the distribution of the variables is almost linear. \n",
    "\n",
    "Now to check the linearity of the variables he checked for any skewness in the distribution and the outliers in the data.\n",
    "\n",
    "The variables he checked for were 'OverallQual', 'YearBuilt', 'TotalBsmtSF', 'GrLivArea', 'Neighborhood' and the target variable 'SalePrice'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Histograms\n",
    "num_cols = ['YearBuilt', 'TotalBsmtSF', 'GrLivArea', 'SalePrice']\n",
    "for i in range(0,len(num_cols),2):\n",
    "    if len(num_cols) > i+1:\n",
    "        plt.figure(figsize=(10,4))\n",
    "        plt.subplot(121)\n",
    "        sns.distplot(df[num_cols[i]], kde=False)\n",
    "        plt.subplot(122)            \n",
    "        sns.distplot(df[num_cols[i+1]], kde=False)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        sns.distplot(df[num_cols[i]], kde=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br /> \n",
    "### The Kernel Density Estimation\n",
    "***\n",
    "The kernel density estimate may be less familiar, but it can be a useful tool for plotting the shape of a distribution.\n",
    "\n",
    "Like the histogram, the KDE plots encodes the density of observations on one axis with height along the other axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Histograms\n",
    "\n",
    "num_cols = ['YearBuilt', 'TotalBsmtSF', 'GrLivArea', 'SalePrice']\n",
    "for i in range(0,len(num_cols),2):\n",
    "    if len(num_cols) > i+1:\n",
    "        plt.figure(figsize=(10,4))\n",
    "        plt.subplot(121)\n",
    "        sns.distplot(df[num_cols[i]], hist=True, kde=True)\n",
    "        plt.subplot(122)            \n",
    "        sns.distplot(df[num_cols[i+1]], hist=True, kde=True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        sns.distplot(df[num_cols[i]], hist=True, kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "John observed that all of the histograms are left or right skewed, hence a transformation is required to make them linear.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br /> \n",
    "### Boxplots\n",
    "***\n",
    "A box plot (or box-and-whisker plot) shows the distribution of quantitative data in a way that facilitates comparisons between variables or across levels of a categorical variable. \n",
    "\n",
    "The box shows the quartiles of the dataset while the whiskers extend to show the rest of the distribution.\n",
    "\n",
    "Using this John could find the features which could be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "tips = sns.load_dataset(\"tips\")\n",
    "sns.boxplot(x=tips[\"total_bill\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br /> \n",
    "### More About Boxplots (1/2)\n",
    "***\n",
    "The box plot (a.k.a. box and whisker diagram) is a standardized way of displaying the distribution of data based on the five number summary: \n",
    "* Minimum\n",
    "* First quartile\n",
    "* Median\n",
    "* Third quartile\n",
    "* Maximum. \n",
    "\n",
    "In the simplest box plot the central rectangle spans the first quartile to the third quartile (the interquartile range or IQR). \n",
    "\n",
    "A segment inside the rectangle shows the median and \"whiskers\" above and below the box show the locations of the minimum and maximum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br /> \n",
    "### More About Boxplots (2/2)\n",
    "***\n",
    "\n",
    "![](./images/boxplot.png)\n",
    "\n",
    "**Outliers** are either 3×IQR or more above the third quartile or 3×IQR or more below the first quartile.\n",
    "\n",
    "**Suspected outliers** are are slightly more central versions of outliers: either 1.5×IQR or more above the third quartile or 1.5×IQR or more below the first quartile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# BoxPlots\n",
    "\n",
    "num_cols = ['YearBuilt', 'TotalBsmtSF', 'GrLivArea', 'SalePrice']\n",
    "facet = None\n",
    "for i in range(0,len(num_cols),2):\n",
    "    if len(num_cols) > i+1:\n",
    "        plt.figure(figsize=(10,4))\n",
    "        plt.subplot(121)\n",
    "        sns.boxplot(facet, num_cols[i],data = df)\n",
    "        plt.subplot(122)            \n",
    "        sns.boxplot(facet, num_cols[i+1],data = df)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        sns.boxplot(facet, num_cols[i],data = df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Categorical Features in Univariate Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br /> \n",
    "### Countplots\n",
    "***\n",
    "Most of the buildings are built post 1960 and most houses have an area in the range 1-2k sq feet. There are quite a few outliers in the sale price and living area and these might be correlated. John wanted to check the distribution of the categorical columns.\n",
    "\n",
    "He used Countplots for this which does nothing but shows the counts of observations in each categorical bin using bars.\n",
    "\n",
    "A count plot can be thought of as a histogram across a categorical, instead of quantitative, variable. \n",
    "\n",
    "He chose the following columns from the above categorical_features_columns :\n",
    "* Neighborhood\n",
    "* SaleCondition\n",
    "* BldgType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sns.countplot('SaleCondition', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.countplot(y='Neighborhood', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../images/Concept-Alert.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br /> \n",
    "## What is Multivariate Analysis?\n",
    "***\n",
    "John starts to wonder, what if two or more variables are related? This would surely help derive further insight on the houses! \n",
    "\n",
    "In Multivariate analysis John tries to find the relations between multiple variables. Obviously, in real life problems variables can be any combination of numeric or categorical variables.\n",
    "The combinations are:\n",
    "* Numeric vs Numeric\n",
    "* Numeric vs Categorical\n",
    "* Categorical vs Categorical\n",
    "\n",
    "Another aspect of variable combination we look at is:\n",
    "* Feature vs Feature\n",
    "* Feature vs Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Numeric vs Numeric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For these kind of plots John uses a scatterplot of the two variables. Although one can use a variety of plots in seaborn to do a quick and dirty EDA to some sophisticated plots. lmplot is one of the plots to do a scatterplot in seaborn, it'll by default fit a regression line on top which you can control using 'fit_reg' argument.\n",
    " \n",
    "In these plot you will also see him plotting some important feature against our target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "sns.lmplot('GrLivArea', 'SalePrice', data=df, fit_reg=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Using a scatterplot John can also detect *Multivariate outliers*, in this case there are two houses which have an area above ~4500 and they don't follow the trend. Removing these would give a better fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Another plot that John could use is a jointplot which gives a plethora of information in a single plot. It has:\n",
    "* Scatter Plot\n",
    "* Regression line fit to the data.\n",
    "* Histogram and kde of individual variables.\n",
    "* Pearson correlation and p value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sns.jointplot('TotalBsmtSF', 'SalePrice', data=df, kind='reg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">**Another plot one can use is a hexplot which plots two numeric variables. Darker colors signify more points.**</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sns.jointplot('YearBuilt', 'SalePrice', data=df, kind='hex')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Since John is going to use linear regression for modelling, its necessary to remove correlated variables to improve our model. He must find correlations using pandas 'corr' function and can visualize the correlation matrix using a heatmap in seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(df.corr(), cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "John saw the following inferences from above -\n",
    "- A lot of variables are correlated to SalePrice which is good. \n",
    "- GrLivArea is highly correlated with TotRmsAbvGrd.\n",
    "- Also Year the garage was built(GarageYrBlt) is correlated with the year the building was built(YearBuilt). \n",
    "\n",
    "**It's a good idea to remove such correlated variables during feature selection.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">**Now John plots some top variables to see the relations between them. He starts by filtering top 10 variables which are highly correlated with SalePrice.**</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#saleprice correlation matrix\n",
    "k = 10 #number of variables for heatmap\n",
    "cols = df.corr().nlargest(k, 'SalePrice')['SalePrice'].index\n",
    "cm = df[cols].corr()\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(cm, annot=True, cmap = 'viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- 'OverallQual', 'GrLivArea' and 'TotalBsmtSF' are strongly correlated with 'SalePrice'.\n",
    "- 'GarageCars' and 'GarageArea' are also correlated variables. However no of cars that can fit into a garage is dependent on the garage area and one can remove one of these. Check the correlation between them\n",
    "- TotalBsmtSF and 1stFlrSF are also highy correlated. We can drop one of these.\n",
    "- As pointed out above Yearbuilt and TotRmsAbvGrd are highly correlated, we'll discard TotRmsAbvGrd."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">**An easy way to plot all possible interactions is between a set of numeric variables is using pairplot function in seaborn.**</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Visualizing relations between all major variables\n",
    "cols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\n",
    "sns.pairplot(df[cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "John notices a few interesting things here, in the scatter plot of TotalBsmtSF and GrLivArea, there is a line below which most TotalBsmtSF values fall in which makes sense as the area of Basement usually is lesser than living area. Also there is an exponential increase in SalePrice vs Yearbuilt in the recent past."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Numeric vs Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To plot these relations we can use boxplots and swarmplots. John created these using boxplot function in seaborn. John is creating a boxplot for numerical variables grouped by categories in categorical variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "plt.xticks(rotation = 45)\n",
    "sns.boxplot('Neighborhood', 'SalePrice', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* John concluded that the distribution of SalePrice changes with the individual neighborhoods and can be a good predictor for it.\n",
    "* Next he takes a look at swarmplots which are similar to boxplots, but they also show no of points at each value of numerical variable. A denser plot signifies more observations in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.swarmplot('OverallQual', 'SalePrice', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">Here John can see that there's a marked increase in saleprice as the overall quality increases. So using this variable is a good idea to predict SalePrice.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Categorical vs Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This can be done using using crosstab or graphically using a stacked barplot. John here considers two variables 'Neighborhood' and 'OverallQual' and checks the relation between them using both of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "crosstab = pd.crosstab(index=df[\"Neighborhood\"], columns=df[\"OverallQual\"])\n",
    "crosstab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "crosstab.plot(kind=\"bar\", figsize=(12,8), stacked=True, colormap='Paired')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "John considered some of the initial entries and realised the data :-\n",
    "- Had missing values\n",
    "- Had outliers\n",
    "- Was sub-optimally represented "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treatment of Data\n",
    "***\n",
    "John had already applied linear regression on the house pricing dataset.And it didn't seem to be very helpful.\n",
    "\n",
    "He knows that he cannot apply a ML model on this raw data currently.\n",
    "\n",
    "Stuck with this, he consults his friend Jane for help. Jane suggested him to treat the data and creating new relevant features using the following techniques:\n",
    "- Data Cleaning and Pre-processing\n",
    "- Feature Extraction\n",
    "- Feature Engineering\n",
    "\n",
    "Let’s see how these techniques helped John:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Pre-processing\n",
    "***\n",
    "Data cleaning and preprocessing involves following techniques\n",
    "- Handling outliers\n",
    "- Handling missing values\n",
    "- Handling skewness\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Outilers\n",
    "***\n",
    "\n",
    "**Why Outlier Treatment?**\n",
    "***\n",
    "Jane asked John to starts plotting few graphs of numerical features in order to study their behaviour\n",
    "\n",
    "Let's see how John plots *GrLivArea vs SalePrice*.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scatter plot GrLivArea vs SalePrice\n",
    "\n",
    "# Code starts here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So what did John observe?\n",
    "***\n",
    "* The _SalePrice_ and _GrLivArea_ are in a linear relationship.\n",
    "\n",
    "* At the bottom right there are two points with extremely large GrLivArea that are of a low price. These values are huge oultliers.\n",
    "\n",
    "He decides, to plot a few more graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scatter plot TotalBsmtSF vs SalePrice\n",
    "\n",
    "# Code starts here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that **'TotalBsmtSF'** is also linearly dependent on **'SalePrice'**.\n",
    "\n",
    "At the bottom right of the plot John can noticed that, there is one outlier which has a high Basement area at relatively quite cheap Sale Price.\n",
    "\n",
    "There are quite a few outliers in the data. John considered this to be one of the reasons the Linear Regression model performed poorly.\n",
    "\n",
    "John was through with analyzing the numerical features in the data.\n",
    "\n",
    "Intuitively, John started thinking about the change in behaviour for the categorical features.\n",
    "\n",
    "He decided to go with his intuition and began analysing the categorical features.\n",
    "\n",
    "Wondering how to start, Jane suggested to use boxplots.\n",
    "\n",
    "John plotted a boxplot for the OverallQual and SalePrice to analyze them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot of OverallQual vs SalePrice\n",
    "\n",
    "# Code starts here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Treating outliers is important because\n",
    "Outliers can skew and mislead the training process of machine learning algorithms resulting in longer training times, less accurate models and ultimately poorer results.\n",
    "Many machine learning algorithms are sensitive to the range and distribution of attribute values in the input data.\n",
    "\n",
    "- Surely John knows this, let's see how he removes them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## John's Approach 1: Univariate Methods\n",
    "***\n",
    "John selects a threshold value for the Numerical features and removes all the rows beyond that threshold value.\n",
    "\n",
    "In short :\n",
    "- Visualize the data using scatterplots, histograms and box and whisker plots and look for extreme values.\n",
    "- Assume a distribution (Gaussian) and look for values 1.5 times from the first or third quartile\n",
    "- Filter out outliers candidate from training dataset and assess the model’s performance.\n",
    "\n",
    "Let's see how John treat the ouliers:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deleting outliers\n",
    "df = df.drop(df[(df['GrLivArea']>3000) & (df['GrLivArea']<6000)].index)\n",
    "\n",
    "#Check the graph again\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(df['GrLivArea'], df['SalePrice'])\n",
    "plt.ylabel('SalePrice', fontsize=13)\n",
    "plt.xlabel('GrLivArea', fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What changed?\n",
    "***\n",
    "Let's draw the original plot and the plot after removing the outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, sharex='col', sharey='row', figsize=(10, 7))\n",
    "\n",
    "data = pd.read_csv('../data/train.csv',index_col=0)\n",
    "\n",
    "# Code starts here \n",
    "\n",
    "# Scatter plot for data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Scatter plot for df\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "John had a requirement of a house with living area round 3000sq. The houses with greater Ground Living Area were outliers for him.\n",
    "\n",
    "As, we can easily note from the above plots, that the outliers have been removed.\n",
    "\n",
    "John had heard of few other approaches, that he could have tried to remove the outliers. They are : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach 2: Multivariate Methods\n",
    "\n",
    "- Use clustering methods to identify the natural clusters in the data, identify the points which far away from the cluster centroids\n",
    "- Use dimensionality reduction techniques\n",
    "\n",
    "**Note: ** We will study about these techniques in _Unsupervised Learning_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach 3: Algorithmic Methods\n",
    "\n",
    "- Ensemble methods like random forests and gradient boosting provide relatively robust results with data with outliers.\n",
    "\n",
    "**Note:** We will study about these techniques in _Unsupervised Learning_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Values\n",
    "***\n",
    "\n",
    "John was happy that the outliers had been removed. \n",
    "\n",
    "He was casually skimming through the data when he realised that a lot of the data had missing values.\n",
    "\n",
    "Intuitively, he decided to check out few entries in the data, which had most values as null.\n",
    "\n",
    "For each house entry, he observed the number of features which were missing and arrange them in an ascending order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing data\n",
    "total = df.isnull().sum(axis=1).sort_values(ascending=False)\n",
    "percent = (df.isnull().sum(axis=1)/df.isnull().count(axis=1)).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total_missing_values_per_row', 'Percent'])\n",
    "missing_data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The house entry number 40 had values of 15 features missing. So, did the houses number 1219, 1012, 534 and so on.\n",
    "John realised that there were too many of them\n",
    "\n",
    "Worried, John consulted Jane again on this. Jane explained a few methods to treat missing values.\n",
    "\n",
    "Data can have missing values. It's very normal to have some missing values.\n",
    "\n",
    "**Reasons for Missing Values**\n",
    "\n",
    "- These are values for attributes where a measurement could not be taken or is corrupt for some reason.\n",
    "- Missing that depends on unobserved predictors\n",
    "\n",
    "**Why Missing Value Treatment?**\n",
    "- Missing data in the training data set can reduce the power / fit of a model.\n",
    "- Missing values can lead to a biased model because we have not analyzed the behavior and relationship with other variables correctly\n",
    "- This is useful because some algorithms are unable to work with or exploit missing data.\n",
    "\n",
    "Therefore, it is important to identify and mark this missing data. Once marked, replacement values can be prepared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data[missing_data.Total_missing_values_per_row == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## John's Approach 1: Remove records (rows) that contain a missing value\n",
    "***\n",
    "John had so many houses with missing features.\n",
    "He wondered, what would he do?\n",
    "\n",
    "He found it easy to remove all those rows which contained null values.\n",
    "Neat and Simple, he thought.\n",
    "\n",
    "So, let's drop all those houses which had any feature as 0.\n",
    "If all the features, for a house were available, only then it would be retained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df.copy().dropna(how='any')\n",
    "df_copy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, none of the observations had values for all the features i.e. non-empty rows\n",
    "John would surely lose out all his data in such this case.\n",
    "\n",
    "Hence he could not remove houses (rows) that contain missing value as it will lead to loss of significant information because the missing value is spread all over dataset. \n",
    "\n",
    "In a nutshell :\n",
    "- This approach would work fine if the amount of missing values is very small (up to ~2% of entire data)\n",
    "\n",
    "- But with large amount of scattered missing values, we would lose out on some valuable information.\n",
    "\n",
    "\n",
    "**Note:**\n",
    "\n",
    "A slightly better approach would be to drop only those rows, which had all the values as null.\n",
    "This would surely work better then the previous approach, thought Jon.\n",
    "\n",
    "Let's check the number of houses, with at least some data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df.copy().dropna(how='all')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jon's Approach 2: Remove features (columns) that contain missing values.\n",
    "***\n",
    "John thought of now exploring the features rather than the individual houses.\n",
    "\n",
    "For each feature(column) in the data, John counted the number of houses(rows) with null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing data observing in columns\n",
    "total = df.isnull().sum(axis=0).sort_values(ascending=False)\n",
    "percent = ((df.isnull().sum(axis=0)/df.isnull().count(axis=0))*100).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of null values in the column and their perecentage of the total data\n",
    "missing_data_columns = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "missing_data_columns.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jon observed from the above information that features such as **PoolQC, MiscFeature, Alley, Fence** are having most of the values labeled as missing. Hence, they don't seem to be very important and will not have a significant effect.\n",
    "\n",
    "We can surely drop those columns.\n",
    "So, let's drop these columns with high missing values & check the remaining columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# droping columns containing high missing values \n",
    "df1=df.copy()\n",
    "\n",
    "# Code starts here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the columns left after droping the above features\n",
    "\n",
    "# Code starts here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jon's Approach 3: Missing values Imputation\n",
    "***\n",
    "None of the above techniques worked for John. He dropped a few columns, however he observed that there still were other features with high number of missing values.\n",
    "\n",
    "The *FireplaceQu* column has around 700 missing values whereas *LotFrontage* has around 300 missing values.\n",
    "He was in a dilemma, whether to retain them or drop them.\n",
    "\n",
    "After a long time, he decided to retain them. But, what should he do with the missing values?\n",
    "\n",
    "Let's check out a few columns from the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[[\"PoolQC\",\"MiscFeature\",\"GarageYrBlt\",\"GarageArea\",\"GarageCars\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What he did was that he imputed the missing values with a representative constant value that has meaning within the domain, such as 0 or None, distinct from all other values.\n",
    "\n",
    "He imputed them by proceeding sequentially through features with missing values.\n",
    "\n",
    "- PoolQC : data description says NA means \"No Pool\". That makes sense, given the huge ratio of missing value (+99%) and majority of houses have no Pool at all in general.\n",
    "\n",
    "- MiscFeature : data description says NA means \"no misc feature\"\n",
    "- GarageYrBlt, GarageArea and GarageCars : Replacing missing data with 0 (Since No garage = no cars in such garage.)\n",
    "\n",
    "John consider 'NA',  'no misc features' and 'Nan' to be equivalent to null values and need to be imputed.\n",
    "\n",
    "Let's impute the missing values as per the following :\n",
    "- Categorical Features : None\n",
    "- Numerical Features : 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df.copy()\n",
    "df2[\"PoolQC\"] = df2[\"PoolQC\"].fillna(\"None\")\n",
    "df2[\"MiscFeature\"] = df2[\"MiscFeature\"].fillna(\"None\")\n",
    "\n",
    "for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n",
    "    df2[col] = df2[col].fillna(0)\n",
    "print(df2[[\"PoolQC\",\"MiscFeature\",\"GarageYrBlt\",\"GarageArea\",\"GarageCars\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jon's approach 5: Impute missing values with a mean, median or mode value for the column.\n",
    "***\n",
    "Imputing values with a default value, is a good option.\n",
    "But, we can still perform better, right?\n",
    "\n",
    "John, thought of imputing the missing values with more relevant data rather than a default value.\n",
    "Indeed, a great idea!!\n",
    "\n",
    "John applied this idea on a few features in our data.\n",
    "\n",
    "_Numerical Feature :_ \n",
    "- Since the area of each street connected to the house property most likely have a similar area to other houses in its neighborhood , he filled in missing values by the median LotFrontage of the neighborhood.\n",
    "\n",
    "\n",
    "_Categorical Feature :_\n",
    "- However, for categorical feature one cannot calculate **mean** or **median**.\n",
    "- 'RL' is by far the most common value. So he filled in missing values with 'RL'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation Using Imputer\n",
    "imp_mean = SimpleImputer(missing_values = np.nan, strategy='mean')\n",
    "imp_mean.fit(df2[['LotFrontage']])\n",
    "df2['LotFrontage'] = imp_mean.transform(df[['LotFrontage']])\n",
    "print(df2[\"LotFrontage\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mode value imputation\n",
    "df2['MSZoning'] = df2['MSZoning'].fillna(df2['MSZoning'].mode()[0])\n",
    "print(df2['MSZoning'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Other Methods to Handle Missing Values : **\n",
    "\n",
    "#### Approach 6: Impute missing values with a value estimated by another predictive model\n",
    "\n",
    "We will study these techniques while we learn _Unsupervised Learning_\n",
    "\n",
    "#### Approach 7 : Use algorithms that Support Missing Values\n",
    "\n",
    "Not all algorithms fail when there is missing data.\n",
    "\n",
    "There are certain algorithms that work well even with missing values.\n",
    "For such algorithms, we need not worry about the missing values.\n",
    "\n",
    "Few such algorithms are : \n",
    "- Random Forest\n",
    "- Classification and Regression Trees\n",
    "- KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Skewness\n",
    "***\n",
    "\n",
    "So far John has really made some good progress.\n",
    "\n",
    "However, John knows that the Linear Regression has following assumptions:\n",
    "- Linear relationship\n",
    "- **Multivariate normality**\n",
    "\n",
    "**Multivariate normality means that regression requires all its variables to be normal. \n",
    "By having skewed data one might violate the assumption of normality.**\n",
    "\n",
    "So, he is far from applying a Linear Regression yet and needs to check the skewness of the data.\n",
    "Let's explore a bit more about Skewness.\n",
    "\n",
    "**What is Skewness?**\n",
    "\n",
    "- Skewness is a measure of asymmetry of distribution.\n",
    "- Skewness is a measure of symmetry, or more precisely, the lack of symmetry. \n",
    "\n",
    "**Why handle Skewness?**\n",
    "- Many model building techniques have the assumption that predictor values are distributed normally and have a symmetrical shape. Hence, it is sometimes paramount to deal with skewness.\n",
    "- Symmetric distribution is preferred over skewed distribution as it is easier to interpret and generate inferences. \n",
    "\n",
    "Let's look at some plots.\n",
    "Here, we check the skewness of `GrLivArea` and plot it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#histogram and normal probability plot\n",
    "sns.distplot(df['GrLivArea'], fit=norm);\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(df['GrLivArea'], plot=plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets numerically calculate the skewness for the _GrLivArea_ feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skewed_grLiv = skew(df['GrLivArea'])\n",
    "print(skewed_grLiv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For normally distributed data, the skewness should be about 0. A skewness value > 0 means that there is more weight in the left tail of the distribution.\n",
    "\n",
    "As evident from the histogram plot, the *GrLivArea* is *Left skewed*.\n",
    "\n",
    "Hmm... let's try removing skewness from our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## John's Approach 1: Replacing the data with the log, square root, or inverse transformed data may help to remove the skewness.\n",
    "***\n",
    "\n",
    "John read that applying **log transformation** on the data would reduce the skewness. It was worth a try, he thought.\n",
    "\n",
    "Let's see how he applied Log Transform to remove the skewness in the data and plot it.\n",
    "\n",
    "John chose to focus on the **GrLivArea** feature for which he had already calculated the skewness. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data transformation\n",
    "df_trans=df.copy()\n",
    "df_trans['GrLivArea'] = np.log(df_trans['GrLivArea'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying log_transform on the _GrLivArea_ and plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformed histogram and normal probability plot\n",
    "sns.distplot(df_trans['GrLivArea'], fit=norm)\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(df_trans['GrLivArea'], plot=plt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "John was happy to see that the plot is now much more symmetrical. It seems more like a bell curve now.\n",
    "\n",
    "Let's verify the same numerically.\n",
    "\n",
    "skewness_grLiv = skew(df_trans['GrLivArea'])\n",
    "print(skewness_grLiv)\n",
    "\n",
    "Hmm... Log transformation did remove skewness in the original data.\n",
    "Quite evidently, the skewness value is now much more closer to 0.\n",
    "\n",
    "When Jane learnt about this she informed John about another method **\"Square Root Transformation\"** to remove skewness in the data.\n",
    "\n",
    "Let's see how to apply Square Root transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trans = df.copy()\n",
    "df_trans['GrLivArea'] = np.sqrt(df_trans['GrLivArea'])\n",
    "\n",
    "sns.distplot(df_trans['GrLivArea'], fit=norm)\n",
    "fig = plt.figure()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the log transform; even the square root transformation seems to reduce the skewness.\n",
    "\n",
    "Let's verify the same numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code starts here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "***\n",
    "\n",
    "After such an exhausting work, John was finally ready to apply the Linear Regression on the dataset.\n",
    "\n",
    "## But ....\n",
    "***\n",
    "Linear Regression assumes that all the data, is normally distributed\n",
    "\n",
    "\"What should I do? How can I make the data have a normal distribution?\" \n",
    "\n",
    "Finding himself again in a problem, John called up Jane.\n",
    "\n",
    "Standardization of datasets is a common requirement for many machine learning estimators; they might behave badly if the individual features do not more or less look like standard normally distributed data: Gaussian with zero mean and unit variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization\n",
    "***\n",
    "- Standardization is recommended when regression models are being built.\n",
    "- Standardization of data is to have zero mean and unit variance.\n",
    "\n",
    "\n",
    "Standardization cannot be performed on Categorical data, so we need to separate Numerical Features and Categorical Features.\n",
    "\n",
    "So, Jane asked John to separate the original data into two categories : Numerical and Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_feature = [a for a in range(len(df.dtypes)) if df.dtypes[a] in ['int64','float64']]\n",
    "numeric_data = df.iloc[:,numeric_feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_name = df.columns.difference(df.columns[numeric_feature])\n",
    "cat_data = df.loc[:,cat_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out some of the entries in Numerical Data & Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, before applying Standardization, it is required that all the data needs to be imputed.\n",
    "\n",
    "So, let's first impute the missing values and then standardize the data.\n",
    "Also, after imputing let's check all the columns once.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing the missing values in numeric data to futher process it for standardization\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "fill_Nan = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "imputed_DF = pd.DataFrame(fill_Nan.fit_transform(numeric_data))\n",
    "imputed_DF.columns = numeric_data.columns\n",
    "imputed_DF.index = numeric_data.index\n",
    "imputed_DF.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out some of the entries of the Imputed DataFrame and check if any of the features contain any null value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_DF.head()\n",
    "imputed_DF.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, let's check out some of the columns in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_transform = numeric_data.columns\n",
    "column_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that the missing values have been imputed, Jane asked John to move on to standardizing the data using scale() function in preprocessing module of sklearn library.\n",
    "\n",
    "Also, check out the standardized data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "standardized_DF = preprocessing.scale(imputed_DF)\n",
    "standardized_DF\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "***\n",
    "After observing the data, John realised that it wasn't a great idea to compare two entirely different things.\n",
    "\n",
    "For eg : It wouldn't be wise to compare GrLivArea(squarefoot) and YrSold(time) even though both are numerical features.\n",
    "\n",
    "\n",
    "When there are predictors with different units and ranges, the final model will have coefficients which are very small for some predictors and it makes it difficult to interpret.\n",
    "\n",
    "He needs to somehow **scale down everything in a range**, so that the model's prediction is not affected by the different units at all.\n",
    "\n",
    "**Scaling**\n",
    "\n",
    "An alternative standardization is scaling features to lie between a given minimum and maximum value, often between zero and one, or so that the maximum absolute value of each feature is scaled to unit size.\n",
    "\n",
    "Centering and Scaling will improve the numerical stability of some models\n",
    "\n",
    "But Standardization cannot be applied on categorical data, hence we split the categorical and numerical data in order to standardize the numerical data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing numerical features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "stand_scale= StandardScaler()\n",
    "imputed_DF.loc[:, column_transform] = stand_scale.fit_transform(imputed_DF.loc[:, column_transform])\n",
    "imputed_DF.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algebraic Transformations\n",
    "***\n",
    "\n",
    "John is now thorough with Polynomial Features and dealing with skewness using log transformation and square root transformation.\n",
    "\n",
    "All of these, fall under **ALGEBRAIC TRANSFORMATIONS.**\n",
    "\n",
    "John has already observed that transforming a variable into another form drastically improves a model’s performance.\n",
    "\n",
    "Let's look at another example.\n",
    "\n",
    "Here, John plots the distribution plot and probablity plot for TotalBsmtSF. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#histogram and normal probability plot\n",
    "sns.distplot(df['TotalBsmtSF'], fit=norm);\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(df['TotalBsmtSF'], plot=plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "John faced a problem while looking through the observations of variable `TotalBsmtSF`\n",
    "\n",
    "- A significant number of observations with value zero (houses without basement) as evident by the horizontal section of the blue line.\n",
    "- A big problem because the value zero doesn't allow us to do log transformations.\n",
    "\n",
    "Jane to the rescue again!! She suggested to apply a sqrt transformation here, he will have to create a variable that can get the effect of having or not having basement (binary variable). \n",
    "\n",
    "John, then, did a log transformation to all the non-zero observations, ignoring those with value zero. This way one can transform data, without losing the effect of having or not basement.\n",
    "\n",
    "If the house has a basement then we add them to a different column, else ignore it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create column for new variable \n",
    "#if area>0 it gets 1, for area==0 it gets 0\n",
    "\n",
    "df['NewBsmt'] = 0\n",
    "df.loc[df.TotalBsmtSF > 0, \"NewBsmt\"] = 1\n",
    "len(df.loc[df['NewBsmt'] == 1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's apply square root transform over this new column & then replot the histogram and probability plot for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform data\n",
    "df.loc[df.NewBsmt == 1,'TotalBsmtSF'] = np.sqrt(df['TotalBsmtSF'])\n",
    "\n",
    "# histogram and normal probability plot\n",
    "sns.distplot(df[df['TotalBsmtSF']>0]['TotalBsmtSF'], fit=norm);\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(df[df['TotalBsmtSF']>0]['TotalBsmtSF'], plot=plt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, far we have covered log and sqrt tranformations,\n",
    "Here are a few more useful transforms:\n",
    "\n",
    "- Exponential Transform\n",
    "- Tanh Transform\n",
    "- Polynomial Transform\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "***\n",
    "\n",
    "John was now observing the data more better then ever. After a few minutes of skimming through the data, he observed that there were few insignificant columns present in the data.\n",
    "\n",
    "For eg; the following features conveyed the same thing :\n",
    "- BsmtFullBath\n",
    "- FullBath\n",
    "- HalfBath\n",
    "\n",
    "So, it would be wise to combine all these features together into a single column, just to save some time and space.\n",
    "\n",
    "Unknowingly, partly what John was doing and now, what he's thinking to do is nothing but **FEATURE ENGINEERING.**\n",
    "\n",
    "Seems, like few things start becoming obvious with time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So, what is feature engineering?\n",
    "***\n",
    "\n",
    "Feature engineering is the process of using domain knowledge of the data to create features that make machine learning algorithms work. Feature engineering is fundamental to the application of machine learning, and is both difficult and expensive. \n",
    "\n",
    "Coming up with features is difficult, time-consuming, requires expert knowledge. \"Applied machine learning\" is basically feature engineering. (Andrew Ng)\n",
    "... some machine learning projects succeed and some fail. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What makes the difference?**\n",
    "\n",
    "Easily the most important factor is the features used. (Pedro Domingos)\n",
    "\n",
    "Feature Engineering: The art of coming up with new features with more predictive power using\n",
    "\n",
    "- experience\n",
    "- domain expertise\n",
    "- empirical processes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most important techniques in feature engineering\n",
    "\n",
    "- Applying Domain Expertise\n",
    "- Combining Feature\n",
    "- Encoding of categorical variables\n",
    "- Time series feature extraction\n",
    "- NLP feature extraction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Domain Expertise\n",
    "***\n",
    "Domain expertise or domain knowledge is nothing but expertise in a particular field , and when I say field it can mean anything like field of Education, Healthcare, Consumer Goods, Retail and the list is endless. \n",
    "A domain expert is someone who is not related to the technology aspect but has indepth knowledge about the particular industry, how is it shaping up, the trends, what are the things that will impact the industry.\n",
    "\n",
    "For example, if you are called into develop a particular application for a consumer goods company and specifically a Apparel & Footwear company.\n",
    "The application that you build has to be that compliments the industry and the various facets of it, and you as a technology guy wouldnt know it, this is where a domain expert will come in and explain how that industry works and what would be the best way to have the application built."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Feature\n",
    "***\n",
    "This is one of the crucial part of feature engineering which involves going through the data set variable and understanding the variable(s) and hence combining one or more features to create and new features. This involves some what on intutive decision making and little bit research about the domain of the data set.\n",
    "\n",
    "This Step basically is used in order to hike the performance of the model which is fitted on the data.\n",
    "\n",
    "So, let's combine 4 different features `BsmtFullBath`, `BsmtHalfBath`, `FullBath`, `HalfBath` into a single feature `TotalBath`.\n",
    "\n",
    "Below, we have added all these 4 features in different proportions, to combine them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new features by combining the existing features\n",
    "\n",
    "# Overall pool score\n",
    "df[\"PoolScore\"] = df[\"PoolArea\"] * df[\"PoolQC\"]\n",
    "\n",
    "# Total number of bathrooms\n",
    "df[\"TotalBath\"] = df[\"BsmtFullBath\"] + (0.5 * df[\"BsmtHalfBath\"]) + df[\"FullBath\"] + (0.5 * df[\"HalfBath\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding of categorical variables\n",
    "***\n",
    "So far John had been focusing much on numerical features. \n",
    "He observed that few columns such as ``, `` contain `string` values.\n",
    "\n",
    "Well, what about it?\n",
    "It wouldn't matter much, he thought.\n",
    "But oh boy, was he in for a surprise?\n",
    "\n",
    "We had learned **Numpy** remember?\n",
    "Numpy functions work well only with numbers, however when worked with formats other then `int` or `float`, would either give us gibberish result or would give us errors.\n",
    "\n",
    "So, John now had to convert these categorical features into numbers or vectors or something similar.\n",
    "How do I do that?\n",
    "\n",
    "After struggling for a while, he found the solution.\n",
    "He can do Encoding i.e. encode the string into numbers.\n",
    "\n",
    "Luckily, sklearn provides us functions to do this work.\n",
    "\n",
    "Let's Label Encode the `MSZoning` column to numerical features.\n",
    "Currently this column contains values 'RF', 'RL' and other values. We will change it with numbers\\vectors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Encoding?\n",
    "***\n",
    "Most algorithms we use work with numerical values whereas more often than not categorical data are in text/string (male, female) or bin (0-4, 4-8 etc.) form.\n",
    "\n",
    "One option is to leave these variables out of the algorithms and use only numeric data. But in doing so we can lose out on some critical information.\n",
    "\n",
    "Hence, it is generally a good idea to include the categorical variables into your algorithms by encoding them to convert to numeric values\n",
    "But, first let's learn a thing or two about our categorical variables.\n",
    "\n",
    "Encoding a categorical variable is to categorize them into numerical features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoding\n",
    "***\n",
    "Since different kinds of categorical variables capture different amount of information, we need different techniques to encode them.\n",
    "\n",
    "Label Encoding: Each category is given one label e.g. 0, 1, 2 etc.\n",
    "\n",
    "Label encoding is a handy technique to encode categorical variables. However, such encoded nominal variables might end up in being misinterpreted as ordinal.\n",
    "\n",
    "However, nonlinear models like decision trees can handle such encoding\n",
    "\n",
    "Let's apply Label Encoding on all the categorical features in our dataset. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#Auto encodes any dataframe column of type category or object.\n",
    "\n",
    "def dummyEncode(df):\n",
    "        columnsToEncode = list(df.select_dtypes(include=['category','object']))\n",
    "        le = LabelEncoder()\n",
    "        for feature in columnsToEncode:\n",
    "            try:\n",
    "                df[feature] = le.fit_transform(df[feature])\n",
    "            except:\n",
    "                print('Error encoding '+feature)\n",
    "        return df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummyEncode(cat_data)\n",
    "cat_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try Label Encoding explicitly on the 'MSZoning' column.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "lablel_encoder = LabelEncoder()\n",
    "df['MSZoning'] = lablel_encoder.fit_transform(df['MSZoning'])\n",
    "df['MSZoning']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As evident, now the column `MSZoning` only contains numerical values, rather then String.\n",
    "\n",
    "**Encoding can be performed on the following type of variables : **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOMINAL VARIABLES\n",
    "***\n",
    "Consider following three categorical variables and their values\n",
    "\n",
    "**Colour**: Blue, Green, Red, Yellow\n",
    "\n",
    "**Educational Qualification**: Primary School, Secondary School, Graduate, Post-Graduate, Phd.\n",
    "\n",
    "**Salary Bracket**: 0-50,000, 50,001-100,000, 100,001-150,000, 150,001-200,000\n",
    "\n",
    "Although all three of them are categorical variables, they are different in the amount of information they convey. \n",
    "Let's look at them one by one.\n",
    "\n",
    "Colour conveys blue is different from red. That's all. The value of the variable is not meant to capture any relative difference among the values. Such variables are called `Nominal Variables`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ORDINAL VARIABLES\n",
    "***\n",
    "Now consider educational qualification.\n",
    "\n",
    "The value \"graduate\" does not only conveys that it is different from value say \"Primary school\", it also implies that it is more in terms of qualification than \"Primary School\". \n",
    "\n",
    "Such variables are called `Ordinal Variables` because they convey a sense of order.\n",
    "\n",
    "In our example, Primary School < Secondary School < Graduate < Post-Graduate < Phd. (in terms of qualification)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### INTERVAL VARIABLES\n",
    "***\n",
    "The third variable, salary bracket is similar to educational qualification by conveying order (a person earning 50,001-100,000 earns more salary than 0-50,000). \n",
    "\n",
    "However, here, apart from knowing the order, we also know the interval between the values. \n",
    "\n",
    "Here we can say that the averages of each of the values are separated by 50,000. Such variables are called `Interval variables`.\n",
    "\n",
    "There are two types of broadly used algorithm which performs the task of encoding of variables.\n",
    "\n",
    "Let's have a look at these algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encoding\n",
    "***\n",
    "After studying Label Encoding, John wondered what would happen if the Categories contained more then one string values.\n",
    "For eg; instead of `RL`, `ML` as values for the `MSZoning`, it contained `RL+AB` and `ML+CD` or something similar?\n",
    "He wondered, whether he could have been able to apply the Label Encoding then.\n",
    "\n",
    "Out of curiousity, he started browsing over the internet, and within few clicks of Google Search.\n",
    "The answer turned out to be **One Hot Encoding**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is One Hot Encoding?\n",
    "***\n",
    "A dataset with more dimensions requires more parameters for the model to understand, and that means more rows to reliably learn those parameters.\n",
    "\n",
    "The effect of using One Hot Encoder is addition of a number of columns (dimensions).\n",
    "\n",
    "If the number of rows in the dataset is fixed, addition of extra dimensions without adding more information for the models to learn from can have a detrimental effect on the eventual model accuracy.\n",
    "One way to deal with this `curse of dimensionality` is using hashing techniques.\n",
    "\n",
    "<img src = \"../images/screenshot-docs.google.com-2017-09-27-10-58-21-457.png\" >\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(cat_data, drop_first=True) #one hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### In Class Activity:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Automobile Dataset\n",
    "\n",
    "### Problem Statement :\n",
    "\n",
    "This dataset consist of data From 1985 Ward's Automotive Yearbook. It consists of three types of entities:\n",
    "- The specification of an auto in terms of various characteristics\n",
    "- Its assigned insurance risk rating \n",
    "- Its normalized losses in use as compared to other cars.\n",
    "\n",
    "\n",
    "\n",
    "### About the DataSet :\n",
    "\n",
    "Below are the features in the dataset and their description\n",
    "\n",
    "|Features|Description|\n",
    "|-----|-----|\n",
    "|symboling|It is a risk factor symbol associated to the price & corresponds to the degree to which an automobile is more risky than its price indicates. +3 indicates risky, -3 indicates safety|\n",
    "|make|Indicates the maker or manufacturer of the automobile|\n",
    "|fuel-type|Indicates the type of fuel - diesel or gas|\n",
    "|body-style|Indicates whether the body shape of automobile is a hardtop, wagon, sedan, hatchback or convertible|\n",
    "|drive-wheels|Indicates the configuration of drive wheels for the automobile|\n",
    "|wheel-base|It is the distance between the centers of the front wheel and the rear wheel. It is continuous ranging from 86.6 120.9|\n",
    "|length|Indicates length of the automobile and ranges from 141.1 to 208.1.|\n",
    "|width|Indicates width of the automobile and ranges from 60.3 to 72.3|\n",
    "|height|Indicates the width of the automobile and ranges 47.8 to 59.8|\n",
    "|horsepower|Maximum horsepower the automobile engine can output and ranges from 48 to 288|\n",
    "|peak-rpm|RPM is a way to measure how many times per minute components in the engine rotate. It ranges from 4150 to 6600|\n",
    "|highway-mpg|Indicates the miles per galon typically consumed at highways. It is continuous from 4150 to 6600|\n",
    "|city-mpg|Indicates the miles per galon typically consumed at cities. It is continuous from 13 to 49|\n",
    "|price|Indicates the price of the automobile and ranges from 5118 to 45400|\n",
    "|normalized-losses|It is a continuous variable ranging from 65 to 256|\n",
    "|engine-location|Indicates the location of the engine - front or rear|\n",
    "|engine-type|Indicates the type of engine.|\n",
    "|engine-size|It is continuous from 61 to 326|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and understand your data\n",
    "- The data consists of 205 instances and 14 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/Automobile_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the number of null values in each column get a general description of your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code starts here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  What insights do you get from the data?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot a histogram showing the distribution of the car prices (target variable) and if any skewness, print the value of skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code starts here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Is there a skewness?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code starts here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the pearson correlation of all the features in a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code starts here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Whats are the insights?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot a boxplot that shows the variability of each 'body-style' with respect to the 'price'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code starts here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Does any body-style has Outliers?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot a boxplot showing the variablity in the 'price' for each 'make' of the car "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code starts here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What are the insights you get from this plot?\n",
    "   \n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing for the Automobile dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find out number of records having '?' value for normalized losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of records having '?' value\n",
    "# Code starts here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute the missing values of the numerical data with mean of the column (`normalized-losses and horsepower`)  (Make sure you replace \"?\" by \"NaN\" for entire dataset before Imputing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing '?' by 'NaN' and then Imputing missing data in the columns 'normalized-losses','horsepower'\n",
    "# Code starts here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the skewness of the numeric features and apply square root transformation on features with skewness greater than 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code starts here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine the 'height' and 'width' to make a new feature 'area' of the frame of the car."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code starts here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Label Encode the categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Label Encode the categorical columns\n",
    "def dummyEncode(automobile):\n",
    "    columnsToEncode = list(automobile.select_dtypes(include=['category','object']))\n",
    "    le = LabelEncoder()\n",
    "    for feature in columnsToEncode:\n",
    "        try:\n",
    "            automobile[feature] = le.fit_transform(automobile[feature])\n",
    "        except:\n",
    "            print('Error encoding '+feature)\n",
    "    return automobile\n",
    "\n",
    "\n",
    "# Call the function dummyEncode to encode the categorical features\n",
    "\n",
    "# Code starts here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank You\n",
    "***\n",
    "### Next Session: Logistic Regression\n",
    "\n",
    "- Difference between Linear and Logistic Regression\n",
    "     - Sigmoid function\n",
    "     - Cost function with Gradient Descent\n",
    "     - Evaluation metrics"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
